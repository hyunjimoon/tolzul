---
date: 2025-04-01
tags:
  - vikash
---


invitation
to [[hazhir_rahmandad]], [[feedback from matt, abdullah]], 

modeling community: 

mail template: 
- share my vision of summary of [[ðŸŒ™amoon]], 
- how their mentoring helped me find meaning in efforts
- meaning of this event 
- plans to develop this together

surprising : scott stern and josh tenanbaum are mingling 

face validity : 

criticality: 

- system dynamics [[hazhir_rahmandad]]
- the enterprise [[feedback from matt, abdullah]]
- mobility venture [[jinhua_zhao]] - explain his contribution
### social planner session
- bayesian+evolutionary entrepreneurship [[steve_pinker]] - 
- 


## 1. theory of mind via probabilist inference

- rationality might matter (only one agent knowing all the thing - let's not develop this) - reaoning that underpin cooperative intelligence

- theory of mind as invers planning (human with ) 
- understand human mind - rational belief  (generative - reasoning and planning - ai is generative ; goal beleif desire)
- goal ~ goal_prior(), policy ~ planner(state, goal), act~policy(state); 
- pddl.ji
- model based planning + probapog -> theory of mind as inverse planning + languge model 
- cooperative decision making and language
- saling cooperation(alingment)

based on what we say and do + (no)
infer goal online

given initial sate s0, sequence of action a, goals g in G, 
infer distribution over goals P(g|a_1:t, so) at each step t
hal 9000 ()

search budget
â­ï¸strategy = plan pi

constrain trying to invert
fast bottom up heuristic to generate plausible
- fully observed state (pomdp belief approximation)
- planning - 
- cognitive appraoch (understan other's mind - guessing the other's thinking and wanting)
- heuristic bottom up proposals (guessing from dictionary)
- n-gram model (open ended goal inference)scales much better than large space goals
- ðŸ™‹â€â™€ï¸copilots data set of action instead of n-gram model

reasoning about other agent's planning (prob.programming and inverse planning)

HAL was to withhold the real purpose of the mission (i.e., contact with alien intelligence) from the crew until a certain point. This secrecy went against HALâ€™s other directiveâ€”to be fully honest and cooperative with the humans onboard.

two hard problems in CS: naming and hashing validation (off by 1 error)

modeling utterances with llm likelihoods; prompt and completion
prompting llm

humans are good at precision and recall + we're measuring the correlation

multimodal bayesian instruction following + clips achieve much higher realiability than gpt + safe goal assitance under uncertainty (neuro symbolic system)

## 2. coarse to fine synthesis of proababilistic programs

phase transition between logic and probabilisticc 

sequence of refinment - coarse 
natural reasoning ols kernel, (probabilisty ; proofs and models; traces of models and source code)
â­ï¸style
natural deduction and natural intelligence

for each type, random constructor

attention - depends on the (maturation of the field)

perception of dots (core elements - rudimentally - more realistic approach)
- ðŸ™‹â€â™€ï¸bug or feature on not modeling attention : depends on the field, and it depends on the project and its maturation within the field. So here we're trying to break ground in AI by saying, Give models of kinds of reasoning processes that I think have really, not really been explored in the past. So I think it would be a mistake of when building those models to try to accurately model the details of any particular implementation, including the ones you know that might be inside an organism you know, and especially details of sensory processing and architectural limits of attention

"too complex" judgement - do computer science ai (if you )
## 3. language model probabilistic programming

vikash was suprised by - application of probcomp framework

language models are probabilistic construct

semantics of natural language - model it symbolically 
write x s.t. y P(x); - reweigt it by prob. 

NOT LEARNED BUT ENGINEERED REPRESENTATION
unpack some of the semantics

engineer not warps 
when you generate word you know what comes last? starts with word and ending with this phrase 

self-steering, natrual language probabilistic programming 1billion

## 4. scaling thought towards human levels of robustness, via inference controllers

1,2,3 as different facets of reasoning: about minds, in words, symbolic strengths, -> SMC
inference controllers 

prompt -> sample from posterior pops out
generating particles - annoying 
intuitive is more robust, efficient, than engineered one

scaling of monte carlo approximation
runtime system monitoring (coherent and stable - chains of thought reasoning; rational they are spitball, reason)
PARTICLE IS NOT ENOUGH
how can we generate so many hypotheses from any proposal so shockingly efficiently and easily? How does that relate to the structure of knowledge? How can we do that with computing elements that are a million times slower than silicon? We will actually talk about technically serious answers to some of those problems t

- we are right at the edge!!!! so we're struggling with metaphor 
survival of the fittest that defies some temporal sequence - hierachical goal - individual level and social level goal; 

width () and depth (rejuvanation)) - socring to gauge

inference time program struggles

define bad sets - 

remotely approach human thought (inference - aerodynamics of flow)
