#!/usr/bin/env python3
"""
ì „ë¼ì¢Œìˆ˜êµ° ê²¬ë¦¬ì‚¬ì˜ êµ°ë ¹ â€” ì „ì²´ í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸
Complete Test Pipeline Runner

í•µì‹¬ ì •ì‹ : ì¼ê´€ì„± ê²€ì¦ì˜ ìë™í™”
Core Spirit: Automated Consistency Verification

ì‹¤í–‰ ë°©ë²•:
    python run_all_tests.py              # ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    python run_all_tests.py --verbose    # ìƒì„¸ ì¶œë ¥
    python run_all_tests.py --quick      # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë§Œ
    python run_all_tests.py --report     # HTML ë¦¬í¬íŠ¸ ìƒì„±

=============================================================================
í†µì œì‚¬: âš“ ì´ìˆœì‹  ë¬¸í˜„ì§€ (Moon)
=============================================================================
"""

import argparse
import subprocess
import sys
from pathlib import Path
from datetime import datetime
from typing import List, Tuple, Optional
import json

# Base paths
TEST_ROOT = Path(__file__).resolve().parent
EMPIRICS_ROOT = TEST_ROOT.parent
UNIT_TEST_DIR = TEST_ROOT / "unit"


# ============================================================================
# FLEET CONFIGURATION
# ============================================================================

FLEET_CONFIG = {
    "í†µì œì‚¬": "âš“ ì´ìˆœì‹  ë¬¸í˜„ì§€ (Moon)",
    "motto": "í•„ì‚¬ì¦‰ìƒ (å¿…æ­»å½ç”Ÿ) â€” ì£½ìœ¼ë ¤ í•˜ë©´ ì‚´ ê²ƒì´ìš”, ì‚´ë ¤ í•˜ë©´ ì£½ì„ ê²ƒì´ë‹¤",
    "test_philosophy": "è¦‹åˆ©æ€ç¾© â€” ì´ìµì„ ë³´ë©´ ì˜ë¡œì›€ì„ ìƒê°í•˜ë¼"
}

TEST_MODULES = {
    "hypothesis_consistency": {
        "name": "ê°€ì„¤-êµ¬í˜„ ì¼ê´€ì„±",
        "file": "test_hypothesis_consistency.py",
        "commander": "ê¹€ì™„ ğŸ™",
        "priority": 1,
        "description": "VÂ² â†’ V(1-V) ìˆ˜ì‹ ì¼ê´€ì„± ê²€ì¦"
    },
    "vagueness_scorer": {
        "name": "Vagueness ìŠ¤ì½”ì–´ëŸ¬",
        "file": "test_vagueness_scorer.py",
        "commander": "ê¹€ì™„ ğŸ™",
        "priority": 2,
        "description": "ìŠ¤ì½”ì–´ë§ ì•Œê³ ë¦¬ì¦˜ ì •í™•ì„± ê²€ì¦"
    },
    "models_consistency": {
        "name": "íšŒê·€ëª¨í˜• ì¼ê´€ì„±",
        "file": "test_models_consistency.py",
        "commander": "ë‚˜ëŒ€ìš© ğŸ…",
        "priority": 1,
        "description": "íšŒê·€ ìˆ˜ì‹ê³¼ ê°€ì„¤ ì¼ì¹˜ ê²€ì¦"
    },
    "plotting_consistency": {
        "name": "ì‹œê°í™” ì¼ê´€ì„±",
        "file": "test_plotting_consistency.py",
        "commander": "ì–´ì˜ë‹´ ğŸ‘¾",
        "priority": 3,
        "description": "í”Œë¡¯ì´ ê°€ì„¤ì„ ì •í™•íˆ ë°˜ì˜í•˜ëŠ”ì§€ ê²€ì¦"
    }
}


# ============================================================================
# BANNER
# ============================================================================

def print_banner():
    """Print fleet banner"""
    print("=" * 70)
    print("ì „ë¼ì¢Œìˆ˜êµ° ê²¬ë¦¬ì‚¬ì˜ êµ°ë ¹ â€” í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸")
    print("Test Pipeline for PhD Thesis Paper Generation System")
    print("=" * 70)
    print(f"í†µì œì‚¬: {FLEET_CONFIG['í†µì œì‚¬']}")
    print(f"ì¢Œìš°ëª…: {FLEET_CONFIG['motto']}")
    print(f"í…ŒìŠ¤íŠ¸ ì² í•™: {FLEET_CONFIG['test_philosophy']}")
    print("-" * 70)
    print(f"ì‹¤í–‰ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)


# ============================================================================
# TEST RUNNER
# ============================================================================

class TestRunner:
    """Test pipeline runner"""

    def __init__(self, verbose: bool = False, quick: bool = False):
        self.verbose = verbose
        self.quick = quick
        self.results: List[Tuple[str, bool, str]] = []

    def run_single_test(self, module_key: str) -> Tuple[bool, str]:
        """
        Run a single test module

        Returns:
            (success: bool, output: str)
        """
        if module_key not in TEST_MODULES:
            return False, f"Unknown test module: {module_key}"

        module = TEST_MODULES[module_key]
        test_file = UNIT_TEST_DIR / module["file"]

        if not test_file.exists():
            return False, f"Test file not found: {test_file}"

        print(f"\n{'='*60}")
        print(f"ğŸ§ª {module['name']} ({module['commander']})")
        print(f"   {module['description']}")
        print(f"{'='*60}")

        # Build pytest command
        cmd = [sys.executable, "-m", "pytest", str(test_file)]

        if self.verbose:
            cmd.extend(["-v", "--tb=short"])
        else:
            cmd.extend(["-q", "--tb=line"])

        if self.quick:
            cmd.extend(["-x"])  # Stop on first failure

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                cwd=str(EMPIRICS_ROOT),
                timeout=120
            )

            output = result.stdout + result.stderr
            success = result.returncode == 0

            if self.verbose:
                print(output)
            else:
                # Print summary
                if success:
                    print("âœ… PASSED")
                else:
                    print("âŒ FAILED")
                    # Print failure details
                    for line in output.split('\n'):
                        if 'FAILED' in line or 'Error' in line:
                            print(f"   {line}")

            return success, output

        except subprocess.TimeoutExpired:
            return False, "Test timed out after 120 seconds"
        except Exception as e:
            return False, f"Error running test: {e}"

    def run_all_tests(self) -> bool:
        """
        Run all tests in priority order

        Returns:
            True if all tests passed
        """
        print_banner()

        # Sort by priority
        sorted_modules = sorted(
            TEST_MODULES.items(),
            key=lambda x: x[1]["priority"]
        )

        all_passed = True

        for module_key, module in sorted_modules:
            success, output = self.run_single_test(module_key)
            self.results.append((module_key, success, output))

            if not success:
                all_passed = False

                if self.quick:
                    print("\nâš ï¸ Quick mode: Stopping on first failure")
                    break

        self.print_summary()
        return all_passed

    def print_summary(self):
        """Print test summary"""
        print("\n" + "=" * 70)
        print("ğŸ“Š TEST SUMMARY")
        print("=" * 70)

        passed = sum(1 for _, success, _ in self.results if success)
        total = len(self.results)

        for module_key, success, _ in self.results:
            module = TEST_MODULES[module_key]
            status = "âœ…" if success else "âŒ"
            print(f"  {status} {module['name']} ({module['commander']})")

        print("-" * 70)
        print(f"  ì´ ê²°ê³¼: {passed}/{total} passed")

        if passed == total:
            print("\nğŸŠ ì „ë¼ì¢Œìˆ˜êµ°ì˜ ì„ë¬´ ì™„ìˆ˜!")
            print("   ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼ â€” ì¼ê´€ì„± ê²€ì¦ ì™„ë£Œ")
        else:
            print(f"\nâš ï¸ {total - passed} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            print("   ìœ„ ì‹¤íŒ¨ í•­ëª©ì„ í™•ì¸í•˜ê³  ìˆ˜ì •í•˜ì„¸ìš”")

        print("=" * 70)

    def generate_html_report(self, output_path: Optional[Path] = None) -> Path:
        """Generate HTML test report"""
        if output_path is None:
            output_path = TEST_ROOT / "reports" / f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"

        output_path.parent.mkdir(parents=True, exist_ok=True)

        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>ì „ë¼ì¢Œìˆ˜êµ° ê²¬ë¦¬ì‚¬ì˜ êµ°ë ¹ â€” í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸</title>
    <style>
        body {{ font-family: 'Noto Sans KR', sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; }}
        h1 {{ color: #2c3e50; }}
        .passed {{ color: #27ae60; }}
        .failed {{ color: #e74c3c; }}
        .test-module {{ border: 1px solid #ddd; padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .summary {{ background: #f8f9fa; padding: 20px; border-radius: 5px; }}
        pre {{ background: #2d2d2d; color: #f8f8f2; padding: 15px; overflow-x: auto; }}
    </style>
</head>
<body>
    <h1>ì „ë¼ì¢Œìˆ˜êµ° ê²¬ë¦¬ì‚¬ì˜ êµ°ë ¹ â€” í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸</h1>
    <p>ìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>

    <div class="summary">
        <h2>ìš”ì•½</h2>
        <p>í†µê³¼: {sum(1 for _, s, _ in self.results if s)} / {len(self.results)}</p>
    </div>

    <h2>ìƒì„¸ ê²°ê³¼</h2>
"""

        for module_key, success, output in self.results:
            module = TEST_MODULES[module_key]
            status_class = "passed" if success else "failed"
            status_icon = "âœ…" if success else "âŒ"

            html_content += f"""
    <div class="test-module">
        <h3 class="{status_class}">{status_icon} {module['name']} ({module['commander']})</h3>
        <p>{module['description']}</p>
        <details>
            <summary>ìƒì„¸ ì¶œë ¥</summary>
            <pre>{output}</pre>
        </details>
    </div>
"""

        html_content += """
</body>
</html>
"""

        output_path.write_text(html_content, encoding='utf-8')
        print(f"\nğŸ“„ HTML ë¦¬í¬íŠ¸ ìƒì„±: {output_path}")

        return output_path

    def generate_json_report(self, output_path: Optional[Path] = None) -> Path:
        """Generate JSON test report for CI integration"""
        if output_path is None:
            output_path = TEST_ROOT / "reports" / f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        output_path.parent.mkdir(parents=True, exist_ok=True)

        report = {
            "timestamp": datetime.now().isoformat(),
            "fleet_config": FLEET_CONFIG,
            "summary": {
                "total": len(self.results),
                "passed": sum(1 for _, s, _ in self.results if s),
                "failed": sum(1 for _, s, _ in self.results if not s)
            },
            "tests": []
        }

        for module_key, success, output in self.results:
            module = TEST_MODULES[module_key]
            report["tests"].append({
                "key": module_key,
                "name": module["name"],
                "commander": module["commander"],
                "priority": module["priority"],
                "success": success,
                "output_length": len(output)
            })

        output_path.write_text(json.dumps(report, indent=2, ensure_ascii=False), encoding='utf-8')
        print(f"ğŸ“„ JSON ë¦¬í¬íŠ¸ ìƒì„±: {output_path}")

        return output_path


# ============================================================================
# CONSISTENCY CHECK (Quick Pre-flight)
# ============================================================================

def run_consistency_check() -> bool:
    """
    Quick consistency check without full pytest

    Checks:
    1. VÂ² vs V(1-V) formula alignment
    2. P1/P2/P3 domain consistency
    3. Emoji consistency
    """
    print("\nğŸ” Quick Consistency Check")
    print("-" * 40)

    issues = []

    # Check chap2_theory.py for V(1-V)
    theory_file = EMPIRICS_ROOT / "src" / "scripts" / "paper_generation" / "chap2_theory.py"
    if theory_file.exists():
        content = theory_file.read_text()
        if "V(1-V)" in content:
            print("  âœ… H1 uses V(1-V) in theory")
        elif "VÂ²" in content or "V**2" in content:
            issues.append("Theory still uses old VÂ² formula")
            print("  âŒ Theory uses old VÂ² formula")
    else:
        print("  âš ï¸ chap2_theory.py not found")

    # Check models.py for U-shape term
    models_file = EMPIRICS_ROOT / "src" / "models.py"
    if models_file.exists():
        content = models_file.read_text()
        has_ushape = any(p in content for p in ["vagueness_ushape", "vagueness_squared", "**2"])
        if has_ushape:
            print("  âœ… models.py has U-shape term")
        else:
            issues.append("models.py missing U-shape term (vagueness_ushape or vagueness**2)")
            print("  âš ï¸ models.py missing U-shape term")
    else:
        print("  âš ï¸ models.py not found")

    # Check emoji consistency
    for chap_file in (EMPIRICS_ROOT / "src" / "scripts" / "paper_generation").glob("chap*.py"):
        content = chap_file.read_text()
        if "ğŸ•³ï¸" in content or "ğŸª¾" in content or "ğŸ§©" in content:  # Old emojis
            issues.append(f"{chap_file.name} still has old emoji (should be ğŸ¦¾ for P2, ğŸ¤¹ for P3)")
            print(f"  âŒ {chap_file.name} has old emoji")

    print("-" * 40)
    if issues:
        print(f"Found {len(issues)} consistency issue(s)")
        return False
    else:
        print("âœ… Quick consistency check passed")
        return True


# ============================================================================
# MAIN
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="ì „ë¼ì¢Œìˆ˜êµ° ê²¬ë¦¬ì‚¬ì˜ êµ°ë ¹ â€” í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    python run_all_tests.py              # Run all tests
    python run_all_tests.py --verbose    # Verbose output
    python run_all_tests.py --quick      # Stop on first failure
    python run_all_tests.py --check      # Quick consistency check only
    python run_all_tests.py --report     # Generate HTML report
    python run_all_tests.py --module hypothesis_consistency  # Run single module
        """
    )

    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Verbose output'
    )
    parser.add_argument(
        '--quick', '-q',
        action='store_true',
        help='Stop on first failure'
    )
    parser.add_argument(
        '--check', '-c',
        action='store_true',
        help='Quick consistency check only (no pytest)'
    )
    parser.add_argument(
        '--report', '-r',
        action='store_true',
        help='Generate HTML report after tests'
    )
    parser.add_argument(
        '--json',
        action='store_true',
        help='Generate JSON report (for CI)'
    )
    parser.add_argument(
        '--module', '-m',
        type=str,
        choices=list(TEST_MODULES.keys()),
        help='Run specific test module'
    )
    parser.add_argument(
        '--list', '-l',
        action='store_true',
        help='List available test modules'
    )

    args = parser.parse_args()

    # List modules
    if args.list:
        print("\nğŸ“‹ Available Test Modules:\n")
        for key, module in TEST_MODULES.items():
            print(f"  {key}")
            print(f"    Name: {module['name']}")
            print(f"    Commander: {module['commander']}")
            print(f"    Description: {module['description']}")
            print()
        return 0

    # Quick check only
    if args.check:
        success = run_consistency_check()
        return 0 if success else 1

    # Run tests
    runner = TestRunner(verbose=args.verbose, quick=args.quick)

    if args.module:
        # Run single module
        print_banner()
        success, _ = runner.run_single_test(args.module)
        runner.results.append((args.module, success, ""))
        runner.print_summary()
    else:
        # Run all tests
        success = runner.run_all_tests()

    # Generate reports
    if args.report:
        runner.generate_html_report()
    if args.json:
        runner.generate_json_report()

    return 0 if all(s for _, s, _ in runner.results) else 1


if __name__ == "__main__":
    sys.exit(main())
