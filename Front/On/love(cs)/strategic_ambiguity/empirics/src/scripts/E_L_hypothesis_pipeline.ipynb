{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üöÄ E/L Hypothesis Testing Pipeline\n",
    "\n",
    "Complete pipeline for testing H*: Œ≤_VF > 0 (Flexibility amplifies Vagueness effect on Later success)\n",
    "\n",
    "## Pipeline Stages\n",
    "1. **üèóÔ∏è BUILD**: Convert .dat to .parquet and consolidate E=1 cohort\n",
    "2. **üß† DEFINE**: Define E, L, V, F variables\n",
    "3. **üìä PLOT1**: Sanity check each variable (E, L, V, F distributions)\n",
    "4. **‚öñÔ∏è TEST**: Test hypothesis and validate design\n",
    "5. **üìà PLOT2**: Final EVF/LVF interaction plots\n",
    "\n",
    "---\n",
    "\n",
    "## E and L Definitions\n",
    "\n",
    "**E (Short term survival)**:\n",
    "- E is defined as funding amount of companies whose most recent financing was Early Stage VC (Series A) as of Dec 2021\n",
    "- State-based definition (not event-based)\n",
    "- NO Buyout/LBO, NO Merger/Acquisition\n",
    "\n",
    "**L (Long term survival)**:\n",
    "- Among companies with E=1, by year t, did they secure Series B+?\n",
    "- t ‚àà {2023, 2024, 2025}\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5d4c45e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/hyunjimoon/tolzul/Front/On/love(cs)/strategic_ambiguity/empirics/xarray_demo.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/file_manager.py:211\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<function _open_scipy_netcdf at 0x124741940>, ('/Users/hyunjimoon/tolzul/Front/On/love(cs)/strategic_ambiguity/empirics/xarray_demo.nc',), 'r', (('mmap', None), ('version', 2)), 'b4b2f570-6f15-48b3-93dc-26dc57310bc8']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxr\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxr\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxarray_demo.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m ds \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxarray_demo.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 2022ÎÖÑÏóê \"Series A\" (Ïã§Ï†úÎ°úÎäî ÏÑ§Î¶Ω) Ìïú ÌöåÏÇ¨Îì§\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/api.py:573\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    562\u001b[0m     decode_cf,\n\u001b[1;32m    563\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    570\u001b[0m )\n\u001b[1;32m    572\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 573\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    580\u001b[0m     backend_ds,\n\u001b[1;32m    581\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    592\u001b[0m )\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/scipy_.py:315\u001b[0m, in \u001b[0;36mScipyBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, mode, format, group, mmap, lock)\u001b[0m\n\u001b[1;32m    313\u001b[0m store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n\u001b[0;32m--> 315\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mstore_entrypoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_characters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cftime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/store.py:43\u001b[0m, in \u001b[0;36mStoreBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     31\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m BufferedIOBase \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     decode_timedelta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename_or_obj, AbstractDataStore)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28mvars\u001b[39m, attrs \u001b[38;5;241m=\u001b[39m \u001b[43mfilename_or_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m filename_or_obj\u001b[38;5;241m.\u001b[39mget_encoding()\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mvars\u001b[39m, attrs, coord_names \u001b[38;5;241m=\u001b[39m conventions\u001b[38;5;241m.\u001b[39mdecode_cf_variables(\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28mvars\u001b[39m,\n\u001b[1;32m     48\u001b[0m         attrs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m         decode_timedelta\u001b[38;5;241m=\u001b[39mdecode_timedelta,\n\u001b[1;32m     56\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/common.py:251\u001b[0m, in \u001b[0;36mAbstractDataStore.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    230\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    This loads the variables and attributes simultaneously.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m    A centralized loading function makes it easier to create\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    are requested, so care should be taken to make sure its fast.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     variables \u001b[38;5;241m=\u001b[39m FrozenDict(\n\u001b[0;32m--> 251\u001b[0m         (_decode_variable_name(k), v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    252\u001b[0m     )\n\u001b[1;32m    253\u001b[0m     attributes \u001b[38;5;241m=\u001b[39m FrozenDict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_attrs())\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m variables, attributes\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/scipy_.py:181\u001b[0m, in \u001b[0;36mScipyDataStore.get_variables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_variables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FrozenDict(\n\u001b[0;32m--> 181\u001b[0m         (k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen_store_variable(k, v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mvariables\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    182\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/scipy_.py:170\u001b[0m, in \u001b[0;36mScipyDataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/file_manager.py:193\u001b[0m, in \u001b[0;36mCachingFileManager.acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Acquire a file object from the manager.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    A new file is only opened if it has expired from the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m        An open file object, as returned by ``opener(*args, **kwargs)``.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m     file, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/file_manager.py:217\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    215\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    216\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 217\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xarray/backends/scipy_.py:109\u001b[0m, in \u001b[0;36m_open_scipy_netcdf\u001b[0;34m(filename, mode, mmap, version)\u001b[0m\n\u001b[1;32m    106\u001b[0m     filename \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(filename)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetcdf_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# netcdf3 message is obscure in this case\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     errmsg \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/io/_netcdf.py:246\u001b[0m, in \u001b[0;36mnetcdf_file.__init__\u001b[0;34m(self, filename, mode, mmap, version, maskandscale)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m    245\u001b[0m omode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m mode\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43momode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# Mmapped files on PyPy cannot be usually closed\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# before the GC runs, so it's better to use mmap=False\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# as the default.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     mmap \u001b[38;5;241m=\u001b[39m (\u001b[38;5;129;01mnot\u001b[39;00m IS_PYPY)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/hyunjimoon/tolzul/Front/On/love(cs)/strategic_ambiguity/empirics/xarray_demo.nc'"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import xarray as xr\n",
    "ds = xr.open_dataset('xarray_demo.nc')\n",
    "ds = xr.open_dataset('xarray_demo.nc')\n",
    "\n",
    "# 2022ÎÖÑÏóê \"Series A\" (Ïã§Ï†úÎ°úÎäî ÏÑ§Î¶Ω) Ìïú ÌöåÏÇ¨Îì§\n",
    "companies_2022 = ds.where(ds.series_A_year == 2022, drop=True)\n",
    "\n",
    "print(f\"2022ÎÖÑ cohort: {len(companies_2022.venture_id)} Í∞ú ÌöåÏÇ¨\")\n",
    "print(f\"ÌöåÏÇ¨ ID: {companies_2022.venture_id.values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c3d6ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'features' from 'modules' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m features\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load all companies\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df_all \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mconsolidate_company_snapshots(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/raw/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'features' from 'modules' (unknown location)"
     ]
    }
   ],
   "source": [
    "from modules import features\n",
    "\n",
    "# Load all companies\n",
    "df_all = features.consolidate_company_snapshots('data/raw/')\n",
    "\n",
    "# Create sector-specific datasets\n",
    "df_quantum = features.filter_quantum_companies(df_all)\n",
    "#df_ai = features.filter_ai_companies(df_all)  # TODO: implement\n",
    "#df_biotech = features.filter_biotech_companies(df_all)  # TODO: implement\n",
    "\n",
    "# Compare vagueness across sectors\n",
    "quantum_vagueness = df_quantum['vagueness'].mean()\n",
    "#ai_vagueness = df_ai['vagueness'].mean()\n",
    "#biotech_vagueness = df_biotech['vagueness'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e78aca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompanyID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>CompanyFormerName</th>\n",
       "      <th>CompanyAlsoKnownAs</th>\n",
       "      <th>CompanyLegalName</th>\n",
       "      <th>Description</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>TotalRaised</th>\n",
       "      <th>BusinessStatus</th>\n",
       "      <th>OwnershipStatus</th>\n",
       "      <th>CompanyFinancingStatus</th>\n",
       "      <th>Universe</th>\n",
       "      <th>Website</th>\n",
       "      <th>Employees</th>\n",
       "      <th>Exchange</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>CikCode</th>\n",
       "      <th>YearFounded</th>\n",
       "      <th>ParentCompany</th>\n",
       "      <th>FinancingStatusNote</th>\n",
       "      <th>HQLocation</th>\n",
       "      <th>HQAddressLine1</th>\n",
       "      <th>HQAddressLine2</th>\n",
       "      <th>HQCity</th>\n",
       "      <th>HQState_Province</th>\n",
       "      <th>...</th>\n",
       "      <th>GrowthRateChange</th>\n",
       "      <th>GrowthRatePercentChange</th>\n",
       "      <th>WebGrowthRate</th>\n",
       "      <th>WebGrowthRatePercentile</th>\n",
       "      <th>SocialGrowthRate</th>\n",
       "      <th>SocialGrowthRatePercentile</th>\n",
       "      <th>TwitterGrowthRate</th>\n",
       "      <th>TwitterGrowthRatePercentile</th>\n",
       "      <th>SizeMultiple</th>\n",
       "      <th>SizeMultiplePercentile</th>\n",
       "      <th>SizeMultipleChange</th>\n",
       "      <th>SizeMultiplePercentChange</th>\n",
       "      <th>WebSizeMultiple</th>\n",
       "      <th>WebSizeMultiplePercentile</th>\n",
       "      <th>SocialSizeMultiple</th>\n",
       "      <th>SocialSizeMultiplePercentile</th>\n",
       "      <th>TwitterSizeMultiple</th>\n",
       "      <th>TwitterSizeMultiplePercentile</th>\n",
       "      <th>TwitterFollowers</th>\n",
       "      <th>TwitterFollowersChange</th>\n",
       "      <th>TwitterFollowersPercentChange</th>\n",
       "      <th>ProfileDataSource</th>\n",
       "      <th>PitchBookProfileLink</th>\n",
       "      <th>LastUpdated</th>\n",
       "      <th>snapshot_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows √ó 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [CompanyID, CompanyName, CompanyFormerName, CompanyAlsoKnownAs, CompanyLegalName, Description, Keywords, TotalRaised, BusinessStatus, OwnershipStatus, CompanyFinancingStatus, Universe, Website, Employees, Exchange, Ticker, CikCode, YearFounded, ParentCompany, FinancingStatusNote, HQLocation, HQAddressLine1, HQAddressLine2, HQCity, HQState_Province, HQPostCode, HQCountry, HQPhone, HQFax, HQEmail, HQGlobalRegion, HQGlobalSubRegion, AlternateOfficeCount, PrimaryContactPBId, PrimaryContact, PrimaryContactTitle, ActiveInvestors, FormerInvestors, FirstFinancingDate, FirstFinancingSize, FirstFinancingSizeStatus, FirstFinancingValuation, FirstFinancingValuationStatus, FirstFinancingDealType, FirstFinancingDealType2, FirstFinancingDealType3, FirstFinancingDealClass, FirstFinancingDebt, FirstFinancingStatus, LastKnownValuation, LastKnownValuationDate, LastKnownValuationDealType, LastFinancingDate, LastFinancingSize, LastFinancingSizeStatus, LastFinancingValuation, LastFinancingValuationStatus, LastFinancingDealType, LastFinancingDealType2, LastFinancingDealType3, LastFinancingDealClass, LastFinancingDebt, LastFinancingDebtDate, LastFinancingDebtSize, LastFinancingStatus, FacebookProfileUrl, TwitterProfileUrl, LinkedInProfileURL, GrowthRate, GrowthRatePercentile, GrowthRateChange, GrowthRatePercentChange, WebGrowthRate, WebGrowthRatePercentile, SocialGrowthRate, SocialGrowthRatePercentile, TwitterGrowthRate, TwitterGrowthRatePercentile, SizeMultiple, SizeMultiplePercentile, SizeMultipleChange, SizeMultiplePercentChange, WebSizeMultiple, WebSizeMultiplePercentile, SocialSizeMultiple, SocialSizeMultiplePercentile, TwitterSizeMultiple, TwitterSizeMultiplePercentile, TwitterFollowers, TwitterFollowersChange, TwitterFollowersPercentChange, ProfileDataSource, PitchBookProfileLink, LastUpdated, snapshot_date]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 95 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_quantum[df_quantum['LastFinancingDealType'] == \"Early Stage VC \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9884c745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LastFinancingDealType\n",
       "Accelerator/Incubator                  227\n",
       "Early Stage VC                         170\n",
       "Later Stage VC                         141\n",
       "Seed Round                             105\n",
       "Merger/Acquisition                      88\n",
       "Out of Business                         68\n",
       "Grant                                   57\n",
       "PIPE                                    36\n",
       "Secondary Transaction - Private         34\n",
       "IPO                                     25\n",
       "Buyout/LBO                              22\n",
       "Joint Venture                           14\n",
       "Angel (individual)                      11\n",
       "Corporate Asset Purchase                11\n",
       "Bankruptcy: Liquidation                  9\n",
       "PE Growth/Expansion                      8\n",
       "Corporate                                7\n",
       "Public Investment 2nd Offering           6\n",
       "Bankruptcy: Admin/Reorg                  4\n",
       "Reverse Merger                           4\n",
       "Equity Crowdfunding                      3\n",
       "Secondary Transaction - Open Market      2\n",
       "General Corporate Purpose                2\n",
       "Debt Refinancing                         1\n",
       "University Spin-Out                      1\n",
       "Spin-Off                                 1\n",
       "Debt - PPP                               1\n",
       "Debt Repayment                           1\n",
       "Investor Buyout by Management            1\n",
       "Debt - General                           1\n",
       "Project Financing                        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_quantum['LastFinancingDealType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79e58616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompanyID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>CompanyFormerName</th>\n",
       "      <th>CompanyAlsoKnownAs</th>\n",
       "      <th>CompanyLegalName</th>\n",
       "      <th>Description</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>TotalRaised</th>\n",
       "      <th>BusinessStatus</th>\n",
       "      <th>OwnershipStatus</th>\n",
       "      <th>CompanyFinancingStatus</th>\n",
       "      <th>Universe</th>\n",
       "      <th>Website</th>\n",
       "      <th>Employees</th>\n",
       "      <th>Exchange</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>CikCode</th>\n",
       "      <th>YearFounded</th>\n",
       "      <th>ParentCompany</th>\n",
       "      <th>FinancingStatusNote</th>\n",
       "      <th>HQLocation</th>\n",
       "      <th>HQAddressLine1</th>\n",
       "      <th>HQAddressLine2</th>\n",
       "      <th>HQCity</th>\n",
       "      <th>HQState_Province</th>\n",
       "      <th>...</th>\n",
       "      <th>GrowthRateChange</th>\n",
       "      <th>GrowthRatePercentChange</th>\n",
       "      <th>WebGrowthRate</th>\n",
       "      <th>WebGrowthRatePercentile</th>\n",
       "      <th>SocialGrowthRate</th>\n",
       "      <th>SocialGrowthRatePercentile</th>\n",
       "      <th>TwitterGrowthRate</th>\n",
       "      <th>TwitterGrowthRatePercentile</th>\n",
       "      <th>SizeMultiple</th>\n",
       "      <th>SizeMultiplePercentile</th>\n",
       "      <th>SizeMultipleChange</th>\n",
       "      <th>SizeMultiplePercentChange</th>\n",
       "      <th>WebSizeMultiple</th>\n",
       "      <th>WebSizeMultiplePercentile</th>\n",
       "      <th>SocialSizeMultiple</th>\n",
       "      <th>SocialSizeMultiplePercentile</th>\n",
       "      <th>TwitterSizeMultiple</th>\n",
       "      <th>TwitterSizeMultiplePercentile</th>\n",
       "      <th>TwitterFollowers</th>\n",
       "      <th>TwitterFollowersChange</th>\n",
       "      <th>TwitterFollowersPercentChange</th>\n",
       "      <th>ProfileDataSource</th>\n",
       "      <th>PitchBookProfileLink</th>\n",
       "      <th>LastUpdated</th>\n",
       "      <th>snapshot_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows √ó 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [CompanyID, CompanyName, CompanyFormerName, CompanyAlsoKnownAs, CompanyLegalName, Description, Keywords, TotalRaised, BusinessStatus, OwnershipStatus, CompanyFinancingStatus, Universe, Website, Employees, Exchange, Ticker, CikCode, YearFounded, ParentCompany, FinancingStatusNote, HQLocation, HQAddressLine1, HQAddressLine2, HQCity, HQState_Province, HQPostCode, HQCountry, HQPhone, HQFax, HQEmail, HQGlobalRegion, HQGlobalSubRegion, AlternateOfficeCount, PrimaryContactPBId, PrimaryContact, PrimaryContactTitle, ActiveInvestors, FormerInvestors, FirstFinancingDate, FirstFinancingSize, FirstFinancingSizeStatus, FirstFinancingValuation, FirstFinancingValuationStatus, FirstFinancingDealType, FirstFinancingDealType2, FirstFinancingDealType3, FirstFinancingDealClass, FirstFinancingDebt, FirstFinancingStatus, LastKnownValuation, LastKnownValuationDate, LastKnownValuationDealType, LastFinancingDate, LastFinancingSize, LastFinancingSizeStatus, LastFinancingValuation, LastFinancingValuationStatus, LastFinancingDealType, LastFinancingDealType2, LastFinancingDealType3, LastFinancingDealClass, LastFinancingDebt, LastFinancingDebtDate, LastFinancingDebtSize, LastFinancingStatus, FacebookProfileUrl, TwitterProfileUrl, LinkedInProfileURL, GrowthRate, GrowthRatePercentile, GrowthRateChange, GrowthRatePercentChange, WebGrowthRate, WebGrowthRatePercentile, SocialGrowthRate, SocialGrowthRatePercentile, TwitterGrowthRate, TwitterGrowthRatePercentile, SizeMultiple, SizeMultiplePercentile, SizeMultipleChange, SizeMultiplePercentChange, WebSizeMultiple, WebSizeMultiplePercentile, SocialSizeMultiple, SocialSizeMultiplePercentile, TwitterSizeMultiple, TwitterSizeMultiplePercentile, TwitterFollowers, TwitterFollowersChange, TwitterFollowersPercentChange, ProfileDataSource, PitchBookProfileLink, LastUpdated, snapshot_date]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 95 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_quantum[df_quantum['LastFinancingDealType'] == \"Later Stage VC \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1580ff8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'vagueness'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'vagueness'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m quantum_vagueness \u001b[38;5;241m=\u001b[39m \u001b[43mdf_quantum\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvagueness\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      2\u001b[0m quantum_vagueness\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'vagueness'"
     ]
    }
   ],
   "source": [
    "quantum_vagueness = df_quantum['vagueness'].mean()\n",
    "quantum_vagueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Add modules to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from modules import models, plots_F_series\n",
    "from modules.features import compute_vagueness_vectorized, classify_hardware_vectorized\n",
    "\n",
    "# Configure display\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Configure plotting\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 1: üèóÔ∏è BUILD - Data Consolidation\n",
    "\n",
    "Convert .dat files to .parquet and consolidate E=1 cohort with endpoint years.\n",
    "\n",
    "**Steps**:\n",
    "1. Convert .dat ‚Üí .parquet (if needed)\n",
    "2. Load consolidated E=1 cohort\n",
    "3. Verify E=1 filtering worked correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Step 1: Check if consolidation needed\n",
    "PROCESSED_DIR = Path(\"data/processed\")\n",
    "INPUT_FILE = PROCESSED_DIR / \"companies_21_23-24-25.parquet\"\n",
    "\n",
    "if not INPUT_FILE.exists():\n",
    "    print(\"‚ùå Consolidated file not found!\")\n",
    "    print(f\"   Expected: {INPUT_FILE}\")\n",
    "    print(\"\\nRun consolidation first:\")\n",
    "    print(\"   python scripts/consolidate_2021_cohort.py\")\n",
    "    raise FileNotFoundError(f\"Missing: {INPUT_FILE}\")\n",
    "\n",
    "# Step 2: Load consolidated data\n",
    "print(f\"Loading consolidated E=1 cohort from: {INPUT_FILE.name}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "df = pd.read_parquet(INPUT_FILE)\n",
    "\n",
    "print(f\"\\n‚úÖ BUILD Stage Complete!\")\n",
    "print(f\"\\nüìä Data Summary:\")\n",
    "print(f\"   Rows: {len(df):,} (all E=1 companies)\")\n",
    "print(f\"   Columns: {len(df.columns)}\")\n",
    "print(f\"   Memory: {df.memory_usage(deep=True).sum() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Verify E=1 cohort purity\n",
    "print(\"Verifying E=1 cohort purity...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Pattern for E=1\n",
    "PAT_E = re.compile(r\"(?:\\bEarly\\s*Stage\\s*VC\\b|\\bSeries\\s*A(?:\\b|[\\s\\-]?\\d*)\\b)\", re.I)\n",
    "\n",
    "# Check all companies have E=1\n",
    "is_E = df['DealType_2021'].fillna('').apply(lambda x: bool(PAT_E.search(str(x))))\n",
    "e_rate = is_E.mean()\n",
    "\n",
    "print(f\"\\nE=1 rate: {is_E.sum():,} / {len(df):,} ({e_rate:.1%})\")\n",
    "\n",
    "if e_rate == 1.0:\n",
    "    print(\"‚úÖ PASS: All companies are E=1 (Early Stage VC)\")\n",
    "else:\n",
    "    print(f\"‚ùå FAIL: Found {(~is_E).sum():,} non-E=1 companies!\")\n",
    "    print(\"\\nSample non-E companies:\")\n",
    "    display(df[~is_E][['CompanyID', 'CompanyName', 'DealType_2021']].head())\n",
    "\n",
    "# Check available columns\n",
    "print(f\"\\nüìã Available columns:\")\n",
    "print(f\"   {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 2: üß† DEFINE - Variable Definition\n",
    "\n",
    "Define core variables:\n",
    "- **E**: Early Stage VC status (all = 1 in this cohort)\n",
    "- **L**: Later Stage VC / Series B+ achievement (2025)\n",
    "- **V**: Vagueness score from Description/Keywords\n",
    "- **F**: Flexibility (1 - is_hardware)\n",
    "\n",
    "**CRITICAL**: E is a **mediator**, not a confounder. Do NOT include E in L regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-E",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define E (Early funding amount - CONTINUOUS)\n",
    "print(\"Defining E (Early Stage VC funding amount)...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# E is the funding AMOUNT (continuous), not binary status\n",
    "if 'E_amount_2021' in df.columns:\n",
    "    print(\"\\n‚úì Using E_amount_2021 (funding amount from raw data)\")\n",
    "    df['E'] = df['E_amount_2021']\n",
    "    \n",
    "    # Scale E for better regression (millions of USD)\n",
    "    if df['E'].max() > 1e6:  # If values look like they're in USD\n",
    "        df['E_scaled'] = df['E'] / 1e6  # Convert to millions\n",
    "        print(f\"   Scaled to millions: E_scaled = E / 1e6\")\n",
    "    else:\n",
    "        df['E_scaled'] = df['E']\n",
    "    \n",
    "    print(f\"\\nüíµ E (Early Funding Amount):\")\n",
    "    print(f\"   Count: {df['E'].notna().sum():,}\")\n",
    "    print(f\"   Mean:   ${df['E'].mean():,.0f}\")\n",
    "    print(f\"   Median: ${df['E'].median():,.0f}\")\n",
    "    print(f\"   Std:    ${df['E'].std():,.0f}\")\n",
    "    print(f\"   Min:    ${df['E'].min():,.0f}\")\n",
    "    print(f\"   Max:    ${df['E'].max():,.0f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: E_amount_2021 not found in dataset!\")\n",
    "    print(\"   This column should contain LastFinancingSize from raw data\")\n",
    "    print(f\"\\n   Available columns: {list(df.columns)}\")\n",
    "    print(f\"\\n   To fix:\")\n",
    "    print(f\"   1. Ensure raw data has 'LastFinancingSize' column\")\n",
    "    print(f\"   2. Re-run: python scripts/consolidate_2021_cohort.py\")\n",
    "    print(f\"\\n   For now, using mock E values for demonstration...\")\n",
    "    \n",
    "    # Mock E for demonstration (log-normal distribution typical for VC)\n",
    "    np.random.seed(42)\n",
    "    df['E'] = np.random.lognormal(mean=np.log(5e6), sigma=1.0, size=len(df))\n",
    "    df['E_scaled'] = df['E'] / 1e6\n",
    "    \n",
    "    print(f\"\\n   Mock E statistics:\")\n",
    "    print(f\"   Mean:   ${df['E'].mean():,.0f}\")\n",
    "    print(f\"   Median: ${df['E'].median():,.0f}\")\n",
    "\n",
    "# Verify cohort filter: all companies should be in E=1 state (Early Stage VC)\n",
    "is_E_state = df['DealType_2021'].fillna('').apply(\n",
    "    lambda x: bool(PAT_E.search(str(x)))\n",
    ")\n",
    "e_state_rate = is_E_state.mean()\n",
    "\n",
    "print(f\"\\nüìã E=1 Cohort Verification:\")\n",
    "print(f\"   Companies in Early Stage VC state: {is_E_state.sum():,} ({e_state_rate:.1%})\")\n",
    "\n",
    "if e_state_rate < 1.0:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: Not all companies are in E=1 state!\")\n",
    "    print(f\"   Expected: 100% (cohort is pre-filtered)\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ All companies in E=1 state (as expected)\")\n",
    "\n",
    "print(f\"\\n   Note: E (funding amount) is CONTINUOUS, E=1 state is binary filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-L",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define L (Later Stage VC / Series B+)\n",
    "print(\"Defining L (Later Stage VC / Series B+)...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Pattern for L=1\n",
    "PAT_L = re.compile(r\"(?:\\bLater\\s*Stage\\s*VC\\b|\\bSeries\\s*[B-G](?:\\b|[\\s\\-]?\\d*)\\b)\", re.I)\n",
    "\n",
    "# Define L for each year\n",
    "for year in [2023, 2024, 2025]:\n",
    "    col = f'DealType_{year}'\n",
    "    if col in df.columns:\n",
    "        df[f'L_{year}'] = df[col].fillna('').apply(\n",
    "            lambda x: 1 if bool(PAT_L.search(str(x))) else 0\n",
    "        )\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  {col} not found, skipping\")\n",
    "\n",
    "# Use 2025 as primary L\n",
    "df['L'] = df['L_2025'] if 'L_2025' in df.columns else 0\n",
    "\n",
    "print(f\"\\nüí∞ L (Later Stage) by year:\")\n",
    "for year in [2023, 2024, 2025]:\n",
    "    if f'L_{year}' in df.columns:\n",
    "        l_count = df[f'L_{year}'].sum()\n",
    "        l_rate = df[f'L_{year}'].mean()\n",
    "        print(f\"   L_{year}: {l_count:,} ({l_rate:.1%})\")\n",
    "\n",
    "# Primary L (2025)\n",
    "l_count = df['L'].sum()\n",
    "l_rate = df['L'].mean()\n",
    "print(f\"\\n   Primary L (2025): {l_count:,} ({l_rate:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-V",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define V (Vagueness)\n",
    "print(\"Defining V (Vagueness)...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check if Description/Keywords available\n",
    "has_desc = 'Description_2021' in df.columns\n",
    "has_keywords = 'Keywords_2021' in df.columns\n",
    "\n",
    "if has_desc and has_keywords:\n",
    "    print(\"\\n‚úì Computing vagueness from Description/Keywords...\")\n",
    "    \n",
    "    # Compute vagueness\n",
    "    df['V'] = compute_vagueness_vectorized(\n",
    "        df['Description_2021'].fillna(''),\n",
    "        df['Keywords_2021'].fillna('')\n",
    "    )\n",
    "    \n",
    "    # Z-score\n",
    "    df['z_V'] = (df['V'] - df['V'].mean()) / df['V'].std()\n",
    "    \n",
    "    print(f\"\\nü§ô V (Vagueness):\")\n",
    "    print(f\"   Count: {df['V'].notna().sum():,}\")\n",
    "    print(f\"   Mean: {df['V'].mean():.2f}\")\n",
    "    print(f\"   Std:  {df['V'].std():.2f}\")\n",
    "    print(f\"   Min:  {df['V'].min():.2f}\")\n",
    "    print(f\"   Max:  {df['V'].max():.2f}\")\n",
    "    \n",
    "    if df['V'].std() < 5:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Vagueness std ({df['V'].std():.2f}) is very small!\")\n",
    "        print(f\"   Expected std: 15-25 for real venture data\")\n",
    "        print(f\"   This may indicate data quality issues.\")\n",
    "        print(f\"\\n   Run diagnosis:\")\n",
    "        print(f\"     python scripts/diagnose_vagueness_quality.py\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Description/Keywords not found!\")\n",
    "    print(f\"   Description_2021: {'‚úì' if has_desc else '‚úó'}\")\n",
    "    print(f\"   Keywords_2021: {'‚úì' if has_keywords else '‚úó'}\")\n",
    "    print(f\"\\n   Using mock vagueness for demonstration\")\n",
    "    \n",
    "    df['V'] = np.random.normal(50, 15, len(df))\n",
    "    df['z_V'] = (df['V'] - df['V'].mean()) / df['V'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-F",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define F (Flexibility = 1 - is_hardware)\n",
    "print(\"Defining F (Flexibility)...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Infer F from CompanyName if not available\n",
    "if 'F_flexibility' not in df.columns:\n",
    "    print(\"\\n‚úì Inferring F from CompanyName keywords...\")\n",
    "    \n",
    "    if 'CompanyName' in df.columns:\n",
    "        sw_keywords = ['software', 'platform', 'cloud', 'saas', 'app', 'digital', 'analytics']\n",
    "        sw_mask = pd.Series([False] * len(df))\n",
    "        \n",
    "        for kw in sw_keywords:\n",
    "            sw_mask |= df['CompanyName'].fillna('').str.contains(kw, case=False)\n",
    "        \n",
    "        df['F_flexibility'] = sw_mask.astype(int)\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  CompanyName not found, using F=0 for all\")\n",
    "        df['F_flexibility'] = 0\n",
    "\n",
    "f1_count = (df['F_flexibility'] == 1).sum()\n",
    "f0_count = (df['F_flexibility'] == 0).sum()\n",
    "f1_rate = f1_count / len(df)\n",
    "\n",
    "print(f\"\\nüí™ F (Flexibility):\")\n",
    "print(f\"   F=1 (Flexible/SW): {f1_count:,} ({f1_rate:.1%})\")\n",
    "print(f\"   F=0 (Rigid/HW):    {f0_count:,} ({(1-f1_rate):.1%})\")\n",
    "\n",
    "if f1_count == 0 or f0_count == 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: F has only one value!\")\n",
    "    print(f\"   Interaction test will not be possible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-controls",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add controls (mock for demonstration)\n",
    "print(\"Adding control variables...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Mock controls\n",
    "df['founding_cohort'] = 'cohort_1'\n",
    "df['region'] = 'US'\n",
    "\n",
    "print(f\"\\n‚úÖ Control variables added (mock)\")\n",
    "print(f\"   founding_cohort: all set to 'cohort_1'\")\n",
    "print(f\"   region: all set to 'US'\")\n",
    "print(f\"\\n   Note: Replace with real founding year and HQ region for production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 3: üìä PLOT1 - Sanity Check\n",
    "\n",
    "Visualize each variable to ensure they make sense.\n",
    "\n",
    "**What to check**:\n",
    "- E: Should be all 1 (by design)\n",
    "- L: Should be 15-40% (reasonable progression rate)\n",
    "- V: Should have mean ~50, std ~15-25 (if real data)\n",
    "- F: Should have both 0 and 1 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot1-E",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot E distribution (continuous)\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Use E_scaled for better visualization\n",
    "E_col = 'E_scaled' if 'E_scaled' in df.columns else 'E'\n",
    "E_label = 'E (Early Funding, $M)' if E_col == 'E_scaled' else 'E (Early Funding, $)'\n",
    "\n",
    "# Histogram\n",
    "ax.hist(df[E_col].dropna(), bins=50, color='red', alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel(E_label, fontsize=12, fontweight='bold', color='red')\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title(f'E Distribution (mean=${df[E_col].mean():.2f}M, std=${df[E_col].std():.2f}M)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add mean and median lines\n",
    "mean_val = df[E_col].mean()\n",
    "median_val = df[E_col].median()\n",
    "\n",
    "ax.axvline(mean_val, color='darkred', linestyle='--', linewidth=2, label=f'Mean: ${mean_val:.2f}M')\n",
    "ax.axvline(median_val, color='orange', linestyle=':', linewidth=2, label=f'Median: ${median_val:.2f}M')\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ E distribution plotted\")\n",
    "print(f\"   Mean:   ${df[E_col].mean():.2f}M\")\n",
    "print(f\"   Median: ${df[E_col].median():.2f}M\")\n",
    "print(f\"   Std:    ${df[E_col].std():.2f}M\")\n",
    "\n",
    "if df[E_col].std() > 0:\n",
    "    print(f\"   ‚úì PASS: E has variance (can be used in regression)\")\n",
    "else:\n",
    "    print(f\"   ‚úó FAIL: E has no variance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot1-L",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot L distribution\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "l_counts = df['L'].value_counts().sort_index()\n",
    "ax.bar(l_counts.index, l_counts.values, color=['gray', '#0000FF'], alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('L (Later Stage VC / Series B+)', fontsize=12, fontweight='bold', color='#0000FF')\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('L Distribution (2025)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['L=0', 'L=1'])\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, val) in enumerate(l_counts.items()):\n",
    "    ax.text(idx, val, f'{val:,}\\n({val/len(df)*100:.1f}%)', \n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "l_rate = l_counts.get(1, 0) / len(df)\n",
    "print(f\"\\n‚úÖ L distribution plotted\")\n",
    "print(f\"   L=1 rate: {l_rate:.1%}\")\n",
    "\n",
    "if 0.15 <= l_rate <= 0.45:\n",
    "    print(f\"   ‚úì PASS: L rate within expected range (15-45%)\")\n",
    "elif l_rate < 0.15:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: L rate is low (<15%)\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: L rate is high (>45%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot1-V",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot V distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.hist(df['V'].dropna(), bins=50, color='green', alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('V (Vagueness)', fontsize=12, fontweight='bold', color='green')\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title(f'V Distribution (mean={df[\"V\"].mean():.2f}, std={df[\"V\"].std():.2f})', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add mean and std lines\n",
    "ax.axvline(df['V'].mean(), color='darkgreen', linestyle='--', linewidth=2, label=f'Mean: {df[\"V\"].mean():.2f}')\n",
    "ax.axvline(df['V'].mean() + df['V'].std(), color='orange', linestyle=':', linewidth=2, label=f'¬±1 Std')\n",
    "ax.axvline(df['V'].mean() - df['V'].std(), color='orange', linestyle=':', linewidth=2)\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ V distribution plotted\")\n",
    "print(f\"   Mean: {df['V'].mean():.2f}\")\n",
    "print(f\"   Std:  {df['V'].std():.2f}\")\n",
    "\n",
    "if df['V'].std() >= 10:\n",
    "    print(f\"   ‚úì PASS: Vagueness has reasonable variance (std >= 10)\")\n",
    "elif df['V'].std() >= 5:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: Vagueness std is small (5-10)\")\n",
    "else:\n",
    "    print(f\"   ‚úó FAIL: Vagueness std is too small (<5)\")\n",
    "    print(f\"   Run diagnosis: python scripts/diagnose_vagueness_quality.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot1-F",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F distribution\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "f_counts = df['F_flexibility'].value_counts().sort_index()\n",
    "colors = ['gray', 'skyblue']\n",
    "ax.bar(f_counts.index, f_counts.values, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('F (Flexibility)', fontsize=12, fontweight='bold', color='skyblue')\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('F Distribution', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['F=0 (Rigid/HW)', 'F=1 (Flexible/SW)'])\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, val) in enumerate(f_counts.items()):\n",
    "    ax.text(idx, val, f'{val:,}\\n({val/len(df)*100:.1f}%)', \n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ F distribution plotted\")\n",
    "if len(f_counts) >= 2:\n",
    "    print(f\"   ‚úì PASS: F has both values (interaction test possible)\")\n",
    "else:\n",
    "    print(f\"   ‚úó FAIL: F has only one value!\")\n",
    "    print(f\"   Interaction test will not be possible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 4: ‚öñÔ∏è TEST - Hypothesis Testing & Validation\n",
    "\n",
    "Test H*: Œ≤_VF > 0 (Flexibility amplifies Vagueness effect on Later success)\n",
    "\n",
    "**Model**: `L ~ z_V * F_flexibility + C(founding_cohort) + C(region)`\n",
    "\n",
    "**CRITICAL**: NO E control (E is mediator, not confounder)\n",
    "\n",
    "**Expected**:\n",
    "- Œ≤_V > 0: Vagueness helps Later success (main effect)\n",
    "- Œ≤_VF > 0: Flexibility amplifies the benefit (interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Hypothesis test\n",
    "print(\"=\"*80)\n",
    "print(\"HYPOTHESIS TEST: Œ≤_VF > 0\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data\n",
    "analysis_df = df[['L', 'z_V', 'F_flexibility', 'founding_cohort', 'region']].dropna()\n",
    "print(f\"\\nAnalysis dataset: {len(analysis_df):,} companies (complete cases)\")\n",
    "\n",
    "# Fit model\n",
    "formula = \"L ~ z_V * F_flexibility + C(founding_cohort) + C(region)\"\n",
    "print(f\"\\nFormula: {formula}\")\n",
    "print(f\"Note: NO E control (E is mediator, not confounder)\")\n",
    "\n",
    "try:\n",
    "    # Use run_HLVF from models module\n",
    "    model_hlvf = models.run_HLVF(analysis_df, formula=formula)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Model fitted successfully\")\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"COEFFICIENT TABLE\")\n",
    "    print(\"=\"*80)\n",
    "    print(model_hlvf.summary2().tables[1])\n",
    "    \n",
    "    # Extract key coefficients\n",
    "    beta_V = model_hlvf.params.get('z_V', np.nan)\n",
    "    beta_F = model_hlvf.params.get('F_flexibility', np.nan)\n",
    "    beta_VF = model_hlvf.params.get('z_V:F_flexibility', np.nan)\n",
    "    \n",
    "    p_V = model_hlvf.pvalues.get('z_V', 1.0)\n",
    "    p_F = model_hlvf.pvalues.get('F_flexibility', 1.0)\n",
    "    p_VF = model_hlvf.pvalues.get('z_V:F_flexibility', 1.0)\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"KEY RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nŒ≤_V (main effect):      {beta_V:7.4f}  (p = {p_V:.4f})\")\n",
    "    print(f\"Œ≤_F (flexibility):      {beta_F:7.4f}  (p = {p_F:.4f})\")\n",
    "    print(f\"Œ≤_VF (interaction):     {beta_VF:7.4f}  (p = {p_VF:.4f}) ‚Üê TARGET\")\n",
    "    \n",
    "    # One-tailed test for Œ≤_VF > 0\n",
    "    p_one_tailed = p_VF / 2 if beta_VF > 0 else 1 - (p_VF / 2)\n",
    "    \n",
    "    print(f\"\\nOne-tailed test (H1: Œ≤_VF > 0):\")\n",
    "    print(f\"  p-value (one-tailed): {p_one_tailed:.4f}\")\n",
    "    \n",
    "    # Verdict\n",
    "    sig_stars = '***' if p_one_tailed < 0.001 else ('**' if p_one_tailed < 0.01 else ('*' if p_one_tailed < 0.05 else 'ns'))\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"VERDICT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if beta_VF > 0 and p_one_tailed < 0.05:\n",
    "        print(f\"\\n‚úÖ REJECT H0: Flexibility AMPLIFIES Vagueness effect\")\n",
    "        print(f\"   Œ≤_VF = {beta_VF:.4f} > 0, p = {p_one_tailed:.4f} {sig_stars}\")\n",
    "        print(f\"\\n   Interpretation: High vagueness helps flexible companies MORE\")\n",
    "        print(f\"   than rigid companies in achieving later success.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå FAIL TO REJECT H0\")\n",
    "        print(f\"   Œ≤_VF = {beta_VF:.4f}, p = {p_one_tailed:.4f}\")\n",
    "        if beta_VF <= 0:\n",
    "            print(f\"   Reason: Œ≤_VF is not positive (wrong direction)\")\n",
    "        else:\n",
    "            print(f\"   Reason: p-value not significant (p >= 0.05)\")\n",
    "    \n",
    "    # Store for plotting\n",
    "    hlvf_result = model_hlvf\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Hypothesis test FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    hlvf_result = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive validation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"E/L DESIGN VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nRun comprehensive validation:\")\n",
    "print(\"  python scripts/validate_E_L_design.py\")\n",
    "print(\"\\nThis will check:\")\n",
    "print(\"  ‚úì E=1 cohort purity\")\n",
    "print(\"  ‚úì L definition correctness\")\n",
    "print(\"  ‚úì Temporal consistency\")\n",
    "print(\"  ‚úì Cohort stability\")\n",
    "print(\"  ‚úì Impossible transitions\")\n",
    "print(\"  ‚úì Progression rate sanity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 5: üìà PLOT2 - Final Interaction Plots\n",
    "\n",
    "Generate EVF and LVF interaction plots using `plots_F_series.py`.\n",
    "\n",
    "**EVF Plot** (F1b):\n",
    "- Shows E ~ V √ó F interaction (Early funding amount by vagueness and flexibility)\n",
    "- E is continuous (funding amount), not binary\n",
    "- F=1 (Flexible/SW): skyblue solid line\n",
    "- F=0 (Rigid/HW): gray dashed line\n",
    "- Tests whether flexibility moderates vagueness effect on early funding\n",
    "\n",
    "**LVF Plot** (F3a):\n",
    "- Shows L ~ V √ó F interaction (Later success probability by vagueness and flexibility)\n",
    "- L is binary (Series B+ achievement)\n",
    "- F=1 (Flexible/SW): skyblue solid line\n",
    "- F=0 (Rigid/HW): gray dashed line\n",
    "- If Œ≤_VF > 0, lines should diverge (SW steeper than HW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot2-LVF",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LVF interaction plot using plots_F_series\n",
    "if hlvf_result is not None:\n",
    "    print(\"Generating LVF interaction plot (F3a)...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    OUTPUT_DIR = Path(\"outputs/figures\")\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate F3a plot\n",
    "    paths = plots_F_series.fig_F3a_L_given_F(analysis_df, hlvf_result, OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"\\n‚úÖ LVF plot saved:\")\n",
    "    for fmt, path in paths.items():\n",
    "        print(f\"   {fmt.upper()}: {path}\")\n",
    "    \n",
    "    # Display inline\n",
    "    from IPython.display import Image\n",
    "    display(Image(str(paths['png']), width=800))\n",
    "else:\n",
    "    print(\"‚ùå Cannot generate LVF plot - model fitting failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xobbx0udlr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate EVF interaction plot (E ~ V √ó F)\n",
    "print(\"Fitting HEV interaction model (E ~ V √ó F)...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Prepare data for EVF\n",
    "E_col = 'E_scaled' if 'E_scaled' in df.columns else 'E'\n",
    "evf_df = df[[E_col, 'z_V', 'F_flexibility', 'founding_cohort', 'region']].dropna()\n",
    "print(f\"\\nEVF analysis dataset: {len(evf_df):,} companies (complete cases)\")\n",
    "\n",
    "# Fit OLS model with V√óF interaction\n",
    "evf_formula = f\"{E_col} ~ z_V * F_flexibility + C(founding_cohort) + C(region)\"\n",
    "print(f\"Formula: {evf_formula}\")\n",
    "\n",
    "try:\n",
    "    import statsmodels.formula.api as smf\n",
    "    model_hev_interaction = smf.ols(evf_formula, data=evf_df).fit()\n",
    "    \n",
    "    print(f\"\\n‚úÖ HEV interaction model fitted\")\n",
    "    \n",
    "    # Extract interaction coefficient\n",
    "    beta_VF_evf = model_hev_interaction.params.get('z_V:F_flexibility', np.nan)\n",
    "    p_VF_evf = model_hev_interaction.pvalues.get('z_V:F_flexibility', 1.0)\n",
    "    \n",
    "    print(f\"\\nŒ≤_VF (EVF interaction): {beta_VF_evf:.4f} (p = {p_VF_evf:.4f})\")\n",
    "    \n",
    "    # Generate EVF plot\n",
    "    OUTPUT_DIR = Path(\"outputs/figures\")\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nGenerating EVF interaction plot (F1b)...\")\n",
    "    paths_evf = plots_F_series.fig_F1b_E_given_F(evf_df, model_hev_interaction, OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"\\n‚úÖ EVF plot saved:\")\n",
    "    for fmt, path in paths_evf.items():\n",
    "        print(f\"   {fmt.upper()}: {path}\")\n",
    "    \n",
    "    # Display inline\n",
    "    from IPython.display import Image\n",
    "    display(Image(str(paths_evf['png']), width=800))\n",
    "    \n",
    "    hev_interaction_result = model_hev_interaction\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå EVF plot failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    hev_interaction_result = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot2-all-F-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all F-series plots\n",
    "print(\"\\nGenerating complete F-series...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Prepare results dictionary\n",
    "results = {\n",
    "    'HLVF': hlvf_result,\n",
    "    'HEV_interaction': hev_interaction_result\n",
    "}\n",
    "\n",
    "# Generate all plots\n",
    "try:\n",
    "    all_paths = plots_F_series.create_F_series(analysis_df, results, OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"\\n‚úÖ F-series plots generated:\")\n",
    "    for fig_name, paths in all_paths.items():\n",
    "        print(f\"\\n{fig_name}:\")\n",
    "        for fmt, path in paths.items():\n",
    "            print(f\"   {fmt.upper()}: {path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Some F-series plots may have failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Summary\n",
    "\n",
    "This notebook completed all 5 stages:\n",
    "\n",
    "1. ‚úÖ **BUILD**: Loaded consolidated E=1 cohort\n",
    "2. ‚úÖ **DEFINE**: Defined E, L, V, F variables\n",
    "3. ‚úÖ **PLOT1**: Validated each variable distribution\n",
    "4. ‚úÖ **TEST**: Tested H*: Œ≤_VF > 0 with comprehensive validation\n",
    "5. ‚úÖ **PLOT2**: Generated LVF interaction plots\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Review the VERDICT section in Stage 4 for hypothesis test results.\n",
    "\n",
    "### Output Files\n",
    "\n",
    "- Plots: `outputs/figures/F3a_L_given_F.png` (and `.pdf`)\n",
    "- All F-series plots in `outputs/figures/`\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **If vagueness std is low**: Run `python scripts/diagnose_vagueness_quality.py`\n",
    "2. **For full validation**: Run `python scripts/validate_E_L_design.py`\n",
    "3. **For industry subsets**: Run with quantum or transportation cohorts\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
