"""
src/data_prep.py
===================================
Step 06: Measurement & Data Preparation Factory

Transforms raw transaction data into a multi-dimensional Xarray cube (panel_v3.nc).
Key Logic:
  1. Decomposes Vagueness into V_market (Entropy) and V_tech (Abstractness).
  2. Computes Founder Credibility (Serial Entrepreneurship) with caching.
  3. Builds Time-Varying Outcomes (L, S) across horizons [24, 36, 48].
  4. Identifies Moderators (F: is_software, Sector).
"""

from pathlib import Path
import pandas as pd
import numpy as np
import xarray as xr
import logging
import re
from typing import Optional, Tuple, List

# ë¡œì»¬ ëª¨ë“ˆ ìž„í¬íŠ¸ (src íŒ¨í‚¤ì§€ ë‚´)
from .definitions import (
    MARKET_KEYWORDS, TECH_SPEC_PATTERNS, CONSTANTS,
    ARCH_SOFTWARE_KEYWORDS, ARCH_HARDWARE_KEYWORDS,
    NANDA_SECTOR_TO_CODE
)
from .vagueness_v3 import StrategicVaguenessScorerV3
from .cache_manager import CacheManager

logger = logging.getLogger(__name__)

# =============================================================================
# 1. Helper Functions (Data Cleaning)
# =============================================================================

def _read_raw(input_path: str) -> pd.DataFrame:
    path = Path(input_path)
    if not path.exists():
        raise FileNotFoundError(f"Input file not found: {path}")
    
    if path.suffix == '.parquet':
        return pd.read_parquet(path)
    return pd.read_csv(path, low_memory=False)

def _standardize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """PitchBook ë“± ë²¤ë”ë³„ ì»¬ëŸ¼ëª…ì„ í‘œì¤€í™”"""
    rename_map = {
        'CompanyID': 'company_id', 'Description': 'description', 'Keywords': 'keywords',
        'DealDate': 'deal_date', 'DealAmount': 'deal_amount', 
        'PrimaryIndustryCode': 'industry_code', 'PreMoneyValuation': 'premoney',
        'PostMoneyValuation': 'postmoney', 'YearFounded': 'year_founded',
        'PrimaryContactPBId': 'person_id', 'PrimaryContactTitle': 'person_title'
    }
    # ëŒ€ì†Œë¬¸ìž ë¬´ì‹œ ë§¤í•‘ì„ ìœ„í•´ ì»¬ëŸ¼ëª… ì •ê·œí™” ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
    df = df.rename(columns=rename_map)
    
    # ë‚ ì§œ ë³€í™˜
    if 'deal_date' in df.columns:
        df['deal_date'] = pd.to_datetime(df['deal_date'], errors='coerce')
    
    return df

def _infer_round_type(series: pd.Series) -> pd.Series:
    """ë¼ìš´ë“œ ëª…ì¹­ ì •ê·œí™” (Series A, B...)"""
    s = series.fillna('').astype(str).str.lower()
    out = pd.Series('Other', index=s.index)
    out[s.str.contains('seed|angel')] = 'Seed'
    out[s.str.contains('series a') | (s == 'a')] = 'A'
    out[s.str.contains('series b') | (s == 'b')] = 'B'
    out[s.str.contains(r'series [c-z]')] = 'Late'
    return out

# =============================================================================
# 2. Feature Engineering Logic
# =============================================================================

def _classify_is_software(text: str) -> int:
    """F (Exercisability) ì¸¡ì •: SW í‚¤ì›Œë“œê°€ HWë³´ë‹¤ ë§Žìœ¼ë©´ 1"""
    if not isinstance(text, str): return 0
    text = text.lower()
    sw_count = sum(1 for k in ARCH_SOFTWARE_KEYWORDS if k in text)
    hw_count = sum(1 for k in ARCH_HARDWARE_KEYWORDS if k in text)
    # Tie-breaker: SW ìš°ì„  (í˜„ëŒ€ ë²¤ì²˜ì˜ ê¸°ë³¸ ì†ì„±)
    return 1 if sw_count >= hw_count else 0

def _compute_founder_credibility(raw_df: pd.DataFrame, cache: CacheManager) -> pd.DataFrame:
    """
    C (Founder Credibility): ì¸ëª…ë¶€ êµ¬ì¶• ë° ìºì‹±
    """
    step_name = 'founder_features'
    scenario = 'all'
    
    # 1. ìºì‹œ í™•ì¸
    if cache.is_valid(step=step_name, scenario=scenario):
        logger.info("ðŸ“œ Founder DB found in cache. Loading...")
        return cache.load_step(step=step_name, scenario=scenario)

    # 2. ìºì‹œ ì—†ìœ¼ë©´ ê³„ì‚°
    logger.info("âš¡ Computing Founder Credibility (Expensive Operation)...")
    df = raw_df.copy()
    df['person_id'] = df['person_id'].fillna('UNKNOWN')
    
    # Founder/CEO í•„í„°ë§
    role_mask = df['person_title'].fillna('Founder').str.contains(
        r'Founder|Co-Founder|CEO|Chief Executive|President', case=False, regex=True
    )
    
    # ì°½ì—…ìž ë°ì´í„° ì¶”ì¶œ ë° ì •ë ¬
    founders = df.loc[role_mask & (df['person_id'] != 'UNKNOWN')].copy()
    founders = founders.sort_values(['person_id', 'year_founded'])
    
    # ì‹œì  ì˜ì¡´ì  Serial Count (ì´ì „ ì°½ì—… íšŸìˆ˜)
    founders['serial_count'] = founders.groupby('person_id').cumcount()
    founders['is_serial'] = (founders['serial_count'] >= 1).astype(int)
    
    # íšŒì‚¬ë³„ Max Credibility (ê³µë™ì°½ì—…ìž ì¤‘ í•œ ëª…ì´ë¼ë„ ê²½í—˜ ìžˆìœ¼ë©´ ì¸ì •)
    founder_db = founders.groupby('company_id')['is_serial'].max().reset_index()
    founder_db.columns = ['company_id', 'founder_credibility']
    
    # 3. ì €ìž¥
    cache.save_step(step=step_name, data=founder_db, scenario=scenario)
    return founder_db

def _compute_outcomes_and_early_funding(df: pd.DataFrame, horizons: List[int]) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    E (Early Funding), L (Later Success), S (Step-up) ê³„ì‚°
    """
    df['norm_round'] = _infer_round_type(df['DealType'] if 'DealType' in df.columns else df['Round'])
    
    # Series A (Early Funding Baseline)
    series_a = df[df['norm_round'] == 'A'].sort_values('deal_date').drop_duplicates('company_id')
    series_a = series_a.set_index('company_id')
    
    # Outcomes (L, S) Dictionary
    outcomes = {
        'L_outcome': {},
        'S_stepup': {}
    }
    
    for h in horizons:
        # Horizon (ê°œì›”) ì´í›„ì˜ ë‚ ì§œ ê³„ì‚°
        cutoff_date = series_a['deal_date'] + pd.to_timedelta(h, unit='D') * 30.5
        
        # Series B ì´ìƒì´ë©´ì„œ Horizon ì´ë‚´ì¸ ë”œ ì°¾ê¸°
        later_rounds = df[
            (df['norm_round'].isin(['B', 'Late'])) & 
            (df['company_id'].isin(series_a.index))
        ].copy()
        
        # ê° íšŒì‚¬ë³„ë¡œ Horizon ì´ë‚´ì— ì„±ê³µí–ˆëŠ”ì§€ ì²´í¬
        # (ì´ ë¶€ë¶„ì€ ë²¡í„°í™”ê°€ ê¹Œë‹¤ë¡œì›Œ ë¡œì§ ì„¤ëª…ìš©ìœ¼ë¡œ ê°„ì†Œí™”í•¨. ì‹¤ì œ êµ¬í˜„ ì‹œ merge/filter ì‚¬ìš© ê¶Œìž¥)
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ í”Œëž˜ê·¸ ë¡œì§ ì˜ˆì‹œ:
        success_ids = []
        stepups = {}
        
        # (ì‹¤ì œë¡œëŠ” ë” ìµœì í™”ëœ íŒë‹¤ìŠ¤ ì—°ì‚° í•„ìš”)
        # ... [ìƒëžµ: ì¡°ì¸ ì—°ì‚°ìœ¼ë¡œ L, S ê³„ì‚°] ...
        # ë‚˜ëŒ€ìš©ì´ êµ¬í˜„í•  ì˜ì—­. ì—¬ê¸°ì„œëŠ” êµ¬ì¡°ë§Œ ìž¡ìŒ.
    
    # Placeholder for demonstration (ë‚˜ëŒ€ìš©ì´ ì±„ìš¸ ë¡œì§)
    # ì‹¤ì œë¡œëŠ” Series A ë°ì´í„°í”„ë ˆìž„(E)ê³¼ Outcome ë°ì´í„°í”„ë ˆìž„(L, S)ì„ ë¦¬í„´
    return series_a, pd.DataFrame() 


# =============================================================================
# 3. Main Pipeline Function
# =============================================================================

def build_panel_v3(
    input_parquet_path: str,
    output_netcdf_path: str = None,
    index_col: Optional[str] = 'company_id'
) -> xr.Dataset:
    """
    Build panel dataset v3: Parquet -> Xarray NetCDF with V-Decomposition.
    """
    # 1. Init Cache & Load Raw
    cache = CacheManager()
    raw = _read_raw(input_parquet_path)
    raw = _standardize_columns(raw)
    logger.info(f"ðŸ“‚ Raw Data Loaded: {len(raw):,} rows")

    # 2. Founder Credibility (Merge Cached DB)
    founder_db = _compute_founder_credibility(raw, cache)
    raw = raw.merge(founder_db, on='company_id', how='left')
    raw['founder_credibility'] = raw['founder_credibility'].fillna(0).astype('int8')

    # 3. Text Processing & V-Scoring (V3)
    logger.info("ðŸ§  Scoring Vagueness (V3: Market Entropy vs Tech Abstractness)...")
    
    # í…ìŠ¤íŠ¸ ê²°í•©
    raw['full_text'] = (raw['description'].fillna('') + ' ' + raw['keywords'].fillna('')).str.lower()
    
    # Scorer í˜¸ì¶œ
    scorer = StrategicVaguenessScorerV3()
    scorer.fit(raw['full_text']) # Density ë¶„í¬ í•™ìŠµ
    
    # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ Seriesë¡œ ì „ë‹¬
    v_scores = scorer.score_series(raw['full_text']) # DataFrame ë°˜í™˜
    
    # ê²°ê³¼ ë³‘í•©
    raw = pd.concat([raw, v_scores], axis=1)
    
    # 4. Moderators (F, Sector)
    raw['is_software'] = raw['full_text'].apply(_classify_is_software).astype('int8')
    
    # Nanda Sector Mapping (ê°„ì†Œí™”)
    raw['nanda_sector'] = raw['industry_code'].map(lambda x: 0).astype('int8') # TODO: ë§¤í•‘ ë¡œì§ ì—°ê²°

    # 5. Outcomes (Time-Varying L, S) & E (Static)
    # ë‹¨ìˆœí™”ë¥¼ ìœ„í•´, ì—¬ê¸°ì„œëŠ” Series Aê°€ ìžˆëŠ” ê¸°ì—…ë§Œ í•„í„°ë§í•˜ì—¬ íŒ¨ë„ì„ ë§Œë“­ë‹ˆë‹¤.
    # ì‹¤ì œ êµ¬í˜„ ì‹œ _compute_outcomes_and_early_funding ë¡œì§ì„ ì •êµí•˜ê²Œ ì§œì•¼ í•¨.
    
    # (ê°€ì •) ì´ë¯¸ ë²¤ì²˜ ë‹¨ìœ„ë¡œ ì •ë¦¬ëœ ë°ì´í„°ë¼ê³  ì¹©ë‹ˆë‹¤.
    # ì‹¤ì œë¡œëŠ” Transaction -> Venture ë³€í™˜ì´ í•„ìš”í•¨.
    venture_df = raw.drop_duplicates('company_id').set_index('company_id')
    
    # 6. Assemble Xarray Dataset
    logger.info("ðŸ“¦ Assembling Venture Cube (Xarray)...")
    
    venture_ids = venture_df.index.values
    horizons = CONSTANTS['HORIZONS']
    
    # ë°ì´í„° ë°°ì—´ ìƒì„± (N x H)
    # L_outcomeê³¼ S_stepupì€ ë³„ë„ ë¡œì§ìœ¼ë¡œ ì±„ì›Œì•¼ í•¨ (ì—¬ê¸°ì„œëŠ” 0/NaNìœ¼ë¡œ ì´ˆê¸°í™”)
    L_data = np.zeros((len(venture_ids), len(horizons)), dtype='int8')
    S_data = np.full((len(venture_ids), len(horizons)), np.nan, dtype='float32')
    
    ds = xr.Dataset(
        data_vars={
            # Predictors
            'V_market_entropy': (('venture_id'), venture_df['V_market_entropy'].values.astype('float32')),
            'V_tech_abstractness': (('venture_id'), venture_df['V_tech_abstractness'].values.astype('float32')),
            'V_composite': (('venture_id'), venture_df['V_composite'].values.astype('float32')),
            
            # Moderators
            'is_software': (('venture_id'), venture_df['is_software'].values.astype('int8')),
            'founder_credibility': (('venture_id'), venture_df['founder_credibility'].values.astype('int8')),
            'nanda_sector': (('venture_id'), venture_df['nanda_sector'].values.astype('int8')),
            
            # Outcomes (Time-varying)
            'L_outcome': (('venture_id', 'horizon'), L_data),
            'S_stepup': (('venture_id', 'horizon'), S_data),
        },
        coords={
            'venture_id': venture_ids,
            'horizon': horizons
        },
        attrs={
            'description': 'Panel V3: Decomposed Vagueness & Time-varying Outcomes',
            'source_file': str(input_parquet_path),
            'schema_version': '3.0'
        }
    )

    # 7. Save via CacheManager
    logger.info(f"ðŸ’¾ Saving Xarray Panel to Cache...")
    cache.save_xarray(step='panel_v3', dataset=ds, scenario='all')
    
    # ì‚¬ìš©ìžê°€ ìš”ì²­í•œ ê²½ë¡œì—ë„ ì €ìž¥
    if output_netcdf_path:
        output_path = Path(output_netcdf_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        ds.to_netcdf(output_path)
        logger.info(f"âœ“ Backup saved to: {output_path}")

    return ds

if __name__ == "__main__":
    # CLI ì‹¤í–‰ìš© (ì˜ˆì‹œ)
    import sys
    if len(sys.argv) > 1:
        build_panel_v3(sys.argv[1], sys.argv[2] if len(sys.argv) > 2 else None)