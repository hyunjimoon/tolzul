# Pipeline Architecture - Complete System Overview

## üèóÔ∏è System Architecture

```
empirics_ent_strat_ops/
‚îÇ
‚îú‚îÄ‚îÄ üìÇ src/ (Unified Analysis Package)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py      # Package initialization
‚îÇ   ‚îú‚îÄ‚îÄ features.py      # Data loading & feature engineering
‚îÇ   ‚îú‚îÄ‚îÄ models.py        # Statistical models (H1, H2)
‚îÇ   ‚îú‚îÄ‚îÄ vagueness_v2.py  # Vagueness scorer V2
‚îÇ   ‚îú‚îÄ‚îÄ plotting.py      # F-series visualization
‚îÇ   ‚îú‚îÄ‚îÄ empirical.py     # œÑ trajectory & xarray utilities
‚îÇ   ‚îú‚îÄ‚îÄ multiverse.py    # Specification curve analysis
‚îÇ   ‚îú‚îÄ‚îÄ cli.py           # Pipeline CLI (all 5 steps)
‚îÇ   ‚îî‚îÄ‚îÄ README.md        # Module documentation
‚îÇ
‚îú‚îÄ‚îÄ üìÇ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/             # PitchBook .dat files (gitignored)
‚îÇ   ‚îî‚îÄ‚îÄ processed/       # Parquet cache (auto-generated)
‚îÇ
‚îú‚îÄ‚îÄ üìÇ outputs/          # All results
‚îÇ   ‚îú‚îÄ‚îÄ all/             # Full dataset results
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/      # H1/H2 coefficients
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ figures/     # F3a plot
‚îÇ   ‚îú‚îÄ‚îÄ quantum/         # Quantum subset
‚îÇ   ‚îî‚îÄ‚îÄ transportation/  # Transportation subset
‚îÇ
‚îú‚îÄ‚îÄ üìÇ config/
‚îÇ   ‚îî‚îÄ‚îÄ datasets.yaml    # Dataset configurations
‚îÇ
‚îú‚îÄ‚îÄ üìÇ modules/ (ARCHIVED - don't use!)
‚îÇ   ‚îî‚îÄ‚îÄ ...              # Old duplicate code
‚îÇ
‚îú‚îÄ‚îÄ run_all.sh           # Master pipeline script
‚îî‚îÄ‚îÄ requirements.txt     # Python dependencies
```

## üîÑ Data Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  INPUT: PitchBook .dat files                                ‚îÇ
‚îÇ  ‚îú‚îÄ Company20210101.dat, Company20220101.dat, ...          ‚îÇ
‚îÇ  ‚îî‚îÄ Deal20230501.dat (if available)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 1: Load Data (python -m src.cli load-data)          ‚îÇ
‚îÇ  ‚îú‚îÄ src.features.consolidate_company_snapshots()           ‚îÇ
‚îÇ  ‚îú‚îÄ Merge all snapshots                                    ‚îÇ
‚îÇ  ‚îú‚îÄ Keep latest info per company                           ‚îÇ
‚îÇ  ‚îî‚îÄ Cache to parquet (10-50x speedup)                      ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  OUTPUT: data/processed/consolidated_companies.parquet     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 2: Engineer Features (python -m src.cli engineer-fe) ‚îÇ
‚îÇ  ‚îú‚îÄ src.vagueness_v2.StrategicVaguenessScorerV2           ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ Compute V_raw from Description + Keywords          ‚îÇ
‚îÇ  ‚îú‚îÄ src.features.engineer_features()                       ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ z_vagueness (standardized)                         ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ is_software (HW/SW classification)                 ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ z_employees_log                                    ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ sector_fe (sector dummies)                         ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ founding_cohort                                    ‚îÇ
‚îÇ  ‚îî‚îÄ Create survival/progression variables                  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  OUTPUT: data/processed/features_all.parquet               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 3: Filter Datasets (python -m src.cli filter-datasets) ‚îÇ
‚îÇ  ‚îú‚îÄ src.features.filter_quantum_companies()                ‚îÇ
‚îÇ  ‚îú‚îÄ src.features.filter_transportation_companies()         ‚îÇ
‚îÇ  ‚îî‚îÄ Create 3 variants:                                     ‚îÇ
‚îÇ      ‚îú‚îÄ outputs/all/dataset.parquet                        ‚îÇ
‚îÇ      ‚îú‚îÄ outputs/quantum/dataset.parquet                    ‚îÇ
‚îÇ      ‚îî‚îÄ outputs/transportation/dataset.parquet             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 4: Run Models (python -m src.cli run-models)        ‚îÇ
‚îÇ  ‚îú‚îÄ src.models.test_h1_early_funding()                     ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ E ~ V + controls (OLS)                             ‚îÇ
‚îÇ  ‚îú‚îÄ src.models.test_h2_main_growth()                       ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ L ~ V √ó F + controls (Logit)                       ‚îÇ
‚îÇ  ‚îî‚îÄ For each dataset (all, quantum, transportation)        ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  OUTPUT (per dataset):                                     ‚îÇ
‚îÇ  ‚îú‚îÄ models/h1_coefficients.csv                             ‚îÇ
‚îÇ  ‚îú‚îÄ models/h2_main_coefficients.csv                        ‚îÇ
‚îÇ  ‚îî‚îÄ models/h2_analysis_dataset.csv                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 5: Generate Plots (python -m src.cli generate-plots) ‚îÇ
‚îÇ  ‚îú‚îÄ src.plotting.fig_F3a_L_given_F()                       ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ Vagueness √ó Hardware interaction                   ‚îÇ
‚îÇ  ‚îî‚îÄ W2 color palette (Software=skyblue, HW=gray)           ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  OUTPUT (per dataset):                                     ‚îÇ
‚îÇ  ‚îú‚îÄ figures/F3a_interaction.png (300 DPI)                  ‚îÇ
‚îÇ  ‚îî‚îÄ figures/F3a_interaction.pdf (vector)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
                    ‚úÖ COMPLETE
```

## üìã Pipeline Steps Detail

### Step 1: Load Data
**CLI Command**: `python -m src.cli load-data` (src/cli.py)
**Module**: `src/features.py`
**Function**: `consolidate_company_snapshots()`

**What it does**:
1. Finds all `Company*.dat` files in `data/raw/`
2. Loads each snapshot with its date
3. Merges all snapshots chronologically
4. Keeps most recent info per company
5. Saves to parquet for 10-50x speedup

**Cache behavior**:
- First run: Parses .dat files (slow)
- Subsequent runs: Loads parquet (fast)
- Auto-detects cache existence

### Step 2: Engineer Features
**CLI Command**: `python -m src.cli engineer-features` (src/cli.py)
**Modules**: `src/features.py`, `src/vagueness_v2.py`
**Functions**: `engineer_features()`, `StrategicVaguenessScorerV2`

**What it does**:
1. Loads consolidated data
2. Computes vagueness scores (V2 algorithm)
3. Classifies HW vs SW from keywords
4. Creates control variables (employees, cohort, sector)
5. Standardizes all variables (z-scores)

**Key features created**:
- `z_vagueness`: Standardized vagueness score
- `is_software`: 1=Software/Flexible, 0=Hardware/Rigid
- `z_employees_log`: Log employees (standardized)
- `sector_fe`: Sector fixed effects
- `founding_cohort`: Cohort indicators

### Step 3: Filter Datasets
**CLI Command**: `python -m src.cli filter-datasets` (src/cli.py)
**Module**: `src/features.py`
**Functions**: `filter_quantum_companies()`, `filter_transportation_companies()`

**What it does**:
1. Loads feature-engineered data
2. Applies keyword filters for each dataset
3. Saves 3 variants

**Filters**:
- **Quantum**: "quantum", "qubit", "quantum computing", etc.
- **Transportation**: "autonomous", "mobility", "EV", etc.
- **All**: No filter (full dataset)

### Step 4: Run Models
**CLI Command**: `python -m src.cli run-models --dataset all` (src/cli.py)
**Module**: `src/models.py`
**Functions**: `test_h1_early_funding()`, `test_h2_main_growth()`

**What it does**:
1. Loads filtered dataset
2. Runs H1 (OLS): Early Funding ~ Vagueness
3. Runs H2 (Logit): Growth ~ Vagueness √ó Hardware
4. Saves coefficients and analysis dataset

**Models**:
- **H1**: `E ~ V + employees + cohort + sector`
- **H2**: `L ~ V √ó F + employees + cohort + sector`

**Robustness**: Multi-stage Logit fallback
1. Standard MLE (maxiter=100)
2. L1 regularization (Œ±=0.1)
3. Stronger L1 (Œ±=0.5)

### Step 5: Generate Plots
**CLI Command**: `python -m src.cli generate-plots --dataset all` (src/cli.py)
**Module**: `src/plotting.py`
**Function**: `fig_F3a_L_given_F()`

**What it does**:
1. Loads H2 analysis dataset
2. Generates interaction plot
3. Saves both PNG (300 DPI) and PDF (vector)

**F3a Plot** (THE MONEY PLOT):
- X-axis: Vagueness (green)
- Y-axis: P(Series B+) (blue)
- Lines:
  - Software (skyblue, solid): Positive/flat slope
  - Hardware (gray, dashed): Positive slope (diverging scissors)

## üîß Module Responsibilities

### src/features.py
**"Data Chef" - Prepares all ingredients**
- Load raw .dat files
- Merge snapshots
- Basic feature engineering
- HW/SW classification
- Dataset filtering

### src/vagueness_v2.py
**"Vagueness Calculator" - Specialized scorer**
- Strategic Vagueness Score computation
- Two-component algorithm:
  - S_cat: Categorical vagueness
  - S_concdef: Concreteness deficit
- Formula: `V = 0.5√ómax() + 0.5√ómean()`

### src/models.py
**"Statistical Engine" - Runs all tests**
- H1: OLS regression
- H2: Logit regression
- Formula management
- Robust convergence

### src/plotting.py
**"Artist" - Creates all figures**
- F-series plots (F1-F6)
- W2 color palette
- Interaction visualization
- Publication-ready output

### src/empirical.py
**"Strategist" - Advanced analysis**
- œÑ trajectory calculation
- xarray cohort tensor
- Productive/Destructive Ambiguity indices
- State-based cohort utilities

### src/multiverse.py
**"Defender" - Robustness checks**
- Specification grid generation
- Multiverse execution
- Spec curve visualization
- Sensitivity analysis

## üéØ Design Principles

### 1. **Separation of Concerns**
- `src/cli.py`: Pipeline orchestration CLI
- `src/`: All logic and formulas
- No analysis code in pipeline scripts

### 2. **DRY (Don't Repeat Yourself)**
- One function, one place
- No duplicate implementations
- Reusable across pipeline steps

### 3. **Modularity**
- Each step is standalone
- Can run individually
- Can skip steps (e.g., `--quick` mode)

### 4. **Caching Strategy**
- Step 1: Parquet cache (10-50x speedup)
- Step 2-5: Use cached features
- `--quick` mode skips Step 1

### 5. **Three Dataset Variants**
- All companies (baseline)
- Quantum (high-tech, high uncertainty)
- Transportation (regulatory constraints)
- Compare effect sizes across datasets

## üöÄ Usage Patterns

### Run Everything
```bash
./run_all.sh
```

### Quick Mode (Skip Step 1)
```bash
./run_all.sh --quick
```

### Single Dataset
```bash
./run_all.sh --dataset quantum
```

### Individual Steps
```bash
python -m src.cli load-data
python -m src.cli engineer-features
python -m src.cli filter-datasets
python -m src.cli run-models --dataset quantum
python -m src.cli generate-plots --dataset quantum
```

### Test Specific Module
```python
from features import consolidate_company_snapshots
from vagueness_v2 import StrategicVaguenessScorerV2
from models import test_h2_main_growth

# Use directly
df = consolidate_company_snapshots('data/raw')
scorer = StrategicVaguenessScorerV2()
result = test_h2_main_growth(df)
```

## üìä Expected Outputs

### File Structure After Full Run
```
outputs/
‚îú‚îÄ‚îÄ all/
‚îÇ   ‚îú‚îÄ‚îÄ dataset.parquet                  # Filtered data
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ h1_coefficients.csv         # H1 results
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ h2_main_coefficients.csv    # H2 results
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ h2_analysis_dataset.csv     # Data for plotting
‚îÇ   ‚îî‚îÄ‚îÄ figures/
‚îÇ       ‚îú‚îÄ‚îÄ F3a_interaction.png         # THE MONEY PLOT
‚îÇ       ‚îî‚îÄ‚îÄ F3a_interaction.pdf
‚îú‚îÄ‚îÄ quantum/
‚îÇ   ‚îî‚îÄ‚îÄ [same structure]
‚îî‚îÄ‚îÄ transportation/
    ‚îî‚îÄ‚îÄ [same structure]
```

### Output Files Per Dataset
- **1 dataset file**: Filtered companies
- **3 model files**: H1, H2 coefficients + analysis data
- **2 figure files**: PNG + PDF

**Total**: 6 files √ó 3 datasets = 18 files

## üîç Quality Checks

### After Each Step
1. **Step 1**: Check parquet file size > 100 MB
2. **Step 2**: Check `z_vagueness` column exists
3. **Step 3**: Check quantum dataset ~1,144 rows
4. **Step 4**: Check coefficient CSV files created
5. **Step 5**: Check F3a plot shows interaction

### Final Validation
```bash
python validate_pipeline.py
```

Checks:
- All output files exist
- Files have expected sizes
- Models converged
- Plots display correctly

## üìö Related Documentation

- **src/ Modules**: `src/README.md`
- **Pipeline CLI**: `src/cli.py` - run `python -m src.cli --help`
- **Testing**: `PIPELINE_TEST_GUIDE.md`
- **Vagueness Scorer**: `docs_archive/VAGUENESS_SCORER_V2_GUIDE.md`
- **xarray Design**: `XARRAY_DESIGN_README.md`
- **State-based Cohorts**: `STATE_BASED_COHORTS_README.md`

## üõ†Ô∏è Extension Points

### Adding Step 06: œÑ Trajectory
```python
# python -m src.cli trajectory (future implementation)
from empirical import calculate_tau_trajectory, prepare_cohort_tensor
from models import run_trajectory_model  # TODO: Add to src/models.py
from plotting import plot_tau_evolution  # TODO: Add to src/plotting.py
```

### Adding Step 07: Multiverse
```python
# python -m src.cli multiverse (future implementation)
from multiverse import build_spec_grid, run_spec_curve, plot_spec_curve
```

### Adding New Dataset Filter
```python
# In src/features.py
def filter_biotech_companies(df):
    keywords = ['biotech', 'pharmaceutical', 'therapeutics']
    return df[df['Keywords'].str.contains('|'.join(keywords), case=False, na=False)]

# In config/datasets.yaml
datasets:
  biotech:
    name: "Biotech & Healthcare"
    filter_function: "filter_biotech_companies"
    output_dir: "outputs/biotech"
```

---

**This architecture provides a clean, modular, production-ready pipeline for your thesis analysis.** üöÄ
