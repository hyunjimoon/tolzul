{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ğŸš€ E/L Hypothesis Testing Pipeline\n",
    "\n",
    "Complete pipeline for testing H*: Î²_VF > 0 (Flexibility amplifies Vagueness effect on Later success)\n",
    "\n",
    "## Pipeline Stages\n",
    "1. **ğŸ—ï¸ BUILD**: Convert .dat to .parquet and consolidate E=1 cohort\n",
    "2. **ğŸ§  DEFINE**: Define E, L, V, F variables\n",
    "3. **ğŸ“Š PLOT1**: Sanity check each variable (E, L, V, F distributions)\n",
    "4. **âš–ï¸ TEST**: Test hypothesis and validate design\n",
    "5. **ğŸ“ˆ PLOT2**: Final EVF/LVF interaction plots\n",
    "\n",
    "---\n",
    "\n",
    "## E and L Definitions\n",
    "\n",
    "**E (Short term survival)**:\n",
    "- E is defined as funding amount of companies whose most recent financing was Early Stage VC (Series A) as of Dec 2021\n",
    "- State-based definition (not event-based)\n",
    "- NO Buyout/LBO, NO Merger/Acquisition\n",
    "\n",
    "**L (Long term survival)**:\n",
    "- Among companies with E=1, by year t, did they secure Series B+?\n",
    "- t âˆˆ {2023, 2024, 2025}\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/psutil/_psutil_osx.cpython-310-darwin.so' could not be imported from '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/psutil/_psutil_osx.cpython-310-darwin.so, 0x0002'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Setup: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Add modules to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from modules import models, plots_F_series\n",
    "from modules.features import compute_vagueness_vectorized, classify_hardware_vectorized\n",
    "\n",
    "# Configure display\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Configure plotting\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4s951unyzai",
   "source": "---\n## Stage 0: ğŸ” DATA EXPLORATION (ë‚˜ëŒ€ìš© ì§€ì›ìš©)\n\nP1 \"Vagueness as Preserved Options\" ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° êµ¬ì¡° íƒìƒ‰.\n\n**íƒìƒ‰ ëª©í‘œ**:\n1. NetCDF / Parquet ë°ì´í„°ì˜ ì „ì²´ ì»¬ëŸ¼ êµ¬ì¡°\n2. **TechReadiness proxy** í›„ë³´ ì—´ í™•ì¸ (patent, tech keywords ë“±)\n3. Series B survival ë³€ìˆ˜ í™•ì¸\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gdnxo97mxiq",
   "source": "# =============================================================================\n# ğŸ” Stage 0: Data Exploration for P1 \"Vagueness as Preserved Options\"\n# =============================================================================\n# NetCDF íŒŒì¼ ê¸°ë°˜ íƒìƒ‰ (í„°ë¯¸ë„ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥)\n\nimport xarray as xr\nfrom pathlib import Path\n\nprint(\"=\"*80)\nprint(\"ğŸ” P1 DATA EXPLORATION: TechReadiness Proxy íƒìƒ‰ (NetCDF)\")\nprint(\"=\"*80)\n\n# ë°ì´í„° íŒŒì¼ ê²½ë¡œ\nPROCESSED_DIR = Path(\"data/processed\")\n\n# íƒìƒ‰í•  NetCDF íŒŒì¼ë“¤\nNC_FILES = [\n    \"features_all.nc\",\n    \"companies_21_23-24-25_transportation.nc\",\n    \"companies_21_23-24-25.nc\",\n    \"consolidated_companies.nc\",\n]\n\nfor fname in NC_FILES:\n    fpath = PROCESSED_DIR / fname\n    if fpath.exists():\n        print(f\"\\n{'='*80}\")\n        print(f\"ğŸ“ {fname}\")\n        print(\"=\"*80)\n        try:\n            ds = xr.open_dataset(fpath)\n            \n            # ê¸°ë³¸ ì •ë³´\n            print(f\"\\nğŸ“ DIMENSIONS: {dict(ds.dims)}\")\n            print(f\"ğŸ“ COORDINATES: {list(ds.coords)}\")\n            \n            # ëª¨ë“  ë°ì´í„° ë³€ìˆ˜ ì¶œë ¥\n            print(f\"\\nğŸ“‹ ALL DATA VARIABLES ({len(ds.data_vars)}):\")\n            for i, var in enumerate(ds.data_vars):\n                dtype = ds[var].dtype\n                dims = ds[var].dims\n                print(f\"   {i+1:3d}. {var:<40} | dims={dims} | dtype={dtype}\")\n            \n            # TechReadiness proxy í›„ë³´ ê²€ìƒ‰\n            print(f\"\\nğŸ” TECHREADINESS PROXY í›„ë³´:\")\n            tech_keywords = ['patent', 'tech', 'algorithm', 'proprietary', 'ip', \n                           'invention', 'license', 'r&d', 'research', 'hardware', 'software']\n            found_tech = False\n            for var in ds.data_vars:\n                var_lower = var.lower()\n                for kw in tech_keywords:\n                    if kw in var_lower:\n                        found_tech = True\n                        arr = ds[var].values\n                        print(f\"   âœ“ {var}\")\n                        try:\n                            import numpy as np\n                            if np.issubdtype(arr.dtype, np.number):\n                                valid = arr[~np.isnan(arr)] if np.issubdtype(arr.dtype, np.floating) else arr\n                                print(f\"     Stats: mean={np.mean(valid):.3f}, std={np.std(valid):.3f}, unique={len(np.unique(valid))}\")\n                            else:\n                                unique_vals = list(set(arr.flatten()))[:5]\n                                print(f\"     Sample values: {unique_vals}\")\n                        except:\n                            pass\n                        break\n            if not found_tech:\n                print(\"   (ì—†ìŒ - Text ê¸°ë°˜ proxy í•„ìš”)\")\n            \n            # Survival / Series B ê´€ë ¨ ë³€ìˆ˜ ê²€ìƒ‰\n            print(f\"\\nğŸ¯ SURVIVAL (Series B) ê´€ë ¨ ë³€ìˆ˜:\")\n            survival_keywords = ['series', 'stage', 'later', 'funding', 'deal', 'growth', 'L_', 'survival']\n            found_survival = False\n            for var in ds.data_vars:\n                var_lower = var.lower()\n                for kw in survival_keywords:\n                    if kw in var_lower:\n                        found_survival = True\n                        arr = ds[var].values\n                        print(f\"   âœ“ {var}\")\n                        try:\n                            import numpy as np\n                            if np.issubdtype(arr.dtype, np.number):\n                                valid = arr[~np.isnan(arr)] if np.issubdtype(arr.dtype, np.floating) else arr\n                                print(f\"     Stats: mean={np.mean(valid):.3f}, unique={len(np.unique(valid))}\")\n                        except:\n                            pass\n                        break\n            if not found_survival:\n                print(\"   (L_ ë³€ìˆ˜ ì—†ìŒ - DealTypeì—ì„œ íŒŒìƒ í•„ìš”)\")\n            \n            # Vagueness ê´€ë ¨ ë³€ìˆ˜\n            print(f\"\\nğŸŒ«ï¸ VAGUENESS ê´€ë ¨ ë³€ìˆ˜:\")\n            vague_keywords = ['vague', 'v_', 'z_v']\n            for var in ds.data_vars:\n                var_lower = var.lower()\n                for kw in vague_keywords:\n                    if kw in var_lower:\n                        arr = ds[var].values\n                        print(f\"   âœ“ {var}\")\n                        try:\n                            import numpy as np\n                            valid = arr[~np.isnan(arr)]\n                            print(f\"     Stats: mean={np.mean(valid):.3f}, std={np.std(valid):.3f}\")\n                        except:\n                            pass\n                        break\n            \n            # Attributes\n            if ds.attrs:\n                print(f\"\\nğŸ“ ATTRIBUTES:\")\n                for k, v in ds.attrs.items():\n                    print(f\"   {k}: {v}\")\n            \n            ds.close()\n            \n        except Exception as e:\n            print(f\"   âŒ Error: {e}\")\n    else:\n        print(f\"\\nâŒ {fname} - NOT FOUND\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"âœ… NetCDF ë°ì´í„° íƒìƒ‰ ì™„ë£Œ!\")\nprint(\"=\"*80)\nprint(\"\"\"\nğŸ“‹ ë‚˜ëŒ€ìš©(Claude)ì—ê²Œ ì´ ì¶œë ¥ ì „ì²´ë¥¼ ë³µì‚¬í•´ ì£¼ì„¸ìš”.\n   - TechReadiness proxy í›„ë³´ í™•ì¸\n   - Series B survival ë³€ìˆ˜ í™•ì¸  \n   - Vagueness ë³€ìˆ˜ í˜„í™© í™•ì¸\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "stage1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 1: ğŸ—ï¸ BUILD - Data Consolidation\n",
    "\n",
    "Convert .dat files to .parquet and consolidate E=1 cohort with endpoint years.\n",
    "\n",
    "**Steps**:\n",
    "1. Convert .dat â†’ .parquet (if needed)\n",
    "2. Load consolidated E=1 cohort\n",
    "3. Verify E=1 filtering worked correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Step 1: Check if consolidation needed\n",
    "PROCESSED_DIR = Path(\"data/processed\")\n",
    "INPUT_FILE = PROCESSED_DIR / \"companies_21_23-24-25.parquet\"\n",
    "\n",
    "if not INPUT_FILE.exists():\n",
    "    print(\"âŒ Consolidated file not found!\")\n",
    "    print(f\"   Expected: {INPUT_FILE}\")\n",
    "    print(\"\\nRun consolidation first:\")\n",
    "    print(\"   python scripts/consolidate_2021_cohort.py\")\n",
    "    raise FileNotFoundError(f\"Missing: {INPUT_FILE}\")\n",
    "\n",
    "# Step 2: Load consolidated data\n",
    "print(f\"Loading consolidated E=1 cohort from: {INPUT_FILE.name}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "df = pd.read_parquet(INPUT_FILE)\n",
    "\n",
    "print(f\"\\nâœ… BUILD Stage Complete!\")\n",
    "print(f\"\\nğŸ“Š Data Summary:\")\n",
    "print(f\"   Rows: {len(df):,} (all E=1 companies)\")\n",
    "print(f\"   Columns: {len(df.columns)}\")\n",
    "print(f\"   Memory: {df.memory_usage(deep=True).sum() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Verify E=1 cohort purity\n",
    "print(\"Verifying E=1 cohort purity...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Pattern for E=1\n",
    "PAT_E = re.compile(r\"(?:\\bEarly\\s*Stage\\s*VC\\b|\\bSeries\\s*A(?:\\b|[\\s\\-]?\\d*)\\b)\", re.I)\n",
    "\n",
    "# Check all companies have E=1\n",
    "is_E = df['DealType_2021'].fillna('').apply(lambda x: bool(PAT_E.search(str(x))))\n",
    "e_rate = is_E.mean()\n",
    "\n",
    "print(f\"\\nE=1 rate: {is_E.sum():,} / {len(df):,} ({e_rate:.1%})\")\n",
    "\n",
    "if e_rate == 1.0:\n",
    "    print(\"âœ… PASS: All companies are E=1 (Early Stage VC)\")\n",
    "else:\n",
    "    print(f\"âŒ FAIL: Found {(~is_E).sum():,} non-E=1 companies!\")\n",
    "    print(\"\\nSample non-E companies:\")\n",
    "    display(df[~is_E][['CompanyID', 'CompanyName', 'DealType_2021']].head())\n",
    "\n",
    "# Check available columns\n",
    "print(f\"\\nğŸ“‹ Available columns:\")\n",
    "print(f\"   {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 2: ğŸ§  DEFINE - Variable Definition\n",
    "\n",
    "Define core variables:\n",
    "- **E**: Early Stage VC status (all = 1 in this cohort)\n",
    "- **L**: Later Stage VC / Series B+ achievement (2025)\n",
    "- **V**: Vagueness score from Description/Keywords\n",
    "- **F**: Flexibility (1 - is_hardware)\n",
    "\n",
    "**CRITICAL**: E is a **mediator**, not a confounder. Do NOT include E in L regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-E",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define E (Early funding amount - CONTINUOUS)\n",
    "print(\"Defining E (Early Stage VC funding amount)...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# E is the funding AMOUNT (continuous), not binary status\n",
    "if 'E_amount_2021' in df.columns:\n",
    "    print(\"\\nâœ“ Using E_amount_2021 (funding amount from raw data)\")\n",
    "    df['E'] = df['E_amount_2021']\n",
    "    \n",
    "    # Scale E for better regression (millions of USD)\n",
    "    if df['E'].max() > 1e6:  # If values look like they're in USD\n",
    "        df['E_scaled'] = df['E'] / 1e6  # Convert to millions\n",
    "        print(f\"   Scaled to millions: E_scaled = E / 1e6\")\n",
    "    else:\n",
    "        df['E_scaled'] = df['E']\n",
    "    \n",
    "    print(f\"\\nğŸ’µ E (Early Funding Amount):\")\n",
    "    print(f\"   Count: {df['E'].notna().sum():,}\")\n",
    "    print(f\"   Mean:   ${df['E'].mean():,.0f}\")\n",
    "    print(f\"   Median: ${df['E'].median():,.0f}\")\n",
    "    print(f\"   Std:    ${df['E'].std():,.0f}\")\n",
    "    print(f\"   Min:    ${df['E'].min():,.0f}\")\n",
    "    print(f\"   Max:    ${df['E'].max():,.0f}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  WARNING: E_amount_2021 not found in dataset!\")\n",
    "    print(\"   This column should contain LastFinancingSize from raw data\")\n",
    "    print(f\"\\n   Available columns: {list(df.columns)}\")\n",
    "    print(f\"\\n   To fix:\")\n",
    "    print(f\"   1. Ensure raw data has 'LastFinancingSize' column\")\n",
    "    print(f\"   2. Re-run: python scripts/consolidate_2021_cohort.py\")\n",
    "    print(f\"\\n   For now, using mock E values for demonstration...\")\n",
    "    \n",
    "    # Mock E for demonstration (log-normal distribution typical for VC)\n",
    "    np.random.seed(42)\n",
    "    df['E'] = np.random.lognormal(mean=np.log(5e6), sigma=1.0, size=len(df))\n",
    "    df['E_scaled'] = df['E'] / 1e6\n",
    "    \n",
    "    print(f\"\\n   Mock E statistics:\")\n",
    "    print(f\"   Mean:   ${df['E'].mean():,.0f}\")\n",
    "    print(f\"   Median: ${df['E'].median():,.0f}\")\n",
    "\n",
    "# Verify cohort filter: all companies should be in E=1 state (Early Stage VC)\n",
    "is_E_state = df['DealType_2021'].fillna('').apply(\n",
    "    lambda x: bool(PAT_E.search(str(x)))\n",
    ")\n",
    "e_state_rate = is_E_state.mean()\n",
    "\n",
    "print(f\"\\nğŸ“‹ E=1 Cohort Verification:\")\n",
    "print(f\"   Companies in Early Stage VC state: {is_E_state.sum():,} ({e_state_rate:.1%})\")\n",
    "\n",
    "if e_state_rate < 1.0:\n",
    "    print(f\"   âš ï¸  WARNING: Not all companies are in E=1 state!\")\n",
    "    print(f\"   Expected: 100% (cohort is pre-filtered)\")\n",
    "else:\n",
    "    print(f\"   âœ… All companies in E=1 state (as expected)\")\n",
    "\n",
    "print(f\"\\n   Note: E (funding amount) is CONTINUOUS, E=1 state is binary filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-L",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define L (Later Stage VC / Series B+)\n",
    "print(\"Defining L (Later Stage VC / Series B+)...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Pattern for L=1\n",
    "PAT_L = re.compile(r\"(?:\\bLater\\s*Stage\\s*VC\\b|\\bSeries\\s*[B-G](?:\\b|[\\s\\-]?\\d*)\\b)\", re.I)\n",
    "\n",
    "# Define L for each year\n",
    "for year in [2023, 2024, 2025]:\n",
    "    col = f'DealType_{year}'\n",
    "    if col in df.columns:\n",
    "        df[f'L_{year}'] = df[col].fillna('').apply(\n",
    "            lambda x: 1 if bool(PAT_L.search(str(x))) else 0\n",
    "        )\n",
    "    else:\n",
    "        print(f\"   âš ï¸  {col} not found, skipping\")\n",
    "\n",
    "# Use 2025 as primary L\n",
    "df['L'] = df['L_2025'] if 'L_2025' in df.columns else 0\n",
    "\n",
    "print(f\"\\nğŸ’° L (Later Stage) by year:\")\n",
    "for year in [2023, 2024, 2025]:\n",
    "    if f'L_{year}' in df.columns:\n",
    "        l_count = df[f'L_{year}'].sum()\n",
    "        l_rate = df[f'L_{year}'].mean()\n",
    "        print(f\"   L_{year}: {l_count:,} ({l_rate:.1%})\")\n",
    "\n",
    "# Primary L (2025)\n",
    "l_count = df['L'].sum()\n",
    "l_rate = df['L'].mean()\n",
    "print(f\"\\n   Primary L (2025): {l_count:,} ({l_rate:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-V",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define V (Vagueness)\n",
    "print(\"Defining V (Vagueness)...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check if Description/Keywords available\n",
    "has_desc = 'Description_2021' in df.columns\n",
    "has_keywords = 'Keywords_2021' in df.columns\n",
    "\n",
    "if has_desc and has_keywords:\n",
    "    print(\"\\nâœ“ Computing vagueness from Description/Keywords...\")\n",
    "    \n",
    "    # Compute vagueness\n",
    "    df['V'] = compute_vagueness_vectorized(\n",
    "        df['Description_2021'].fillna(''),\n",
    "        df['Keywords_2021'].fillna('')\n",
    "    )\n",
    "    \n",
    "    # Z-score\n",
    "    df['z_V'] = (df['V'] - df['V'].mean()) / df['V'].std()\n",
    "    \n",
    "    print(f\"\\nğŸ¤™ V (Vagueness):\")\n",
    "    print(f\"   Count: {df['V'].notna().sum():,}\")\n",
    "    print(f\"   Mean: {df['V'].mean():.2f}\")\n",
    "    print(f\"   Std:  {df['V'].std():.2f}\")\n",
    "    print(f\"   Min:  {df['V'].min():.2f}\")\n",
    "    print(f\"   Max:  {df['V'].max():.2f}\")\n",
    "    \n",
    "    if df['V'].std() < 5:\n",
    "        print(f\"\\nâš ï¸  WARNING: Vagueness std ({df['V'].std():.2f}) is very small!\")\n",
    "        print(f\"   Expected std: 15-25 for real venture data\")\n",
    "        print(f\"   This may indicate data quality issues.\")\n",
    "        print(f\"\\n   Run diagnosis:\")\n",
    "        print(f\"     python scripts/diagnose_vagueness_quality.py\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Description/Keywords not found!\")\n",
    "    print(f\"   Description_2021: {'âœ“' if has_desc else 'âœ—'}\")\n",
    "    print(f\"   Keywords_2021: {'âœ“' if has_keywords else 'âœ—'}\")\n",
    "    print(f\"\\n   Using mock vagueness for demonstration\")\n",
    "    \n",
    "    df['V'] = np.random.normal(50, 15, len(df))\n",
    "    df['z_V'] = (df['V'] - df['V'].mean()) / df['V'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-F",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define F (Flexibility = 1 - is_hardware)\n",
    "print(\"Defining F (Flexibility)...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Infer F from CompanyName if not available\n",
    "if 'F_flexibility' not in df.columns:\n",
    "    print(\"\\nâœ“ Inferring F from CompanyName keywords...\")\n",
    "    \n",
    "    if 'CompanyName' in df.columns:\n",
    "        sw_keywords = ['software', 'platform', 'cloud', 'saas', 'app', 'digital', 'analytics']\n",
    "        sw_mask = pd.Series([False] * len(df))\n",
    "        \n",
    "        for kw in sw_keywords:\n",
    "            sw_mask |= df['CompanyName'].fillna('').str.contains(kw, case=False)\n",
    "        \n",
    "        df['F_flexibility'] = sw_mask.astype(int)\n",
    "    else:\n",
    "        print(\"   âš ï¸  CompanyName not found, using F=0 for all\")\n",
    "        df['F_flexibility'] = 0\n",
    "\n",
    "f1_count = (df['F_flexibility'] == 1).sum()\n",
    "f0_count = (df['F_flexibility'] == 0).sum()\n",
    "f1_rate = f1_count / len(df)\n",
    "\n",
    "print(f\"\\nğŸ’ª F (Flexibility):\")\n",
    "print(f\"   F=1 (Flexible/SW): {f1_count:,} ({f1_rate:.1%})\")\n",
    "print(f\"   F=0 (Rigid/HW):    {f0_count:,} ({(1-f1_rate):.1%})\")\n",
    "\n",
    "if f1_count == 0 or f0_count == 0:\n",
    "    print(f\"\\nâš ï¸  WARNING: F has only one value!\")\n",
    "    print(f\"   Interaction test will not be possible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-controls",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add controls (mock for demonstration)\n",
    "print(\"Adding control variables...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Mock controls\n",
    "df['founding_cohort'] = 'cohort_1'\n",
    "df['region'] = 'US'\n",
    "\n",
    "print(f\"\\nâœ… Control variables added (mock)\")\n",
    "print(f\"   founding_cohort: all set to 'cohort_1'\")\n",
    "print(f\"   region: all set to 'US'\")\n",
    "print(f\"\\n   Note: Replace with real founding year and HQ region for production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 3: ğŸ“Š PLOT1 - Sanity Check\n",
    "\n",
    "Visualize each variable to ensure they make sense.\n",
    "\n",
    "**What to check**:\n",
    "- E: Should be all 1 (by design)\n",
    "- L: Should be 15-40% (reasonable progression rate)\n",
    "- V: Should have mean ~50, std ~15-25 (if real data)\n",
    "- F: Should have both 0 and 1 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot1-E",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot E distribution (continuous)\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Use E_scaled for better visualization\n",
    "E_col = 'E_scaled' if 'E_scaled' in df.columns else 'E'\n",
    "E_label = 'E (Early Funding, $M)' if E_col == 'E_scaled' else 'E (Early Funding, $)'\n",
    "\n",
    "# Histogram\n",
    "ax.hist(df[E_col].dropna(), bins=50, color='red', alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel(E_label, fontsize=12, fontweight='bold', color='red')\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title(f'E Distribution (mean=${df[E_col].mean():.2f}M, std=${df[E_col].std():.2f}M)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add mean and median lines\n",
    "mean_val = df[E_col].mean()\n",
    "median_val = df[E_col].median()\n",
    "\n",
    "ax.axvline(mean_val, color='darkred', linestyle='--', linewidth=2, label=f'Mean: ${mean_val:.2f}M')\n",
    "ax.axvline(median_val, color='orange', linestyle=':', linewidth=2, label=f'Median: ${median_val:.2f}M')\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… E distribution plotted\")\n",
    "print(f\"   Mean:   ${df[E_col].mean():.2f}M\")\n",
    "print(f\"   Median: ${df[E_col].median():.2f}M\")\n",
    "print(f\"   Std:    ${df[E_col].std():.2f}M\")\n",
    "\n",
    "if df[E_col].std() > 0:\n",
    "    print(f\"   âœ“ PASS: E has variance (can be used in regression)\")\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: E has no variance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot1-L",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot L distribution\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "l_counts = df['L'].value_counts().sort_index()\n",
    "ax.bar(l_counts.index, l_counts.values, color=['gray', '#0000FF'], alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('L (Later Stage VC / Series B+)', fontsize=12, fontweight='bold', color='#0000FF')\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('L Distribution (2025)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['L=0', 'L=1'])\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, val) in enumerate(l_counts.items()):\n",
    "    ax.text(idx, val, f'{val:,}\\n({val/len(df)*100:.1f}%)', \n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "l_rate = l_counts.get(1, 0) / len(df)\n",
    "print(f\"\\nâœ… L distribution plotted\")\n",
    "print(f\"   L=1 rate: {l_rate:.1%}\")\n",
    "\n",
    "if 0.15 <= l_rate <= 0.45:\n",
    "    print(f\"   âœ“ PASS: L rate within expected range (15-45%)\")\n",
    "elif l_rate < 0.15:\n",
    "    print(f\"   âš ï¸  WARNING: L rate is low (<15%)\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  WARNING: L rate is high (>45%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot1-V",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot V distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.hist(df['V'].dropna(), bins=50, color='green', alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('V (Vagueness)', fontsize=12, fontweight='bold', color='green')\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title(f'V Distribution (mean={df[\"V\"].mean():.2f}, std={df[\"V\"].std():.2f})', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add mean and std lines\n",
    "ax.axvline(df['V'].mean(), color='darkgreen', linestyle='--', linewidth=2, label=f'Mean: {df[\"V\"].mean():.2f}')\n",
    "ax.axvline(df['V'].mean() + df['V'].std(), color='orange', linestyle=':', linewidth=2, label=f'Â±1 Std')\n",
    "ax.axvline(df['V'].mean() - df['V'].std(), color='orange', linestyle=':', linewidth=2)\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… V distribution plotted\")\n",
    "print(f\"   Mean: {df['V'].mean():.2f}\")\n",
    "print(f\"   Std:  {df['V'].std():.2f}\")\n",
    "\n",
    "if df['V'].std() >= 10:\n",
    "    print(f\"   âœ“ PASS: Vagueness has reasonable variance (std >= 10)\")\n",
    "elif df['V'].std() >= 5:\n",
    "    print(f\"   âš ï¸  WARNING: Vagueness std is small (5-10)\")\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Vagueness std is too small (<5)\")\n",
    "    print(f\"   Run diagnosis: python scripts/diagnose_vagueness_quality.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot1-F",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F distribution\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "f_counts = df['F_flexibility'].value_counts().sort_index()\n",
    "colors = ['gray', 'skyblue']\n",
    "ax.bar(f_counts.index, f_counts.values, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('F (Flexibility)', fontsize=12, fontweight='bold', color='skyblue')\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('F Distribution', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['F=0 (Rigid/HW)', 'F=1 (Flexible/SW)'])\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, val) in enumerate(f_counts.items()):\n",
    "    ax.text(idx, val, f'{val:,}\\n({val/len(df)*100:.1f}%)', \n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… F distribution plotted\")\n",
    "if len(f_counts) >= 2:\n",
    "    print(f\"   âœ“ PASS: F has both values (interaction test possible)\")\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: F has only one value!\")\n",
    "    print(f\"   Interaction test will not be possible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 4: âš–ï¸ TEST - Hypothesis Testing & Validation\n",
    "\n",
    "Test H*: Î²_VF > 0 (Flexibility amplifies Vagueness effect on Later success)\n",
    "\n",
    "**Model**: `L ~ z_V * F_flexibility + C(founding_cohort) + C(region)`\n",
    "\n",
    "**CRITICAL**: NO E control (E is mediator, not confounder)\n",
    "\n",
    "**Expected**:\n",
    "- Î²_V > 0: Vagueness helps Later success (main effect)\n",
    "- Î²_VF > 0: Flexibility amplifies the benefit (interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Hypothesis test\n",
    "print(\"=\"*80)\n",
    "print(\"HYPOTHESIS TEST: Î²_VF > 0\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data\n",
    "analysis_df = df[['L', 'z_V', 'F_flexibility', 'founding_cohort', 'region']].dropna()\n",
    "print(f\"\\nAnalysis dataset: {len(analysis_df):,} companies (complete cases)\")\n",
    "\n",
    "# Fit model\n",
    "formula = \"L ~ z_V * F_flexibility + C(founding_cohort) + C(region)\"\n",
    "print(f\"\\nFormula: {formula}\")\n",
    "print(f\"Note: NO E control (E is mediator, not confounder)\")\n",
    "\n",
    "try:\n",
    "    # Use run_HLVF from models module\n",
    "    model_hlvf = models.run_HLVF(analysis_df, formula=formula)\n",
    "    \n",
    "    print(f\"\\nâœ… Model fitted successfully\")\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"COEFFICIENT TABLE\")\n",
    "    print(\"=\"*80)\n",
    "    print(model_hlvf.summary2().tables[1])\n",
    "    \n",
    "    # Extract key coefficients\n",
    "    beta_V = model_hlvf.params.get('z_V', np.nan)\n",
    "    beta_F = model_hlvf.params.get('F_flexibility', np.nan)\n",
    "    beta_VF = model_hlvf.params.get('z_V:F_flexibility', np.nan)\n",
    "    \n",
    "    p_V = model_hlvf.pvalues.get('z_V', 1.0)\n",
    "    p_F = model_hlvf.pvalues.get('F_flexibility', 1.0)\n",
    "    p_VF = model_hlvf.pvalues.get('z_V:F_flexibility', 1.0)\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"KEY RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nÎ²_V (main effect):      {beta_V:7.4f}  (p = {p_V:.4f})\")\n",
    "    print(f\"Î²_F (flexibility):      {beta_F:7.4f}  (p = {p_F:.4f})\")\n",
    "    print(f\"Î²_VF (interaction):     {beta_VF:7.4f}  (p = {p_VF:.4f}) â† TARGET\")\n",
    "    \n",
    "    # One-tailed test for Î²_VF > 0\n",
    "    p_one_tailed = p_VF / 2 if beta_VF > 0 else 1 - (p_VF / 2)\n",
    "    \n",
    "    print(f\"\\nOne-tailed test (H1: Î²_VF > 0):\")\n",
    "    print(f\"  p-value (one-tailed): {p_one_tailed:.4f}\")\n",
    "    \n",
    "    # Verdict\n",
    "    sig_stars = '***' if p_one_tailed < 0.001 else ('**' if p_one_tailed < 0.01 else ('*' if p_one_tailed < 0.05 else 'ns'))\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"VERDICT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if beta_VF > 0 and p_one_tailed < 0.05:\n",
    "        print(f\"\\nâœ… REJECT H0: Flexibility AMPLIFIES Vagueness effect\")\n",
    "        print(f\"   Î²_VF = {beta_VF:.4f} > 0, p = {p_one_tailed:.4f} {sig_stars}\")\n",
    "        print(f\"\\n   Interpretation: High vagueness helps flexible companies MORE\")\n",
    "        print(f\"   than rigid companies in achieving later success.\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ FAIL TO REJECT H0\")\n",
    "        print(f\"   Î²_VF = {beta_VF:.4f}, p = {p_one_tailed:.4f}\")\n",
    "        if beta_VF <= 0:\n",
    "            print(f\"   Reason: Î²_VF is not positive (wrong direction)\")\n",
    "        else:\n",
    "            print(f\"   Reason: p-value not significant (p >= 0.05)\")\n",
    "    \n",
    "    # Store for plotting\n",
    "    hlvf_result = model_hlvf\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Hypothesis test FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    hlvf_result = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive validation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"E/L DESIGN VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nRun comprehensive validation:\")\n",
    "print(\"  python scripts/validate_E_L_design.py\")\n",
    "print(\"\\nThis will check:\")\n",
    "print(\"  âœ“ E=1 cohort purity\")\n",
    "print(\"  âœ“ L definition correctness\")\n",
    "print(\"  âœ“ Temporal consistency\")\n",
    "print(\"  âœ“ Cohort stability\")\n",
    "print(\"  âœ“ Impossible transitions\")\n",
    "print(\"  âœ“ Progression rate sanity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 5: ğŸ“ˆ PLOT2 - Final Interaction Plots\n",
    "\n",
    "Generate EVF and LVF interaction plots using `plots_F_series.py`.\n",
    "\n",
    "**EVF Plot** (F1b):\n",
    "- Shows E ~ V Ã— F interaction (Early funding amount by vagueness and flexibility)\n",
    "- E is continuous (funding amount), not binary\n",
    "- F=1 (Flexible/SW): skyblue solid line\n",
    "- F=0 (Rigid/HW): gray dashed line\n",
    "- Tests whether flexibility moderates vagueness effect on early funding\n",
    "\n",
    "**LVF Plot** (F3a):\n",
    "- Shows L ~ V Ã— F interaction (Later success probability by vagueness and flexibility)\n",
    "- L is binary (Series B+ achievement)\n",
    "- F=1 (Flexible/SW): skyblue solid line\n",
    "- F=0 (Rigid/HW): gray dashed line\n",
    "- If Î²_VF > 0, lines should diverge (SW steeper than HW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot2-LVF",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LVF interaction plot using plots_F_series\n",
    "if hlvf_result is not None:\n",
    "    print(\"Generating LVF interaction plot (F3a)...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    OUTPUT_DIR = Path(\"outputs/figures\")\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate F3a plot\n",
    "    paths = plots_F_series.fig_F3a_L_given_F(analysis_df, hlvf_result, OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"\\nâœ… LVF plot saved:\")\n",
    "    for fmt, path in paths.items():\n",
    "        print(f\"   {fmt.upper()}: {path}\")\n",
    "    \n",
    "    # Display inline\n",
    "    from IPython.display import Image\n",
    "    display(Image(str(paths['png']), width=800))\n",
    "else:\n",
    "    print(\"âŒ Cannot generate LVF plot - model fitting failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xobbx0udlr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate EVF interaction plot (E ~ V Ã— F)\n",
    "print(\"Fitting HEV interaction model (E ~ V Ã— F)...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Prepare data for EVF\n",
    "E_col = 'E_scaled' if 'E_scaled' in df.columns else 'E'\n",
    "evf_df = df[[E_col, 'z_V', 'F_flexibility', 'founding_cohort', 'region']].dropna()\n",
    "print(f\"\\nEVF analysis dataset: {len(evf_df):,} companies (complete cases)\")\n",
    "\n",
    "# Fit OLS model with VÃ—F interaction\n",
    "evf_formula = f\"{E_col} ~ z_V * F_flexibility + C(founding_cohort) + C(region)\"\n",
    "print(f\"Formula: {evf_formula}\")\n",
    "\n",
    "try:\n",
    "    import statsmodels.formula.api as smf\n",
    "    model_hev_interaction = smf.ols(evf_formula, data=evf_df).fit()\n",
    "    \n",
    "    print(f\"\\nâœ… HEV interaction model fitted\")\n",
    "    \n",
    "    # Extract interaction coefficient\n",
    "    beta_VF_evf = model_hev_interaction.params.get('z_V:F_flexibility', np.nan)\n",
    "    p_VF_evf = model_hev_interaction.pvalues.get('z_V:F_flexibility', 1.0)\n",
    "    \n",
    "    print(f\"\\nÎ²_VF (EVF interaction): {beta_VF_evf:.4f} (p = {p_VF_evf:.4f})\")\n",
    "    \n",
    "    # Generate EVF plot\n",
    "    OUTPUT_DIR = Path(\"outputs/figures\")\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nGenerating EVF interaction plot (F1b)...\")\n",
    "    paths_evf = plots_F_series.fig_F1b_E_given_F(evf_df, model_hev_interaction, OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"\\nâœ… EVF plot saved:\")\n",
    "    for fmt, path in paths_evf.items():\n",
    "        print(f\"   {fmt.upper()}: {path}\")\n",
    "    \n",
    "    # Display inline\n",
    "    from IPython.display import Image\n",
    "    display(Image(str(paths_evf['png']), width=800))\n",
    "    \n",
    "    hev_interaction_result = model_hev_interaction\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ EVF plot failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    hev_interaction_result = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot2-all-F-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all F-series plots\n",
    "print(\"\\nGenerating complete F-series...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Prepare results dictionary\n",
    "results = {\n",
    "    'HLVF': hlvf_result,\n",
    "    'HEV_interaction': hev_interaction_result\n",
    "}\n",
    "\n",
    "# Generate all plots\n",
    "try:\n",
    "    all_paths = plots_F_series.create_F_series(analysis_df, results, OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"\\nâœ… F-series plots generated:\")\n",
    "    for fig_name, paths in all_paths.items():\n",
    "        print(f\"\\n{fig_name}:\")\n",
    "        for fmt, path in paths.items():\n",
    "            print(f\"   {fmt.upper()}: {path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸  Some F-series plots may have failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¯ Summary\n",
    "\n",
    "This notebook completed all 5 stages:\n",
    "\n",
    "1. âœ… **BUILD**: Loaded consolidated E=1 cohort\n",
    "2. âœ… **DEFINE**: Defined E, L, V, F variables\n",
    "3. âœ… **PLOT1**: Validated each variable distribution\n",
    "4. âœ… **TEST**: Tested H*: Î²_VF > 0 with comprehensive validation\n",
    "5. âœ… **PLOT2**: Generated LVF interaction plots\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Review the VERDICT section in Stage 4 for hypothesis test results.\n",
    "\n",
    "### Output Files\n",
    "\n",
    "- Plots: `outputs/figures/F3a_L_given_F.png` (and `.pdf`)\n",
    "- All F-series plots in `outputs/figures/`\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **If vagueness std is low**: Run `python scripts/diagnose_vagueness_quality.py`\n",
    "2. **For full validation**: Run `python scripts/validate_E_L_design.py`\n",
    "3. **For industry subsets**: Run with quantum or transportation cohorts\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}