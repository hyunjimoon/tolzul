---
title: "Moderator Decision: Architecture vs Credibility"
subtitle: "H2 Bake-off (LEAN Version - Top 30% Features)"
author: "Strategic Ambiguity Research"
date: today
format:
  html:
    toc: false
    theme: cosmo
    embed-resources: true
execute:
  echo: false
  warning: false
jupyter: python3
---

```{python}
#| label: load-data
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv("outputs/h2_analysis_dataset.csv")
h1 = pd.read_csv("outputs/h1_coefficients.csv")
arch_coef = pd.read_csv("outputs/h2_model_architecture.csv")
arch_metrics = pd.read_csv("outputs/h2_model_architecture_metrics.csv")
found_coef = pd.read_csv("outputs/h2_model_founder.csv")
found_metrics = pd.read_csv("outputs/h2_model_founder_metrics.csv")
```

# Core Question

**Which moderator for H2**: Integration Cost (`is_hardware`) or Founder Credibility (`is_serial`)?

**Why it matters**: This choice determines our theoretical story and data requirements for the paper.

---

# 1. Motivation (H1)

**Does vagueness affect early outcomes?**

```{python}
#| label: h1-quick
try:
    vague_row = h1[h1['variable'].str.contains('vagueness', case=False, na=False)]
    if len(vague_row) > 0:
        coef = vague_row['coefficient'].values[0]
        pval = vague_row['p_value'].values[0]
        sig = "âœ“" if pval < 0.05 else "âœ—"
        print(f"**H1 Effect**: Î±â‚ = {coef:.3f}, p = {pval:.3f} {sig}")
        print(f"\nâ†’ Vagueness affects early funding. **But does it help or hurt long-term growth?**")
    else:
        print("H1 results not found")
except:
    print("H1 analysis pending")
```

---

# 2. Data Quality Check

```{python}
#| label: balance-check
def risk_label(pct):
    return "ðŸ”´ HIGH" if pct < 10 else "ðŸŸ¡ MED" if pct < 20 else "ðŸŸ¢ LOW"

# Calculate balance
hw_counts = df['is_hardware'].value_counts()
hw_pct = hw_counts.get(1, 0) / hw_counts.sum() * 100
hw_risk = risk_label(hw_pct)

serial_counts = df['is_serial'].value_counts()
serial_pct = serial_counts.get(1, 0) / serial_counts.sum() * 100
serial_risk = risk_label(serial_pct)

balance = pd.DataFrame({
    'Moderator': ['Architecture (is_hardware)', 'Credibility (is_serial)'],
    'Minority %': [f"{hw_pct:.1f}%", f"{serial_pct:.1f}%"],
    'Risk': [hw_risk, serial_risk],
    'N': [f"{len(df):,}", f"{len(df):,}"]
})

print("### Balance Assessment\n")
print(balance.to_markdown(index=False))
```

**Key**: ðŸŸ¢ = good balance, ðŸŸ¡ = caution, ðŸ”´ = high risk of unstable estimates

---

# 3. Bake-off Results

## Architecture (is_hardware)

![](outputs/bakeoff/h2_interaction_is_hardware.png)

## Credibility (is_serial)

![](outputs/bakeoff/h2_interaction_is_serial.png)

---

## Compact Coefficients

```{python}
#| label: compact-coefs

def extract_key_rows(coef_df, moderator_name):
    """Extract only 3 key rows: main, moderator, interaction"""
    rows = []

    # Main effect (z_vagueness)
    main_row = coef_df[coef_df['variable'] == 'z_vagueness']
    if len(main_row) > 0:
        rows.append({
            'Variable': 'z_vagueness',
            'Coef': f"{main_row['coefficient'].values[0]:.3f}",
            'p': f"{main_row['p_value'].values[0]:.3f}"
        })

    # Moderator main effect
    mod_row = coef_df[coef_df['variable'] == moderator_name]
    if len(mod_row) > 0:
        rows.append({
            'Variable': moderator_name,
            'Coef': f"{mod_row['coefficient'].values[0]:.3f}",
            'p': f"{mod_row['p_value'].values[0]:.3f}"
        })

    # Interaction
    int_row = coef_df[
        coef_df['variable'].str.contains('vagueness', na=False) &
        coef_df['variable'].str.contains(moderator_name, na=False)
    ]
    if len(int_row) > 0:
        rows.append({
            'Variable': f'z_vagueness:{moderator_name}',
            'Coef': f"{int_row['coefficient'].values[0]:.3f}",
            'p': f"{int_row['p_value'].values[0]:.3f}"
        })

    return pd.DataFrame(rows)

# Architecture
arch_compact = extract_key_rows(arch_coef, 'is_hardware')
arch_compact['Model'] = 'Architecture'

# Credibility
found_compact = extract_key_rows(found_coef, 'is_serial')
found_compact['Model'] = 'Credibility'

# Combine
compact = pd.concat([arch_compact, found_compact], ignore_index=True)

print("### Key Coefficients (3 rows per model)\n")
print(compact[['Model', 'Variable', 'Coef', 'p']].to_markdown(index=False))
```

---

# 4. Decision Matrix

```{python}
#| label: decision-matrix

# Extract key metrics
arch_n = arch_metrics['nobs'].values[0]
arch_r2 = arch_metrics['prsquared'].values[0]
found_n = found_metrics['nobs'].values[0]
found_r2 = found_metrics['prsquared'].values[0]

# Extract interaction p-values
try:
    arch_int_p = float(arch_compact[arch_compact['Variable'].str.contains(':', na=False)]['p'].values[0])
    arch_sig = "âœ“ Sig" if arch_int_p < 0.05 else "âœ— NS"
except:
    arch_int_p = 1.0
    arch_sig = "N/A"

try:
    found_int_p = float(found_compact[found_compact['Variable'].str.contains(':', na=False)]['p'].values[0])
    found_sig = "âœ“ Sig" if found_int_p < 0.05 else "âœ— NS"
except:
    found_int_p = 1.0
    found_sig = "N/A"

decision = pd.DataFrame({
    'Criterion': [
        'Data Quality (Balance)',
        'Statistical Evidence (p < 0.05)',
        'Model Fit (Pseudo-RÂ²)',
        'â”€' * 30,
        '**VERDICT** (Human Decision)'
    ],
    'Architecture': [
        f"{hw_pct:.1f}% {hw_risk}",
        f"p = {arch_int_p:.3f} {arch_sig}",
        f"{arch_r2:.3f}",
        "â”€" * 10,
        "[PENDING]"
    ],
    'Credibility': [
        f"{serial_pct:.1f}% {serial_risk}",
        f"p = {found_int_p:.3f} {found_sig}",
        f"{found_r2:.3f}",
        "â”€" * 10,
        "[PENDING]"
    ]
})

print("\n### Decision Matrix (Human Input Required)\n")
print(decision.to_markdown(index=False))
```

---

# Final Decision Template

**CHOSEN MODERATOR**: `[ ]` Architecture  `[ ]` Credibility

**RATIONALE** (fill in):
```
Data quality: ...
Statistical strength: ...
Theoretical fit: ...
```

**NEXT STEPS**:
1. Adopt chosen moderator for H2 main paper
2. Report alternative as robustness check (appendix)
3. Develop theoretical narrative for winner

---

**Report Stats**:
- Figures: 2 (interaction plots)
- Tables: 3 (balance, coefficients, decision matrix)
- Metrics per model: 2 (N, Pseudo-RÂ²)
- Total length: ~200 words

**Design**: Top 30% features only (Hemingway 10% Ã— 3, Abdullah 1/8 Ã— 2.4)

---

*Generated: {python} from datetime import datetime; datetime.now().strftime("%Y-%m-%d %H:%M")*
