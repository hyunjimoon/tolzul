from [[bayes_evol(andrew_josh)]]
# Bayesian Evolution Literature Classification (ìš°ë¦¬ ë…¼ë¬¸ "ë¶ˆí™•ì‹¤í•œ ì•½ì†ì„¤ê³„" ê´€ì )
## Based on Double Reparameterization Framework: P(success) â†’ Ï†(promise) â†’ (Î¼, Ï„)

### í•µì‹¬ í”„ë ˆì„ì›Œí¬: ì°½ì—…ê°€ì˜ ë¶ˆí™•ì‹¤ì„± ì„¤ê³„
- **ì²« ë²ˆì§¸ ì¬ë§¤ê°œë³€ìˆ˜í™”**: P(success) â†’ Ï†(promise) + n (ìì—°ì˜ ë³µì¡ì„±)
- **ë‘ ë²ˆì§¸ ì¬ë§¤ê°œë³€ìˆ˜í™”**: Ï† â†’ (Î¼, Ï„) where Î¼ = aspiration, Ï„ = concentration
- **Strategic Ignorance**: Ï„* = max(0, V/ic - 1) when information cost exceeds value
- **From Player to Designer**: ë¶ˆí™•ì‹¤ì„±ì„ ì œì•½ì—ì„œ ìì›ìœ¼ë¡œ ì „í™˜

---

## ğŸš€ Space Food (ìš°ì£¼ì‹ëŸ‰) Literature Classification

| Paper | Core Concept | ğŸŸ¢ AGREE | ğŸ”´ DISAGREE | ğŸ”µ Our Extension |
|-------|--------------|----------|-------------|------------------|
| **[[ğŸ“œğŸ‘¾_vul14_one_done]]** | 1-3 samples sufficient for near-optimal decision | **Strong Agreement**: Low Ï„ (sparse sampling) = adaptive optimality | Oversampling always better (ìš°ë¦¬: Ï„â†’âˆ causes learning trap) | Our Ï„* formula explains when to stop sampling |
| **[[ğŸ“œğŸ‘¾_stern24_model(beliefs, experimentation)]]** | Entrepreneurs test low-prior strategies first for better signals | Heterogeneous priors drive contrarian experiments | All experiments equally informative | Ï„ modulates experiment informativeness |
| **[[ğŸ“œğŸ‘¾_gans23_choose(entrepreneurship, experimentation)]]** | Entrepreneurial choice under uncertainty with strategic experiments | Experiments reveal both idea quality and strategy fit | Experiments are neutral (ìš°ë¦¬: Ï„ affects bias) | Promise design (Ï†, Ï„) shapes what experiments reveal |
| **[[ğŸ“œğŸ‘¾_tenanbaum11_grow(minds, cognition)]]** | Hierarchical Bayesian models of cognitive development | **Deep Resonance**: Learning as hierarchical prior updates | Learning is passive reception | Ï„ controls active forgetting vs integration |
| **[[ğŸ“œğŸ‘¾_gershman15_compute(rationality, resources)]]** | Bounded rationality as optimal given computational constraints | **Perfect Match**: Resource-rational = our V/ic framework | More computation always better | Strategic ignorance (Ï„=0) can be optimal |
| **[[ğŸ“œğŸ‘¾_busenitz97_recognize(entrepreneurs, biases)]]** | Entrepreneurs use heuristics and biases more than managers | Biases as features not bugs when Ï„ low | Biases are mistakes to eliminate | "Biases" = rational low-Ï„ strategies |
| **[[ğŸ“œğŸ‘¾_arrow69_classify(production, knowledge)]]** | Learning by doing creates knowledge spillovers | Production generates information (reduces n) | Knowledge always reduces uncertainty | Sometimes preserving uncertainty (low Ï„) valuable |
| **[[ğŸ“œğŸ‘¾_meehl67_test(theory, method)]]** | Theory testing requires strong inference | Strong tests need precise predictions (high Ï„) | Always maximize test precision | Optimal Ï„ depends on V/ic ratio |
| **[[ğŸ“œğŸ‘¾_peng21_overload(information, decisions)]]** | Information overload degrades decision quality | **Strong Support**: High i (integration cost) â†’ lower Ï„ optimal | More information always helps | Rational ignorance when i > V/c |
| **[[ğŸ“œğŸ‘¾_johnston02_caution(startups, scaling)]]** | Premature scaling is #1 cause of startup failure | High Ï„ too early = scaling trap | Fast scaling always good if funded | Ï„ should increase gradually with V/ic |
| **[[ğŸ“œğŸ‘¾_nejad22_model(mentorship, accelerators)]]** | Accelerators help calibrate entrepreneurial beliefs | External calibration of Î¼ and Ï„ | One-size-fits-all mentorship | Mentors help optimize personal Ï„* |
| **[[ğŸ“œğŸ‘¾_bhui21_optimize(decisions, resources)]]** | Resource-rational decision-making under constraints | Optimization given cognitive costs = our framework | Unbounded rationality ideal | Bounded optimality through Ï„ choice |
| **[[ğŸ“œğŸ‘¾_mansinghka25_automate(formalization, programming)]]** | Probabilistic programming automates Bayesian inference | Reduces i (integration cost) dramatically | Automation eliminates uncertainty | Lower i â†’ higher optimal Ï„, not elimination |
| **[[ğŸ“œğŸ‘¾_xuan24_plan(instruction, cooperation)]]** | Planning helps coordinate but constrains adaptation | Planning = high Ï„ for coordination | Always plan thoroughly | Ï„* depends on coordination needs |

---

## ğŸ¯ Bayesian Statistical Methods Integration

| Method | Application to Promise Design | Our Innovation |
|--------|-------------------------------|----------------|
| **Prior Predictive Check** | Test if Ï† ~ Beta(Î¼Ï„, (1-Î¼)Ï„) generates realistic success rates | Before promising, simulate outcomes |
| **Posterior Predictive** | Validate updated beliefs match observed pivots | Ï„ controls update magnitude |
| **Simulation-Based Calibration** | Recover true (Î¼, Ï„) from observed promises | Validate double reparameterization |
| **Hierarchical Modeling** | Industry â†’ Founder â†’ Venture nested structure | Ï„ varies across hierarchy levels |
| **Model Comparison** | Test double vs single reparameterization | WAIC shows double superior |

---

## ğŸŒŠ Synthesis: From Decision Under to On Uncertainty

### ğŸ¤  ì±„ì°ê³¼ê±°: The Tyranny of Information Maximization
**What We Must Destroy:**
- "More information = better decisions" dogma that created analysis paralysis
- Prediction-Based Prescription's rigid "predict then prescribe" sequence ignoring endogeneity
- Prior Predictive Checks that validate but never question the prior itself
- The delusion that uncertainty is always the enemy to be eliminated
- Better Place's $850M funeral: the price of information addiction

### ğŸ¥• ë‹¹ê·¼ë¯¸ë˜: The Dawn of Uncertainty Design
**What We Must Build:**
- **Bayesian Cringe** (Gelman): Healthy skepticism of over-precision
- **Strategic Ignorance**: Ï„* = max(0, V/ic - 1) mathematically defines when not knowing beats knowing
- **Endogenous PBP**: Prediction and prescription become one when Ï„ is chosen
- **Prior as Design**: Not what you believe but what you choose to believe
- **Tesla's Triumph**: "Roughly 200 miles" beats "Exactly 5 minutes"

### Key Falsifiable Predictions
1. **Industries with higher n â†’ lower average Ï„** (complexity forces flexibility)
2. **Lower i (e.g., AI era) â†’ bimodal Ï„ distribution** (all-or-nothing strategies)
3. **V/ic ratio determines optimal promise precision** (not market maturity)
4. **Successful founders show Ï„ trajectory: low â†’ high** (not monotonic increase)

---

## ğŸ’¡ Philosophical Foundation: Negative Capability

Building on Keats's "negative capability" - the ability to remain comfortable in uncertainty:

**NC = 1/(Ï„+1)**

- High NC (low Ï„): Tesla's "roughly 200 miles"
- Low NC (high Ï„): Better Place's "exactly 5 minutes"
- Zero NC (Ï„â†’âˆ): Theranos's impossible precision

This quantifies what poets knew intuitively: **comfort with uncertainty is strength, not weakness**.

---

## ğŸ”¬ Methodological Contributions

### For Bayesian Statistics (Andrew's Lens)
- **Endogenous uncertainty**: Ï„ as chosen parameter
- **Double reparameterization**: Computational elegance
- **Rational meaning construction cost**: i as digestion cost

### For Innovation Policy (Josh's Lens)  
- **Stage-appropriate Ï„**: Different policies for different V/ic
- **Market failures from Ï„ mismatch**: Over/under-specification
- **Policy as n-reducer, markets as Ï„-optimizer**: Clear roles

### For Entrepreneurship Theory (Scott's Lens)
- **Unifies Planning vs Action schools**: Both right at different Ï„
- **Explains contrarian success**: Low Ï„ preserves option value
- **Strategic ignorance as capability**: Not bias but feature

---

## ğŸ­ The Promise Paradox Resolution

ìš°ë¦¬ì˜ í•µì‹¬ ì—­ì„¤: **ì •ë°€í•œ ì•½ì†ì€ ì™œ ì‹¤íŒ¨í•˜ê³  ëª¨í˜¸í•œ ì•½ì†ì€ ì™œ ì„±ê³µí•˜ëŠ”ê°€?**

í•´ë‹µ: **Ï„* = max(0, V/ic - 1)**

- Better Place: High Ï„ despite high c â†’ Learning trap â†’ Failure
- Tesla: Low initial Ï„ â†’ Adaptive evolution â†’ Success
- Optimal strategy: Let Ï„ grow with V/ic ratio

**"ë¶ˆí™•ì‹¤ì„±ì€ ê·¹ë³µí•  ì œì•½ì´ ì•„ë‹ˆë¼ ì„¤ê³„í•  ìì›ì´ë‹¤"**

---

*Last updated: Based on deep synthesis of Space Food papers and our double reparameterization framework*