[[09-29|25-09-29]]

3.3 Sensitivity and Calibration of Frequentist Inference Frequentist inference 
(Casella and Berger, 2002; Lehmann and Casella, 2006; Keener, 2011) derives from the interpretation that probability theory can model only the frequencies of repeatable processes. This definition is consistent with the use of probabilities to model the inherent variation of observations, but it does not allow us to define probabilities over the model configuration space itself, as those probabilities would not correspond to the hypothetical frequencies of any repeatable process. Ultimately this strong philosophical MODEL-BASED CALIBRATION 9 assumption implies that we cannot use any form of weighting to quantify consistency in the model configuration space because any self-consistent weighting is equivalent to the assignment of probabilities! Consequently frequentist inference must take the form of definite decisions about which parts of the model configuration space are consistent with an observation and which are not. From a frequentist perspective inference and decisions are one in the same! Because such definite decisions can readily exclude the true data generating process, or model configurations close to the true data generating process, from consideration we have to carefully calibrate these decisions so that such exclusions are sufficiently rare. Ultimately frequentist inference does not define exactly how an observation informs which parts of the model configuration space to keep and which to discard. Rather frequentist inference establishes a means of calibrating any such procedure that might be considered. 3.3.1 Frequentist Inference Any procedure that consumes an observation to produce a definite decision about which parts of the model configuration space are considered consistent takes the mathematical form of an estimator. Estimators are functions from the measurement space to subsets of the model configuration space, mapping observations to subsets of model configurations, ˆθ : Y → T , where T is the space of well-defined subsets of the model configuration space, Θ. A common class of estimators are point estimators that identify a single point in the model configuration space (Figure 4a), ˆθ : Y → Θ, Point estimators formalize the intuition of a “best fit”, where inferences are summarized with a single point at the expense of ignoring the uncertainty inherent in learning from finite observations. The more general class of estimators that identify entire subsets of the model configuration space are known as confidence sets (Figure 4b), or confidence intervals if the model configuration space is one-dimensional. The nomenclature is meant to suggest that if a confidence set has been properly constructed then we can be confident that these sets will contain the true data generating process for sufficiently many observations. 3.3.2 The Frequentist Calibration Criterion The actual choice of which estimator to employ in a given analysis is left to the practitioner. Constraints that enforce desired properties can be imposed to restrict the space of potential estimators, but the choice of these desired properties remains the responsibility of the practitioner. Regardless of how we ultimately select an estimator, however, we can use the model configuration space to calibrate the estimator and determine it’s practical utility. 10 BETANCOURT PY M ˆθ(˜y) π ∗ (a) PY M ˆθ(˜y) π ∗ (b) Fig 4. (a) Point estimators identify a single model configuration, ˆθ(˜y), that is ideally close to the true data generating process, θ ∗ , for any given observation y˜. (b) Confidence sets identify entire subsets of the model configuration space that ideally contain the true data generating process for any given observation. In frequentist inference our actions are definite quantifications of the model configuration space and, by construction, estimators are inferential decision making processes. In order to define a calibration criterion we we must first construct a model-based loss function, L( ˆθ, θ), that quantifies how well ˆθ identifies the true data generating process, θ. Substituting an estimator yields the model-based inferential loss function, Lθˆ(y, θ) ≡ L( ˆθ(y), θ). As with estimators there is no canonical loss function in frequentist inference; instead one must be chosen using whatever domain expertise is available within the context of a particular analysis. In practice this choice often considers the structure of the estimator itself. If we knew that a given θ identified the true data generating process then the sensitivity of the loss of the estimator over the possible observations could be summarized with an expectation over that configuration. This expectation yields an expected loss for each model configuration, Lθˆ(θ) = Z Y dy π(y | θ)Lθˆ(y, θ). Because we don’t have any information about which model configuration identifies the true data generating process before a measurement is made, the frequentist calibration criterion is defined as the maximum expected loss over all possible model configurations, L¯ θˆ = max θ Lθˆ(θ). If the model configuration space is sufficiently rich that it contains the true data generating process, then this calibration criterion defines the worst case loss of our given estimator. MODEL-BASED CALIBRATION 11 Bounding the worst case loss of an estimator is an extremely powerful guarantee on its practical performance, but also a very conservative one as bounds can be dominated by unrealistic but not impossible data generating processes towards the boundaries of the model configuration space. A natural loss function for point estimators is the L p distance between the estimated model configuration and the presumed true data generating process, L p θˆ (y, θ) = ((θ − ˆθ(y))2 ) p/2 . The expected L p=2 loss is known as the variance of an estimator. Similarly, a natural natural loss function for confidence sets is inclusion of the presumed true data generating process, Lθˆ(y, θ) = Iθ[ ˆθ(y)], where the indicator function, I, is defined as Iθ[T] =  1, θ ∈ T 0, else . The expected inclusion loss, or coverage is simply how often the confidence set contains the presumed true data generating process. While this calibration procedure can used to analyze the frequentist properties of a given estimator, they can also be used to optimize the choice of estimator. Given a family of estimators, { ˆθx}, the optimal estimator will satisfy the minimax criterion, ˆθ ∗ = argmin x L ∗ θˆx = argmin x max θ Lθˆx (θ) = argmin x max θ Z Y dy π(y | θ)Lθˆx (y, θ). For example, a desired coverage might be established initially and then a confidence set engineered to ensure that the coverage is met, or exceeded, for all of the data generating processes in the model configuration space. 3.3.3 Frequentist Methods in Practice Aside from the conceptual challenge of choosing a loss function that enforces the needs of a given analysis, the computational burden of frequentist calibration is a significant impediment to its application. In particular, even approximately scanning through the model configuration space to identify the maximal expected loss often requires more computational resources than realistically available to a practitioner. Many frequentist analyses assume sufficiently simple model configuration spaces, estimators, and loss functions such that the maximum expected loss can be computed analytically. The analytic results allow, for example, optimal estimators to be chosen from 12 BETANCOURT families of candidate estimators with strong guarantees on the performance of the best choice. The practical validity of these guarantees, however, requires that the true data generating process be simple enough that it can be contained within the relatively crude model configuration space. For the complex experiments of applied interest this can be a dangerous assumption. Without analytic results one might consider interpolative methods that bound the variation in the expected loss between a grid of points distributed across the model configuration space. At each of these points Monte Carlo methods can be used to simulate observations and approximate the expected loss, and then the properties of the loss function itself can be used to interpolate the expected loss amidst the grid points. These methods can yield reasonable results for low-dimensional model configuration spaces, but as the dimensionality of the model increases even strong smoothness assumptions can become insufficient to inform how to interpolate between the grid points. In order to avoid this curse of dimensionality frequentist analyses unaccommodating to analytic results often resort to asymptotics. Asymptotic analyses assume that the model configuration space is sufficiently regular that as we consider more observations at once the behavior of the model configuration space follows a central limit theorem. Under these conditions the likelihood for any observation concentrates in an increasingly small neighborhood around the maximum likelihood estimator, θML(y) = argmax θ∈Θ π(˜y | θ). Moreover, in this limit the breadth of that neighborhood is given by the inverse of the Fisher information matrix, I(˜y) = ∇2π(˜y | θ)   θ=θML(y) . The concentration in the model configuration space in this asymptotic limit admits convenient analytic approximations to the frequentist calibration procedure. Asymptotic behavior also motivates the concept of profiling, which is of use when the parameter space separates into phenomenological parameters related to the underlying system of interest and nuisance or systematic parameters that are unrelated to that system but still effect the data generating process. Under certain conditions the observations inform the nuisance parameters faster than the the phenomenological parameters; in the asymptotic limit the uncertainty in these parameters becomes negligible and they can be replaced with conditional maximum likelihood estimates. More formally, if the parameterization of the model configuration space decomposes into phenomenological parameters, ϑ, and nuisance parameters, σ, then we define the conditional maximum likelihood estimator as σˆ(ϑ, y˜) = argmax σ π(˜y | ϑ, σ) MODEL-BASED CALIBRATION 13 and the corresponding profile likelihood as πˆ(˜y | ϑ) = π(˜y | ϑ, σˆ(ϑ, y˜)). The profile likelihood can then be used to calibrate estimators of the phenomenological parameters, at least in this limit. The utility of these asymptotic methods depends critically on the structure of the model configuration space and its behavior as we consider more observations. Simpler models typically converge to the asymptotic limit faster and hence require fewer data for asymptotic calibrations to be reasonably accurate. More complex models, however, converge more slowly and may require more data than is practical, or they may not satisfy the necessary conditions to converge at all. Consequently it is crucial to explicitly verify that the asymptotic regime has been reached in a given analysis. As with analytic methods, one has to be especially careful to not employ an over-simplistic model to facilitate the applicability of the asymptotic results while compromising the practical validity of the resulting calibration. 3.4 Sensitivity and Calibration of Bayesian Inference Bayesian inference (Bernardo and Smith, 2009; Gelman et al., 2014) broadens the interpretation of probability theory, allowing it to be used to not only model inherent variation in observations but also provide a probabilistic quantification of consistency between the data generating processes in the model configuration space and observations. This generalization manifests in a unique procedure for constructing inferences which can then be used to inform decisions. Ultimately Bayesian inference decouples inference from decision making, making the assumptions underlying both more explicit and often easier to communicate. Moreover, the fully probabilistic treatment of the Bayesian perspective immediate defines a procedure for constructing sensitivities and calibrations. 3.4.1 Bayesian Inference Bayesian inference compliments the data generating processes in the model configuration space with a prior distribution over the model configuration space itself. The prior distribution quantifies any information on which model configurations are closer to the true data generating process than others that is available before a measurement is made. This information can come from, for example, physical considerations, previous experiments, or even expert elicitation. Careful choices of the prior distribution can go a long way towards regularizing unwelcome behavior of the model configuration space. Together the model configuration space and the prior distribution define the Bayesian joint distribution over the measurement space and the parameter space, π(y, θ) = π(y | θ)π(θ). The titular Bayes’ Theorem conditions this joint distribution on an observation, ˜y, to give 14 BETANCOURT PY M π ∗ (a) PY M π ∗ (b) Fig 5. (a) Bayesian inference begins with a prior distribution, shown here in dark red, over the model configuration space, M, that quantifies information available before a measurement. (b) Information encoded in an observation updates the prior distribution into a posterior distribution that ideally concentrates around the true data generating process, π ∗ . the posterior distribution, π(θ | y˜) = π(˜y, θ) π(˜y) ∝ π(˜y | θ) π(θ). In words, the prior distribution quantifies information available before the measurement, the model configuration space decodes the information within an observations, and the posterior distribution combines both sources of information to quantify the information about the latent system being studies after a measurement (Figure 5). Any well-posed statistical query we might make of our system reduces to interrogations of the posterior distribution. Mathematically this must take the form of a posterior expectation for some function, f, E[f] = Z Θ dθ π(θ | y˜) f(θ). For example, we might consider the posterior mean or median to identify where the posterior is concentrating or the posterior standard deviation or the posterior quartiles to quantify the breadth of the distribution. Posterior expectations also define a unique decision making process in Bayesian inference. First we define the expected loss for a given action by averaging a model-based loss function, L(a, θ), over the posterior distribution, L(a, y˜) = Z Θ dθ π(θ | y˜)L(a, θ). We can then define a decision making process by taking the action with the smallest expected loss, a ∗ (˜y) = min a∈A L(a, y˜). MODEL-BASED CALIBRATION 15 For example, our decision might be to summarize the posterior with a single “best fit” model configuration, ˆθ. Given the loss function L( ˆθ, θ) = (ˆθ − θ) 2 the expected losses for each possible summary becomes L( ˆθ, y˜) = (ˆθ − µ(˜y)) + σ 2 (˜y), where µ(˜y) is the posterior mean and σ(˜y) is the posterior standard deviation. Following the Bayesian decision making process, our optimal decision is to summarize our posterior by reporting the posterior mean, ˆθ ∗ (˜y) = µ(˜y). 3.4.2 The Bayesian Calibration Criterion Bayes’ Theorem provides a unique procedure for constructing inferences and making subsequent decisions given an observation, but there are no guarantees that these decisions will achieve any desired performance for any possible observation. Consequently sensitivity analysis and calibration of this decision making process across possible is still important in Bayesian inference. Instead of having to consider each model configuration equally, however, the prior distribution allows us to diminish the effect of unrealistic but not impossible model configurations. In particular sampling from the joint distribution generates an ensemble of reasonable data generating process and corresponding observations which we can use to quantify the performance of our decisions. For example, we can quantify the sensitivity of any inferential outcome by integrating the model configurations out of the Bayesian joint to give the prior data generating process, π(y) = Z Θ dθ π(y, θ) = Z Θ dθ π(y | θ) π(θ). The prior data generating process probabilistically aggregates the behaviors of all of the possible data generating process in the model configuration space into a single probability distribution over the measurement space. We can then analyze the sensitivity of any inferential outcome by running our analysis over an ensemble of observations sampled from this distribution. Moreover, we can calibrate a decision making process by integrating a model-based loss function against the full Bayesian joint distribution, L¯A = Z Y,Θ dy dθ Laˆ(y, θ)π(θ, y). This calibration immediately quantifies the expected loss as both the observations and data generating processes vary within the scope of our model. 16 BETANCOURT 3.4.3 Bayesian Methods in Practice The unified probabilistic treatment of Bayesian inference ensures that all calculations take the form of expectation values with respect the Bayesian joint distribution, its marginals, such as the prior distribution and the prior data generating process, or its conditionals, such as the posterior distribution. Consequently calculating expectation values, or more realistically accurately estimating them, is the sole computational burden of Bayesian inference. Posterior expectations are challenging to compute, and indeed much of the effort on the frontiers of statistical research concerns the development and understanding of approximation methods. One of the most powerful and well-understood of these is Markov chain Monte Carlo (Robert and Casella, 1999; Brooks et al., 2011) and its state of the art implementations like Hamiltonian Monte Carlo (Betancourt, 2017). On the other hand, expectations with respect to the Bayesian joint distribution are often amenable to much simpler Monte Carlo methods. In particular, if we can draw exact samples from the prior distribution and each of the data generating processes in the model configuration space then we can generate joint samples with the sequential sampling scheme ˜θ ∼ π(θ) y˜ ∼ π(y | ˜θ). For each simulated observation, ˜y, we can construct a subsequent posterior distribution, make posterior-informed decisions, and then compare those decisions to the simulated truth, ˜θ. As we generate a larger sample from the Bayesian joint distribution we can more accurately quantify our sensitivities and calibrations. We can also quantify how sensitivity a calibration is to a particular component of the parameter space, ϑ, by sampling the complementary parameters, σ, from the corresponding conditional prior distribution, σ˜ ∼ π(σ | ϑ) y˜ ∼ π(y | (ϑ, σ˜)). This allows us, for example, to see how our decision making process behaves as for various phenomenlogical behaviors. Interestingly, the application of Monte Carlo to the Bayesian joint distribution is not at all dissimilar to many of the heuristic schemes common in the sciences. Sampling ˜y ∼ π(y | ˜θ) just simulates the experiment conditioned on the model configuration, ˜θ. The addition step θ ∼ π(θ) simply simulates model configurations consistent with the given prior information instead of selecting a few model configuration by hand. One inferential outcome immediately amenable to calibration is the approximation of posterior expectations themselves. Cook, Gelman and Rubin (2006), for example, introduce a natural way to calibrate the estimation of any posterior quantiles. This then immediately provides a procedure for quantifying the accuracy of any algorithm that yields deterministic approximations to posterior quantiles, for example as demonstrated in Yao et al. (2018). MODEL-BASED CALIBRATION 17 Bayesian sensitivity analysis is particularly useful for identifying known pathologies in Bayesian inference by carefully examining the simulated analyses. Consider, for example, the posterior z-score for the parameter component, ˜θn, zn =      µn(˜y) − ˜θn σn(˜y)      , where µn(˜y) denotes the posterior mean of ˜θn and σn(˜y) the corresponding posterior standard deviation. The posterior z-score quantifies how much the posterior distribution envelops the presumed true data generating process along this direction in parameter space. At the same time consider the posterior shrinkage of that parameter component, sn = 1 − σ 2 n (˜y) τ 2 n (˜y) , were τn(˜y) is the prior standard deviation of ˜θn. The posterior shrinkage quantifies how much the posterior distribution contracts from the initial prior distribution. An ideal experiment is extremely informative, with large shrinkage for every observation, while also being accurate, with small z-scores for every observation. In this case the distribution of posteriors derived from prior predictive observations should concentrate towards small z-scores and large posterior shrinkages for each parameter component. On the other hand, small posterior shrinkage indicates an experiment that poorly identifies the given parameter component, while large z-scores indicates inferences biased away from the true data generating process. We can readily visualize this behavior by plotting the posterior z- score verses the posterior shrinkage. Concentration to the top right of this plot indicates overfitting, while concentration to the top left indicates a poorly-chosen prior that biases the model configuration space away from the presumed true data generating process (Figure 6a). Because the Bayesian joint distribution considers only those true data generating consistent with the prior, however, this latter behavior should be impossible within the scope of a model-based sensitivity analysis. By investigating this simple summary we can quickly identify problems with our experimental design (Figure 6b, c). A scatter plot that combines the outcomes for all of the parameters components into one plot first summarizes the aggregate performance of the entire model, and then individual plots for each parameter component can be used to isolate the source of any noted pathological behavior.