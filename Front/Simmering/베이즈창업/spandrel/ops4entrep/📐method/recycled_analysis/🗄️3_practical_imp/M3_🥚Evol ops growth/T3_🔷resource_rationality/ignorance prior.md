[[09-14|25-09-14]]

In Chapter 12 (p. 385) of his book, Jaynes[[52]](https://en.wikipedia.org/wiki/Beta_distribution#cite_note-Jaynes-52) asserts that the _Haldane prior_ Beta(0,0) describes a _prior state of knowledge of complete ignorance_, where we are not even sure whether it is physically possible for an experiment to yield either a success or a failure, while the _Bayes (uniform) prior Beta(1,1) applies if_ one knows that _both binary outcomes are possible_. Jaynes states: "_interpret the Bayes–Laplace (Beta(1,1)) prior as describing not a state of complete ignorance_, but the state of knowledge in which we have observed one success and one failure...once we have seen at least one success and one failure, then we know that the experiment is a true binary one, in the sense of physical possibility." Jaynes [[52]](https://en.wikipedia.org/wiki/Beta_distribution#cite_note-Jaynes-52) does not specifically discuss Jeffreys prior Beta(1/2,1/2) (Jaynes discussion of "Jeffreys prior" on pp. 181, 423 and on chapter 12 of Jaynes book[[52]](https://en.wikipedia.org/wiki/Beta_distribution#cite_note-Jaynes-52) refers instead to the improper, un-normalized, prior "1/_p_ _dp_" introduced by Jeffreys in the 1939 edition of his book,[[59]](https://en.wikipedia.org/wiki/Beta_distribution#cite_note-Jeffreys-59) seven years before he introduced what is now known as Jeffreys' invariant prior: the square root of the determinant of Fisher's information matrix. _"1/p" is Jeffreys' (1946) invariant prior for the [exponential distribution](https://en.wikipedia.org/wiki/Exponential_distribution "Exponential distribution"), not for the Bernoulli or binomial distributions_). However, it follows from the above discussion that Jeffreys Beta(1/2,1/2) prior represents a state of knowledge in between the Haldane Beta(0,0) and Bayes Beta (1,1) prior.

Similarly, [Karl Pearson](https://en.wikipedia.org/wiki/Karl_Pearson "Karl Pearson") in his 1892 book [The Grammar of Science](https://en.wikipedia.org/wiki/The_Grammar_of_Science "The Grammar of Science")[[68]](https://en.wikipedia.org/wiki/Beta_distribution#cite_note-PearsonGrammar-68)[[69]](https://en.wikipedia.org/wiki/Beta_distribution#cite_note-PearsnGrammar2009-69) (p. 144 of 1900 edition) maintained that the Bayes (Beta(1,1) uniform prior was not a complete ignorance prior, and that it should be used when prior information justified to "distribute our ignorance equally"". K. Pearson wrote: "Yet the only supposition that we appear to have made is this: that, knowing nothing of nature, routine and anomy (from the Greek ανομία, namely: a- "without", and nomos "law") are to be considered as equally likely to occur. Now we were not really justified in making even this assumption, for it involves a knowledge that we do not possess regarding nature. We use our _experience_ of the constitution and action of coins in general to assert that heads and tails are equally probable, but we have no right to assert before experience that, as we know nothing of nature, routine and breach are equally probable. In our ignorance we ought to consider before experience that nature may consist of all routines, all anomies (normlessness), or a mixture of the two in any proportion whatever, and that all such are equally probable. Which of these constitutions after experience is the most probable must clearly depend on what that experience has been like."

If there is sufficient [sampling data](https://en.wikipedia.org/wiki/Sample_\(statistics\) "Sample (statistics)"), _and the posterior probability mode is not located at one of the extremes of the domain_ (_x_ = 0 or _x_ = 1), the three priors of Bayes (Beta(1,1)), Jeffreys (Beta(1/2,1/2)) and Haldane (Beta(0,0)) should yield similar [_posterior_ probability](https://en.wikipedia.org/wiki/Posterior_probability "Posterior probability") densities. Otherwise, as Gelman et al.[[70]](https://en.wikipedia.org/wiki/Beta_distribution#cite_note-Gelman-70) (p. 65) point out, "if so few data are available that the choice of noninformative prior distribution makes a difference, one should put relevant information into the prior distribution", or as Berger[[4]](https://en.wikipedia.org/wiki/Beta_distribution#cite_note-BergerDecisionTheory-4) (p. 125) points out "when different reasonable priors yield substantially different answers, can it be right to state that there _is_ a single answer? Would it not be better to admit that there is scientific uncertainty, with the conclusion depending on prior beliefs?."

- 🧍‍♀️(🎲🌲)?

Q2. there's no🧍‍♀️(📍🕸️) - does stochasticity comes before causal network? - leak noise, noisy or, ignorance prior from #tfq 
![[Pasted image 20241009094830.png|100]]

[[Noise and uncertainty]]