
| Source                                           | Core Focus                      | Key Argument                                                                       | When to Choose Parallel                                            | Common Thread               |
| ------------------------------------------------ | ------------------------------- | ---------------------------------------------------------------------------------- | ------------------------------------------------------------------ | --------------------------- |
| [[Front/Simmering/베이즈창업/spandrel/ops4entrep/📐method/recycled_analysis/🗄️3_practical_imp/M3_🥚Evol ops growth/🧭🌏PBE != p(E)+e(P)/📜Margossian24_nested_rhat]]                   | Nested R̂ for many short chains | Parallel chains with shared initialization can provide early convergence detection | When GPU/TPU parallelization available and quick validation needed | Convergence diagnostics     |
| Gelman & Rubin (1992)                            | Original R̂ statistic           | Between-chain variance should match within-chain variance                          | When long-run behavior needs to be verified                        | Variance comparison methods |
| Vehtari et al (2020)                             | Rank-normalized R̂              | Makes diagnostics robust to infinite variance                                      | When dealing with heavy-tailed distributions                       | Robust diagnostics          |
| Vats & Knudson (2021)                            | ESS per chain analysis          | R̂ measures effective sample size per chain                                        | When sample efficiency is critical                                 | Sample size effectiveness   |
| [[Front/Simmering/베이즈창업/spandrel/ops4entrep/📐method/recycled_analysis/🗄️3_practical_imp/M3_🥚Evol ops growth/🧭🌏PBE != p(E)+e(P)/📜Stan manual on auto param tuning in warmup]] | ![[🗄️n2s2n]]                   |                                                                                    |                                                                    |                             |

it's better to run shorter chains in parallel rather than a few long chains, when you have high uncertainty in how sampler is converging to target distribution ([cld1](https://claude.ai/chat/98116811-5abb-4380-8dba-57cce33027ce))


1. For exaptation vs adaptation from Buss's paper:

| Aspect | Adaptation | Exaptation |
|---------|------------|------------|
| Origin | Selected for current function | Co-opted for new function |
| Process | Direct optimization | Opportunistic repurposing |
| Time Frame | Gradual refinement | Rapid discovery of new uses |
| Selection Pressure | Continuous for specific purpose | Not initially selected for current use |
| Change Pattern | Incremental improvements | Sudden functional shifts |

2. From the MCMC paper about short vs long chains:

| Aspect | Long Sequential Chains | Short Parallel Chains |
|---------|----------------------|-------------------|
| Warmup Cost | Fixed per chain | Linear with chains |
| Hardware Utilization | Limited parallelism | Better GPU/TPU usage |
| Sample Independence | High autocorrelation | Lower correlation between chains |
| Convergence Detection | Needs long runs (R̂) | Can use nested R̂ for early detection |
| Effectiveness | Better for known distributions | Better for high uncertainty |

1. "high bar : low bar = sequential : parallel approach" (keep as is)
2. "gradual refinement : sudden discovery = adaptation : exaptation" 
3. "known convergence : unknown convergence = long sequential : short parallel"

The key insight is that parallel approaches excel when:
- There's high uncertainty about the target state/distribution
- Quick exploration of multiple possibilities is needed
- Hardware allows efficient parallel execution
- We need to discover unexpected uses/properties