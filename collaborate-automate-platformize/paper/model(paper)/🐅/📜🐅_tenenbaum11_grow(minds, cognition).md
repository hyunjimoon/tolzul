In coming to understand the world‚Äîin learning concepts, acquiring language, and grasping causal relations‚Äîour minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?

![[Pasted image 20250409083932.png|300]]
# üóÑÔ∏è1: Table of Contents (Question-Answer Format)

| Section/Subsection                   | Question                                                  | Answer                                                                                                                                                                                                                                           | üß±Literature Brick                                                                                                                                                                     |
| ------------------------------------ | --------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1. Introduction                      | Why do minds go beyond the data given?                    | üßç‚Äç‚ôÄÔ∏èHuman minds appear to acquire rich knowledge from sparse, noisy data, creating a "massive mismatch" between inputs and outputs of cognition. Abstract background knowledge must make up the difference.                                     | ‚Ä¢ Problem of induction in philosophy<br>‚Ä¢ Cognitive constraints literature<br>‚Ä¢ Bayesian AI and machine learning                                                                       |
| 2. The Role of Abstract Knowledge    | How does abstract knowledge guide learning and inference? | üß† Abstract knowledge encoded in probabilistic generative models defines structured hypothesis spaces and prior distributions that constrain generalizations from limited data. Bayes's rule provides the formal mechanism for updating beliefs. | ‚Ä¢ Bayesian cognitive models of similarity, causal reasoning and everyday prediction<br>‚Ä¢ Literature on mental models and causal theories<br>‚Ä¢ Work on intuitive physics and psychology |
| 3. The Form of Abstract Knowledge    | What forms does abstract knowledge take across domains?   | üó∫Ô∏è Knowledge takes domain-specific structured representations: trees for taxonomic concepts, directed graphs for causality, spatial layouts for geography, and schemas/grammars for framework theories.                                         | ‚Ä¢ Tree-structured models of word learning<br>‚Ä¢ Directed graphical models for causality<br>‚Ä¢ Research on intuitive theories<br>‚Ä¢ Graph grammars and schemas                             |
| 4. The Origins of Abstract Knowledge | How is abstract knowledge itself acquired?                | üß≠ Hierarchical Bayesian models (HBMs) explain how abstract principles can be learned from experience while simultaneously constraining lower-level learning. "The blessing of abstraction" enables faster learning at higher levels.            | ‚Ä¢ Hierarchical Bayesian models<br>‚Ä¢ Chinese restaurant process models<br>‚Ä¢ Nonparametric Bayesian methods<br>‚Ä¢ Transfer learning literature                                            |
| 5. Open Questions                    | What challenges remain for this approach?                 | üåè Modeling development of common-sense framework theories, implementing efficient approximate algorithms for inference, and explaining how structured symbolic knowledge is represented in neural circuits.                                     | ‚Ä¢ Probabilistic programming languages<br>‚Ä¢ Work on intuitive physics, psychology, biology<br>‚Ä¢ Monte Carlo sampling algorithms<br>‚Ä¢ Neural implementation of symbolic structure        |

# üóÑÔ∏è2: Comparison with Existing Theories

|Aspect|Nativism|Connectionism|Hierarchical Bayesian Approach|
|---|---|---|---|
|**Core Assumption**|Abstract knowledge is present from birth; not learned in any meaningful sense|Abstract knowledge emerges slowly from experience through statistical learning over unstructured representations|Abstract knowledge can be learned efficiently through Bayesian inference over structured representations|
|**Knowledge Representation**|Rich, structured symbolic representations (trees, graphs, schemas)|Distributed patterns of activation across unstructured neural networks|Rich, structured probabilistic generative models (trees, graphs, schemas) with statistical properties|
|**Learning Mechanism**|Simple non-statistical learning; checking hypotheses against data for consistency|Statistical learning through gradient descent on connection weights|Bayesian inference over structured hypothesis spaces; optimizing fit between model predictions and observed data|
|**Handling of Uncertainty**|Often deterministic; limited capacity to represent graded knowledge|Implicit in connection weights; not explicitly represented|Explicitly represented through probabilities; uncertainty is fundamental to the approach|
|**Cross-Domain Transfer**|Domain-specific modules with limited transfer|General learning mechanisms but slow, limited transfer|Domain-general learning mechanisms with rapid, structured transfer through hierarchical models|
|**Explanation for Fast Learning**|Knowledge is innate, not learned|Difficult to explain; generally predicts slow learning|"Blessing of abstraction" - higher-level knowledge pools evidence across many lower-level contexts|
|**View of Development**|Maturation of innate capacities with limited change|Slow, continuous accumulation of knowledge through experience|Rapid acquisition of abstract principles that then guide more specific learning|
|**Evidence From Cognitive Science**|Critical periods; early competence in infants|Neural network learning; distributed representations|Rapid generalization from few examples; ability to learn abstract principles|
|**Computational Tractability**|Often computationally infeasible for realistic problems|Computationally tractable through gradient descent|Often requires approximate inference (e.g., Monte Carlo methods) for realistic problems|

# üóÑÔ∏è3: Practical Implications

| Domain                          | Implication                                                                                             | Example Application                                                                                                          |
| ------------------------------- | ------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |
| **Artificial Intelligence**     | More human-like learning systems require structured representations combined with statistical inference | AI systems that can learn abstract concepts from few examples; combining deep learning with structured probabilistic models  |
| **Cognitive Development**       | New experimental paradigms to test hierarchical Bayesian models of children's learning                  | Investigating how children acquire intuitive theories of physics, psychology, and biology through structured representations |
| **Education**                   | Teaching abstract principles can accelerate domain-specific learning                                    | Curriculum design that introduces structural frameworks before specific content; leveraging existing knowledge structures    |
| **Computational Neuroscience**  | New hypotheses about how the brain implements structured probabilistic knowledge                        | Neural circuit models that implement approximate Bayesian inference through Monte Carlo sampling                             |
| **Cognitive Therapy**           | Targeting higher-level abstract knowledge may be more efficient than addressing specific beliefs        | Interventions that reshape causal frameworks rather than individual associations                                             |
| **Machine Learning**            | Nonparametric Bayesian methods allow flexible, adaptive hypothesis spaces                               | Algorithms that can discover appropriate forms of structure (trees, rings, grids) from data                                  |
| **Automated Science**           | Systems that can discover appropriate structural forms from data                                        | Scientific discovery systems that propose new theoretical frameworks and test them against data                              |
| **Robotics**                    | Robots that can rapidly adapt to new tasks through structured transfer learning                         | Robot manipulation systems that learn abstract principles of physics and object interaction                                  |
| **Computer Vision**             | Visual systems that infer structured scene representations                                              | Scene understanding systems that identify objects, their relationships, and causal interactions                              |
| **Natural Language Processing** | Language models that incorporate probabilistic grammars and structured world knowledge                  | Systems that can learn language structure and meaning from limited data through Bayesian inference                           |