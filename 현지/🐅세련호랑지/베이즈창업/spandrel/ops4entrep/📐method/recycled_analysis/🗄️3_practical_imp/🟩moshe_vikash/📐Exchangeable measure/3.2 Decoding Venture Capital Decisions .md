[[culturate(amoon(thesis))]] with [[jeff_dotson]], [[üíúevaluate]] by [[rahul_baui]]'s seminar on [[2025-03]]
2025-04-03
- [[üìùüëªphantom rationalize meaning]]
- 

| Paper Title (Author) | Year | Role |
|------------|------|------|
| "A Paradigm for Developing Better Measures of Marketing Constructs" (Churchill) | 1979 | Framework for developing marketing construct measurements |
| "Psychometric Theory" (Nunnally) | 1978 | Psychometric theory foundation |
| "Psychological Testing" (Anastasi) | 1988 | Psychological testing methodology |
| "Content Validity in Psychological Assessment" (Haynes) | 1995 | Content validity assessment approach |
| "Objective Tests as Instruments of Psychological Theory" (Loevinger) | 1957 | Theory on psychological measurement |
| "Convergent and Discriminant Validation" (Campbell) | 1959 | Validation methodology using multitrait-multimethod matrix |
| "Validity of Psychological Assessment" (Messick) | 1995 | Psychological assessment validation |


## need for calibration
Imagine you're playing a video game where you need to match heroes with the right teammates. The game shows you a percentage score for each potential team-up, like "90% good match!" But if that number isn't calibrated properly, you might keep picking teams that look great on paper but lose in actual battles. In the startup world, this matters most when founders are trying to match with investors based on their team's background - like having PhDs or previous startup experience. A well-calibrated system means if it tells you "you have an 80% chance of matching with science-focused investors because of your PhD," that prediction should be trustworthy because looking at past similar cases, about 80% of PhD teams actually did get investment from those investors. This helps founders make better decisions about which investors to approach instead of wasting time on matches that look good but rarely work out in reality.

