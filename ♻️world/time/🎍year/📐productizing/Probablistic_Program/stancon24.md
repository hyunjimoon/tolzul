
Learning models as probabilistic inference; "learning to leam", transfer leaming, learning representations and learning inductive biases

my take on probabilistic program, A unifying framework integrating our best computational ideas on intelligence, across multiple eras of cognitive science and Al (Symbols in modern PPLs: probabilistic, flexible, learnable, robust) 

Probabilistic (Bayesian) inference for reasoning about likely unobserved causes from observed effects, and decision making under uncertainty.
Symbolic programs for representing and reasoning with abstract knowledge
Neural networks for pattern recognition and function approximation.
ÔªøÔªøÔªøHow can probabilistic inferences be used to drive action? Utility-based frameworks for decision and planning under uncertainty and risk, such as Bayesian decision theory or Markov decision processes (MDPs).

Table 2 provides a clear comparison between the Value Chain Strategy and the Architectural Strategy across the four key choice areas. The entrepreneurial strategy compass is formed by choosing one option from each of INVESTMENT (Control or Execute) and ORIENTATION (Compete or Collaborate). The combinations result in the four strategies: Intellectual Property (Control + Collaborate), Value Chain (Execute + Collaborate), Disruptor (Execute + Compete), and Architectural (Control + Compete).

### Brynj√≥lfur Gauti Gu√∞r√∫nar J√≥nsson
Copulas in Stan: Modeling Spatial Dependence in Generalized Extreme Value Distributions

Nicolas Irons (University of Washington) Evaluating and optimizing non-pharmaceutical interventions to combat infectious disease transmission 
Causally sound priors for binary experiments


### matti

### jacqueline buros


### YANN: predictiv performance of power posteriors
choose tau -> model specification
frequentist calibfation, infromation matching, predictively, generatilisg bound

choose tau tailored for predictive performance is bad (tempering has vanishing effect)

argmin (tau>0) d(qn(.|y1:n), pn tua(.|y_1:n)
 objective is ill-defined (tau is not predictive weapon in arsonal)
 individual datasets and  (regulates data and prior) - plug in predictive (any value i choose induce)
plugin and posterior (frequentis convergence and poseterior concentration)

2. flat region: infinitely many tau indluce same predictive
3. this holds both in high probability and in expectation : under the same assumptions with high probability E_{y}

max elpd = min kl

dkl (f,p) = epsilon^2 + exp(-Cntau epsilon_n^2). + o(1)

‚≠êÔ∏èparameter uncertainty is of second-order importance relative to data and model uncertainty in terms of predictive accuracy? ~ tempering for predictive
not 

different parameter can lead to parametr uc 

predictive performance of power posteriors 
not as important on 

when approximate schemme was in

Q2. 
Q3. results holds in expectation probability (why in practice when we deviate) - A. some (more likely to hold asymptotically - unifomly over tau; )

### johas mikhael
work with proxy (imperfect -> skewed systematically) - diabetes (has been diagnosed; imperfect proxy)
prediction accuracy is degrded for unsampled population (imperfect proxy -  model measurement process)

proxy Y true outcome U, diagnosis, disease status

uninsured/insured -> unin (underpredict the population)

measurement error is correlated with covariate (omitted variable error)

predictivon accuracy ( )

idea: introduce true outcome as latent variable, use domain kwldg to inform model structure and priors on structural parametres

prediction: partially identified model (standard workflow for identified part, sentitiv analysis for unidentified parameters)

diabetes exampl
= suppose we have all information (diabetes example); logistic regression with latent continuous  
(cross the threshold then have disease) - 

latent disease burden -> diagnosis (second latent; u1-t(x) - e; undiagnosed diabetes)

for insured and uninsured (underpredict the risk is bad)

different between being diabetes and diagnosted (well-calibrated)

by comparins the classical logistcia regression and measuremt model (imperc proxy -< model measure models ; trasnparent assumption; encourage garner necessary prior )

hierarchical bay



Q bob's question: how did you know which direction of error?, diabetes from uninsured --?
Q2. calibrating against predictive situation A. if prediction changes a lot, prior on measurement prior is not yet sensitive to use

use both priro, (robust to ),  (within prediction )
### anna
bayesian workflow

(siterative filtering for multiverse analysis) - supporting iterative model building 
ecology, medicine, psychology, economics (shaping) - transparent

requirements: transparency, reliable estimates, uncertainty quantification, 
challenges: limited resources, intertwined tasks in model building and computations

which models are the more prosimising candidates? (model quality we care about)

recommendation for BW, evaluate and filter  (fewer models with higher quality)

(1) filtering (useful:=usable for unseen data) by predictive abilities: 

negative binomial (circle: poisson, triangle: negative binomial)

‚≠êÔ∏èdependent on the model we started from 

check, change, 

how big you should start from? if your goal is leavning 5 models in the end (as it fits into one plot) 5 times
variability 
20-80, 
(20->7)

verification ???? time to reparameterize
or give up on a model

Q. have you seen meaningful difference between model building in different domain? can domain specific language be desgined for each clustered ecology, medicine, psychology, economics ()
Q2. usable. =useful for unseen data?


1. heuristically, lower thershold on how many models should be left in the end (at least five), how many models you sould start from compared to that number e.g. 20 80 rule so at least five times more 25 models/ 3. when to increase the diversity of model? (not selecting the model; removing the woorse)


‚≠êÔ∏èfit (but data can change -> ); you give me data, we fit. entrepreneur (go and find data to persuade the model - ); resource allocation (rather); absorptive capability (stan is language, algorithm, community); espeically the ones with longer delay and; model fit -> ÏûêÎ¨ºÏá† Î™®Ìòï -> Ìö®ÏÜå Î™®Ìòï (both strong and flexible); ü§®ask bob - covid example of comparing nonparametric gp (upper bound)

### david
you can use $100 to increase the prediction accuracy of rank, but also invest in marketing to shape the ranking function  (resource optimization)

bob said what ppl care about (time series, )
R^2 induced piror hierarchy

integrated reasoning (between logic, visual, shape, prediction for piano playing0??)


prdictive perfomance is less senstive to model complexity than other prior

### sophie

unseen products
1. 
2. how many products contain ti dioxide 
3. how much food do we consume daily (certain food category, category specific (age, gender, ))

distribution of habitual containg ti02 in popoluation (multiple)

no wonder sophie and jonas's  government (downstream fairnemss)
policy measurement, 

simulation (post stratification based on age, gender, province) from the joint posterior of consumption, concentration, market presence (age, gender, province)

allocation of resource (information is knowledge)
more targeted intervention (zero to one vs one to two; inegration and )

ü§î? healthier population (for unknown - new )

### charlotte
use of bayesian hiermodeling using simulated data

motivation -> bayesian hierchical modeling introduction -> bhm poc in publicaly available data -> simulation studies

basket could be used for two clusters

dynamic borrowing method

treatment on survial (borrowing most when clinciat data are in agreemtn with historical data and least when not in agreement)
prior layer (sigma0 is borrowing)

ignores the real data are grouped
published treatment effect for 

sim1: let's not use complete model (cp, pp with infomative/weak priro)
sim2: exponential (four different treatment effects); coverage plot
sim3: same with sim2, but control survivals (look at bias) - less participat for clusters

estimates

‚≠êÔ∏èwhen you simulate -> persuade (charolette: use ; right on the dotted line (persuade bh; sim1), sofie for )

the use of simulation: persuastion -> udpate the mental model which guides the model building (process)

pooling consistent (cover different things) - contract research organization

‚≠êÔ∏èprojecting the near future to change the near near future 

epidemiology : science of health (generic sense- noncummicative, global, rare,= macro science VS biology = aggregation (precise) )

understand world on micro level (human behavior has larger variance ; human health)

more feedack model (compuational biology) - ppl's attitudes to heatlth and nutricitne  VS cultural

social science and economic, statistcs and ml (comp epidemology)

how to explina phenomeno numerical undersatnd relationship

infectious process (organ, body, city, contry, world, quantative data; individual)

communicating modeling - level of aggregation, length of data for train vs test, estimated vs assumed data
reletive noncommunicable  are dynamic (exponential so faster)

model and dat, uncertainty, 

compartmental (disease; SIR vs SEIR)

compartemental vs agent  summary mechanistic vs semi  (communicatle: abc noapprotial; spatial pattern is )

state space, compartment (non communicatble)

rather than viewing each compartment 0 view the state of one person during the course of disease

dyynamics (noncommunicat (spread from person from person - expoential ))

understanding two hiv is differnt from two 

sophie's (health)
- simple: dag with 7 variables (simple, interpretable causl)

ambidextrous; what are some test diagrnostics that can be applied across differet models? interface
even if using the SIR model

elizaveta: imagine you're the modeler who needs to handle all agent based, semi, compartment model - what are some characeristics of good test diagnostics for you? context of this question is, different data generating process requires different evaluation measure for model. even conditional on one modeling say compartmental SIR model, diffusion of idea and pathogen has completely differnt implication and risk to humanlty and i feel odd test quantities for fitting the model are not too different as mu

chris: what's your workflow on choosing which parameter to estimate, as opposed to assumed? e.g. reproduction number is the ratio of two parameter. would you first make all  and with this two degree of freedom there are at least three different ways to   i get confused what to set as estimated and assumed parameter.

i don't this uncertainty not only comes from quantative measurements but also communicative easiness - i assume policy makers and stakeholders feel more comfortable on reproduction number so ()


and i wonder how to desgin test diag should differ e.g. what to recommend for test quantistics

parameterization setting estimated vs assumed parameter/
reproduction does this choice only depende on data at hand but also importance or relevance to policy e.g.  behavior vs biological - some biologial can be assumed (that can)

(macro - 

do you have any example where you set as assumed parameter even if it is uncertain as it is 



sequential data collecting (surveys ; judgent - one quantitiy ; average walth  - representative sample)

1. test diagnostics that can be applied to different modeling other than predictive accuracy (bayesian; abm, agent )
2. modelers bridges software and policy usecase feedback l duties one to the tool builders and the  to  - level of aggregation, length of data for train vs test, estimated vs assumed data
reletive noncommunicable  are dynamic (exponential so faster)

---


Diagramatic representations of dynamic systems (as in Stella or Vensim) serve two key functions:

- For the model developer, an added check for avoiding errors
- For the model user, as a way of communicating how the system works. Diagrams like this were used extensively with stakeholders in Wisconsin, to interactively discuss and mark up the model concept
    

### mitzi

#### code
github closed issue is provided to compare with open issue, 

that takes (all the process of checking, minimal computation, spits out the answer we want) 

1. tracer bullets (how to shoot in the dark) - not bottom up from inside but outside (how user would give data and what they want) - set of scripts do their analysis nicely  (!= prototype)
2. don't repeat yourself (engineering, plumbing finance); java collection code (in sample, modular code, looking at other code - refactoring and design patterns (functionality e.g. io is done by callback - i'm your mcmc engine will give you logprob for all e.g. separate writeup and ) , `effective program`  (c++, java, python; not make the R joke)); unit test for your code, (will make each other check each other's work). 
3. test ruthlessly and effectively: unit test (if you have modularity, can't have 1:1)
4. unit test for validate_diag_inv_metric (unit test should be very fun), signle model integration tests transpiled to C++ and compiled to an executable program, command line interface tests; chain of trust (unitest test every piece )
5. automated testing using jenkins CI: start, clean&steup, verify changes, clang-format, [parallel tests, mac interace tests, upsterm]

#### code of conduct
psychology of computer programming (egoless programming - code review), 
building depending on invisible others 1. understand and accept that you'll make mistake (humble), you're not your code, no matter how much karate you know someone else will always know more (programming is vast, embrace continuous learning), don't write code without consultation (consult with github issue and traceback), treat ppl who know less than you with respect, deference, and patience (ppl love to teach), constant in the world is change, only true authority stems from knowledge not from position (around conference table we're all equals), fight for what you believe but gracefully accept defeat, don't be the coder in the corner (make the effort to find out whether expert how to help you), be kind to coder not the code

e.g. brian's comment on : i'd prefer if this inherited jac is placed in one page

stan developer process
- create issue, branch, if you know its the (bug is triger; test the bug fix and )
	1. .create test, commit, 
	2. test with the thing broken,
	3. now you have test (pull request) show that üêú
	4. fix the branch
	5. merge branch into development

output of (5~6k)
bw can go in one of two direction: production vs researchy (centric new technique, model applied to new problem); robust efficiency, modularity VS (doing simulation and testing, comparison, ben)

bring modularity in stan - not qualified to be stan reviewer

so many in the first pull requests, don't let that scare you, 

it's different repository (differ) - math (modular test) - cmdstan (integration) ; it differs where you're in the stan ecosystem

parser ($4k of testing; few weeks of how stan sampler works (10k iteration VS user done 1m run to off))

service layer






### tutorial for bayesian opt


disease survellience (what data to sample and when, more for less)












