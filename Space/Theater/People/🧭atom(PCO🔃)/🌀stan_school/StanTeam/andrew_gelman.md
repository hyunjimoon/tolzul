---
ì´ë¦„: Andrew Gelman
ì¶œìƒ: 2020-12-06
ì–¸ì–´êµí™˜:
- persons
collection:
- '[[People]]'
field:
- ğŸ…cba
- ğŸ‘¾cog
atom: ğŸ§­atom(PCOğŸ”ƒ)
created: '2024-11-12'
---

---
[[09-11|25-09-11]]

[[ìŠ¤íƒ€ì¼ğŸ‘¾ğŸ¢ğŸ…ğŸ™ğŸ‘¾]]

----

2025-04-17
[[ğŸ“œandrew24_federated_learning]]


excerpts from his writing

1. classical theory (where statistical properties are evaluated based on their long-run frequency distributions) and in Bayesian statistics (**averaging over the prior** distribution).
2. For example, some Bayesians dislike posterior predictive checks, but non-Bayesians mostly seem to ignore the ideaâ€”even though Xiao-Li Meng, Hal Stern, and I wrote our paper in general terms and originally thought our methods might appeal more strongly to non-Bayesians. After all, those statisticians were already using p-values to check model ï¬t, so it seemed like a small step to average over a distribution. But this was a step that, by and large, only Bayesians wanted to take. The reception of this article was what convinced me to focus on **reforming Bayesianism from the inside** rather than trying to develop methods one at a time that would make non-Bayesians happy.
3. partial pooling to learn about groups for which there was only a small amount of local data.
4. Bayesian inference seemed like a theoretical toy and was considered by many leading statisticians as somewhere between a joke and a menace but the hardcore Bayesians such as Lindley, Good, and Box persisted and got some useful methods out of it. ... Bootstrap idea of Efron (1979) is an idea that in some way is obviously wrong (as it assigns zero probability to data that did not occur, which would seem to violate the most basic ideas of statistical sampling) yet has become useful to many and has since been supported in many cases by theory.

[http://www.stat.columbia.edu/~gelman/research/published/copss.pdf](http://www.stat.columbia.edu/~gelman/research/published/copss.pdf)


2021-05-31
Hi, Hyunji. Â I don't really know enough about Operations Research at either school to give an informed opinion. Â I think operations research is usually more theoretical than statistics, but there should be some flexibility on what you can work on. Â Perhaps you can do some innovative synthesis of Bayesian decision analysis with machine learning (generative adversarial networks, etc.). Â It is my impression that for statistics and information science, Columbia is better than Yale. Â And Columbia has a better statsitics department. Â So I recommend Columbia. Â I do not think I can be your primary advisor because I am not in the engineering school, but we can work together and I can probably be on your committee.

- ğŸ›[playroom slide](https://docs.google.com/presentation/d/1EftnfRgzkmxMerBzk7L-pYIYwpVjjYkZpL5Yar0pN2U/edit?slide=id.g10c3c90caca_0_13#slide=id.g10c3c90caca_0_13)