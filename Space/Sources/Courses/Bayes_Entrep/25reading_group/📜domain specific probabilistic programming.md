
| Section/Subsection                           | üîêResearch Question                                                                                                        | üß±Literature Brick                                                                                                                                                                                                                                                                           | üîëKey Message                                                                                                                                                                                                                                                                   | üìäEmpirical Evidence                                                                                                              |
| -------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
| 1. Introduction                              | How can probabilistic programming better support "theory of mind" models that involve recursive reasoning about reasoning? | ‚Ä¢ Recursive rationality (RR) paradigm spans cognitive science, psychology, linguistics, economics<br>‚Ä¢ Current PPLs struggle with two challenges: correctness and efficiency<br>‚Ä¢ Computational models of theory of mind appear in many disciplines [12, 13, 2, 3, 15, 76, 87, 118, 91, 111] | üåè A domain-specific PPL for recursive rationality can address key challenges in modeling theory of mind:<br>1) üß≠ Special syntax/semantics for agency prevents mind-reading/mind-control bugs<br>2) üó∫Ô∏è Array-based compilation enables faster inference on modern hardware    | Fig 1: Illustration of theory of mind in language understanding<br>Example of "perpetration confusion" bug in traditional PPLs    |
| 2. Demo of memo                              | How does memo's syntax and semantics express a theory of mind model concretely?                                            | ‚Ä¢ Rational Speech Acts (RSA) framework [43, 52, 54, 64]<br>‚Ä¢ Pragmatic inference in communication [68]<br>‚Ä¢ Recursive Bayesian reasoning                                                                                                                                                     | üßç‚Äç‚ôÄÔ∏è memo associates each random choice with the agent making it, ensuring proper "frames of mind" are maintained<br>‚Ä¢ Frames track agents' knowledge and uncertainty<br>‚Ä¢ Models compile to array operations for efficient inference                                          | Fig 2: Complete RSA implementation (10 lines)<br>Fig 3: Parameter fitting by grid search and gradient descent                     |
| 3.1 Front-end: tracking "frames of mind"     | How can a PPL's static semantics enforce basic principles of agency?                                                       | ‚Ä¢ Bayesian belief updating<br>‚Ä¢ Epistemic logic [56, 110]<br>‚Ä¢ Planning as inference [25]                                                                                                                                                                                                    | üß† memo enforces four key principles of agency:<br>1) No mind reading: agents don't automatically know others' choices<br>2) Agents can acquire false beliefs<br>3) Referential opacity of belief<br>4) No mind control: agents make their own choices                          | Fig 5: Evolution of nested "frames of mind" in RSA model<br>Visual tracking of knowledge and uncertainty across agents            |
| 3.2 Back-end: lowering memo to array program | How can theory of mind models be compiled to efficient array programs?                                                     | ‚Ä¢ Tensor variable elimination [103]<br>‚Ä¢ Vectorized inference [104]<br>‚Ä¢ Value iteration algorithms [16, 19]                                                                                                                                                                                 | üëì memo turns recursive theory of mind models into array programs where:<br>‚Ä¢ Arrays represent belief distributions<br>‚Ä¢ chooses introduces dimensions<br>‚Ä¢ observes normalizes dimensions<br>‚Ä¢ E[e] uses tensor contraction                                                    | Fig 6: Array representation of listener's beliefs<br>Demonstration of array operations matching Bayesian updates                  |
| 4.1 Case studies                             | How does memo compare to expert implementations of classic theory of mind models?                                          | ‚Ä¢ Scalar implicature [65, 79]<br>‚Ä¢ Schelling coordination games [116, 125]<br>‚Ä¢ MDP planning [19]<br>‚Ä¢ POMDP reasoning [7, 85]                                                                                                                                                               | üó∫Ô∏è memo implementations are typically:<br>‚Ä¢ Shorter (15-60 lines vs 25-199 lines)<br>‚Ä¢ Faster (often 30-200√ó speedup)<br>‚Ä¢ Less prone to subtle reasoning bugs<br>‚Ä¢ Amenable to GPU acceleration and autodiff                                                                  | Table 1: Benchmarks across multiple models<br>Fig 8: Visualization of MDP planning<br>Fig 9: Belief-space value function in POMDP |
| 4.2 memo in the wild                         | How does memo impact real-world research projects?                                                                         | ‚Ä¢ Computational models of lying [138]<br>‚Ä¢ Social relationship inference [81]<br>‚Ä¢ Empathetic explanation [30]<br>‚Ä¢ Caregiving models [88]                                                                                                                                                   | ü§ú memo enables ambitious research by:<br>‚Ä¢ Dramatically reducing code size (50-220 lines ‚Üí 38-120 lines)<br>‚Ä¢ Massively accelerating inference (up to 2,000,000√ó)<br>‚Ä¢ Supporting parameter fitting and cross-validation<br>‚Ä¢ Catching subtle bugs in counterfactual reasoning | Real-world examples of research projects using memo<br>Reports from researchers on productivity improvements                      |
| 4.3 Extensions                               | What novel applications does memo's approach enable?                                                                       | ‚Ä¢ Integration with neural networks [72, 73]<br>‚Ä¢ Resource-rational cognition [94, 133]<br>‚Ä¢ Game theory [63, 70]                                                                                                                                                                             | üß≠ memo's design enables novel modeling capabilities:<br>‚Ä¢ Integration with deep learning (e.g., RSA+neural vision)<br>‚Ä¢ Reasoning about computational cost as part of decision-making<br>‚Ä¢ GPU acceleration for scaling to larger models                                       | Fig 10: Font design with neural RSA<br>Fig 11: Inference about cognition from response time                                       |
| 5. Limitations and Future Work               | What are memo's limitations and potential future directions?                                                               | ‚Ä¢ Array-oriented probabilistic programming<br>‚Ä¢ Continuous distributions<br>‚Ä¢ Language and theory of mind [41, 99, 107]<br>‚Ä¢ LLMs and reasoning [127]                                                                                                                                        | üåè While limited to discrete domains with statically-known choice sequences, memo opens new research directions:<br>‚Ä¢ Understanding how specialized language enables theory of mind<br>‚Ä¢ Using memo as a "language of thought" to improve LLM reasoning                         | Acknowledgment of technical limitations and future research questions                                                             |

**Overall Contribution:** memo demonstrates how a domain-specific language with specialized syntax and semantics for agency, combined with efficient array-based inference, can dramatically simplify and accelerate computational models of theory of mind. By preventing common reasoning bugs and enabling rapid iteration, memo makes the recursive rationality paradigm more accessible to researchers across multiple disciplines.

- **Correctness**: Traditional probabilistic programming languages make it easy to introduce subtle bugs when modeling how agents reason about each other. Memo solves this by making agency explicit in its syntax - each random choice must be associated with an agent, which prevents "mind reading" or "mind control" bugs.
- **Efficiency**: Theory of mind models are typically very slow to run. Memo cleverly compiles these models to array programs that can be executed efficiently on modern hardware (including GPUs), making them dramatically faster.



### Error Decomposition in Statistical Modeling
- **Irreducible Error (œÉ¬≤Œµ)**: The inherent randomness in the phenomenon that cannot be eliminated
- **Bias¬≤**: The squared difference between expected prediction and true value, representing systematic error
- **Variance**: The variability of model prediction for a given point, representing sensitivity to sampling

### Testing Strategies
- **Market Viability Testing (MVT)**: Focuses on minimizing irreducible error + bias¬≤ through controlled experiments
- **Go-to-Market Testing (GMT)**: Focuses on minimizing bias¬≤ + variance through real-world implementation

### Sensor and Motion Models from Probabilistic Robotics
- **Sensor Noise**: Analogous to market feedback uncertainty (How reliable is the information we're receiving?)
- **Motion Noise (p_noise, hd_noise)**: Analogous to implementation uncertainty (How accurately can we execute?)

## Database Representation of Entrepreneurial Testing Framework
Creating this cohesive database table is a mandatory deliverable. The table must demonstrate both column-wise and row-wise cohesiveness, where relationships between cells maintain logical consistency.

| Testing Approach                   | Primary Error Components       | Founder Mindset                 | Under Low Uncertainty                | Under High Uncertainty                                      | Robotics Analogy                                                               |
| ---------------------------------- | ------------------------------ | ------------------------------- | ------------------------------------ | ----------------------------------------------------------- | ------------------------------------------------------------------------------ |
| **Market Viability Testing (MVT)** | Irreducible Error + Bias¬≤      | Analytical, Risk-averse         | Preferred by pessimistic founders    | Shifts to being preferred by optimistic founders            | High sensor noise, low motion noise - Need to improve perception before action |
| **Go-to-Market Testing (GMT)**     | Bias¬≤ + Variance               | Action-oriented, Experimental   | Preferred by optimistic founders     | Shifts to being preferred by pessimistic founders           | High motion noise, low sensor noise - Can explore despite imperfect perception |
| **Hybrid Approach**                | Dynamic balancing of all three | Adaptable, Learning-oriented    | Used when noise sources are balanced | Becomes optimal as uncertainty increases in both dimensions | Simultaneous Localization and Mapping (SLAM) - Learning while acting           |
| **Mathematical Foundation**        | Error = œÉ¬≤Œµ + bias¬≤ + variance | Quantitative decision framework | Error components can be estimated    | Error becomes harder to decompose                           | Bayesian inference using probabilistic sensor and motion models                |

| Testing Approach                   | Primary Error Components       |
| ---------------------------------- | ------------------------------ |
| **Market Viability Testing (MVT)** | Irreducible Error + Bias¬≤      |
| **Go-to-Market Testing (GMT)**     | Bias¬≤ + Variance               |
| **Hybrid Approach**                | Dynamic balancing of all three |