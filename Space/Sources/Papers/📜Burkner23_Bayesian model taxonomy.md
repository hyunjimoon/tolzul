---
collection:
  - "[[Papers]]"
author_ids:
  - burkner
  - bayesian
field:
  - üêÖcba
year: 2023
created: 2025-02-03
diagrams:
  - "[[üìúBurkner23_Bayesian model taxonomy 2025-04-11-8]]"
---

| Section/Subsection                                           | üîêResearch Question / Focus                                                  | üß±Literature Brick / Context                                                                                    | üîëKey Message / Contribution                                                                                                                                                                                                                                |
| :----------------------------------------------------------- | :--------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1. Introduction**                                          | Why clarify different meanings of ‚ÄúBayesian models‚Äù and how to evaluate them | *Probabilistic modeling surge*<br>*Principled Bayesian workflows*                                               | Sets up two main questions:<br>1) What actually is a Bayesian model?<br>2) What makes a good Bayesian model?                                                                                                                                                |
| **2. What is a Bayesian Model?**                             | Defining a unifying taxonomy (P, A, D)                                       | *Recent expansions of Bayesian frameworks*                                                                      | Proposes PAD taxonomy: P = joint distribution, A = approximator, D = data.                                                                                                                                                                                  |
| **2.1 P Models**                                             | How do joint distributions define Bayesian models?                           | *Generative modeling*<br>*Explicit vs. implicit likelihood*                                                     | Focus on joint p(y,Œ∏) factorizing into prior √ó likelihood; covers P<sub>E</sub> vs. P<sub>I</sub> (explicit vs. implicit)                                                                                                                                   |
| **2.2 PD Models**                                            | Why incorporate actual observed data Àúy?                                     | *Posterior derivation in practice*                                                                              | Once data are added, we get the ‚Äúideal‚Äù PD model with analytic posterior, usually unattainable in complex scenarios.                                                                                                                                        |
| **2.3 PA Models**                                            | How does a posterior approximator fit in?                                    | *Role of sampling/optimization algorithms*<br>*Amortized vs. non-amortized inference*                           | Tuples (p(y,Œ∏), p<sub>A</sub>(Œ∏\|y)) capturing approximate posteriors for a given P. Highlights MCMC, VI, ABC, etc.                                                                                                                                         |
| **2.4 PAD Models**                                           | Combining data and approximators: what emerges?                              | *Posterior approximation with real data*                                                                        | Fully specified Bayesian workflow: (P, A, D). Posterior misspecification issues & interplay between model & approximator.                                                                                                                                   |
| **2.5 Intermediate summary**                                 | Recap of the PAD taxonomy                                                    | *Summary of four model classes*                                                                                 | The ‚Äújoint distribution + approximator + data‚Äù perspective clarifies the meaning and scope of ‚ÄúBayesian model.‚Äù                                                                                                                                             |
| **3. What makes a good Bayesian model?**                     | Which criteria measure a model‚Äôs ‚Äúgoodness‚Äù?                                 | *Bayesian model evaluation frameworks*<br>*Holistic utility dimensions*                                         | Introduces 10 utility dimensions: (1) causal consistency, (2) parameter recoverability, (3) predictive performance, (4) fairness, (5) structural faithfulness, (6) parsimony, (7) interpretability, (8) convergence, (9) estimation speed, (10) robustness. |
| **3.1 Causal Consistency**                                   | How to ensure a model supports valid causal claims?                          | *Pearl‚Äôs structural causal models*<br>*Interventions, do-calculus*                                              | DeÔ¨Ånition via DAGs, interventions, the need for correctness of underlying structural assumptions if the model claims causal inference.                                                                                                                      |
| **3.1.1 Structural Causal Models**                           | What is a structural perspective on causation?                               | *Directed acyclic graphs (DAGs)*<br>*Functional equations*                                                      | Variables generated by structural equations g. Graph edges encode causal directions. Model must align with these assumptions to be ‚Äúcausally consistent.‚Äù                                                                                                   |
| **3.1.2 Interventions**                                      | How do do-operators alter model structure?                                   | *Pearl‚Äôs do-calculus*                                                                                           | Changing a node‚Äôs value severs edges from its parents; must check if model can handle p(y\|do(x))                                                                                                                                                           |
| **3.2 Parameter Recoverability**                             | Can we trust the inferred latent parameters and their uncertainty?           | *Identifiability*<br>*Frequentist vs Bayesian calibration*                                                      | Extent to which data inform unknown parameters. Includes identifiability, global information gain, calibration measures, simulation-based checks.                                                                                                           |
| **3.2.1 Identifiability and Information Gain**               | When does data reduce parameter uncertainty?                                 | *Local vs global identifiability*<br>*Posterior contraction*<br>*Bayesian surprise*                             | Posterior narrower than prior ‚Üí real information gained. If ‚Äúno contraction,‚Äù no identifiability.                                                                                                                                                           |
| **3.2.2 Ground-Truth Comparisons**                           | Are parameters accurately recovered under known conditions?                  | *Simulation studies*<br>*Frequentist coverage*<br>*Point vs interval estimates*                                 | Evaluate bias, coverage, and calibration by generating data from known ‚Äútrue‚Äù parameters.                                                                                                                                                                   |
| **3.2.3 Calibration of Posterior Approximations**            | Are approximate posteriors well aligned with the analytic posterior?         | *Simulation-based calibration (SBC)*<br>*Rank histograms, ECDF difference*                                      | SBC checks ‚Äúself-consistency‚Äù; uniform rank distributions suggest approximate posterior is correct if the model is well-specified.                                                                                                                          |
| **3.3 Predictive Performance**                               | How well does the model predict new data or future observations?             | *Cross-validation*<br>*Information criteria*<br>*Predictive checks*                                             | Key for many machine learning tasks. Considers absolute vs relative, prior vs posterior, in-sample vs out-of-sample.                                                                                                                                        |
| **3.3.1 Absolute and Relative Predictive Performance**       | How to compare predictive metrics with or without a known optimum?           | *RMSE, R¬≤, classification accuracy, etc.*                                                                       | Often we compare models relatively, but some tasks allow ‚Äúoptimal‚Äù references.                                                                                                                                                                              |
| **3.3.2 Prior and Posterior Predictive Performance**         | Should we test predictions before or after conditioning on data?             | *Marginal likelihood, Bayes factors*<br>*Gibbs loss, ELPD*                                                      | Tests theoretical alignment (prior predictive) vs updated model adequacy (posterior predictive).                                                                                                                                                            |
| **3.3.3 In-Sample and Out-of-Sample Predictive Performance** | Does the model generalize beyond the training set?                           | *Cross-validation, LOOCV, LOOIC/WAIC*                                                                           | Out-of-sample performance is essential to avoid overfitting. In-sample is an upper bound.                                                                                                                                                                   |
| **3.3.4 Predictions in a Dynamic World**                     | How do changing conditions affect model predictions?                         | *Non-stationary processes*<br>*Temporal model misspecification*                                                 | Real data generating processes may shift over time; static P(A)D model can fail if environment changes.                                                                                                                                                     |
| **3.4 Fairness**                                             | How to ensure model-based decisions are equitable across protected groups?   | *Anti-classification vs classiÔ¨Åcation parity*<br>*Causal fairness approaches*                                   | Highlights ethics & fairness constraints. DiÔ¨Äerent formal definitions can conÔ¨Çict; domain knowledge needed.                                                                                                                                                 |
| **3.5 Structural Faithfulness**                              | Does the model align with known structural or domain constraints?            | *Probabilistic structure*<br>*Physical constraints*<br>*Mechanistic modeling*                                   | ‚ÄúWhat we know about the system‚Äôs data-generation‚Äù must be embedded: variable scales, correlation structures, invariants.                                                                                                                                    |
| **3.5.1 Variable Scales**                                    | Are continuous, discrete, or ordinal variables handled properly?             | *Proper likelihood families*<br>*Choice of distributions*                                                       | Mismatch between data scale and model distribution leads to errors or impossible predictions.                                                                                                                                                               |
| **3.5.2 Probabilistic Structures**                           | How to capture dependencies among observations?                              | *Multilevel models*<br>*Exchangeability assumptions*                                                            | Hierarchical structures, random eÔ¨Äects, or correlation patterns require specialized forms (e.g., temporal, spatial).                                                                                                                                        |
| **3.5.3 Physical Constraints**                               | Enforcing known invariants or laws                                           | *Mechanistic simulators*<br>*Physics-informed models*                                                           | Hard constraints from domain knowledge can reduce parameter dimension and boost interpretability & data efficiency.                                                                                                                                         |
| **3.6 Parsimony**                                            | Is simpler (fewer parameters) always better?                                 | *Occam‚Äôs razor*<br>*Complexity-penalizing measures*                                                             | Favors simpler models if their performance is adequate. Distinction between P-parsimony and A-parsimony.                                                                                                                                                    |
| **3.6.1 P-Parsimony**                                        | How to measure structural simplicity of a P model?                           | *Effective number of parameters (ENP)*<br>*Marginal likelihood (Bayesian evidence)*                             | ENP captures how prior and hierarchical shrinkage reduce dimensionality beyond raw parameter count.                                                                                                                                                         |
| **3.6.2 A-Parsimony**                                        | How ‚Äúsimple‚Äù are posterior approximators themselves?                         | *Hyperparameter tuning*<br>*Implementation complexity*                                                          | Fewer sensitive hyperparameters ‚Üí more robust, easier to compare. Complex approximators (e.g., neural surrogates) can hamper reproducibility.                                                                                                               |
| **3.7 Interpretability**                                     | Can we understand model internals and the meaning of parameters?             | *Intrinsic vs. post-hoc interpretations*<br>*Explanatory vs. predictive modeling*                               | Distinguishes interpretability of P<sub>I</sub> (mechanistic) vs. P<sub>E</sub> (statistical). High dimensional or non-linear often hamper direct interpretation.                                                                                           |
| **3.7.1 Interpretability of P<sub>I</sub> models**           | Do parameters correspond to real-world states or processes?                  | *Mechanistic models*<br>*Physical or domain-specific interpretation*                                            | Usually clearer first-order meaning of each parameter, but interactions may be complex. Simulations essential for exploring high-dimensional P<sub>I</sub>.                                                                                                 |
| **3.7.2 Interpretability of P<sub>E</sub> models**           | Are purely statistical parameters easy to interpret?                         | *Regression coefficients*<br>*Linear vs. non-linear terms*                                                      | Parameter meaning typically is ‚Äúwithin-model‚Äù only, unless carefully chosen structure (e.g., partial pooling). Complexity can bury interpretability.                                                                                                        |
| **3.8 Convergence**                                          | Is the approximated posterior close enough to the analytic posterior?        | *Diagnostics for MCMC, VI, SMC, ABC, neural surrogates*                                                         | Checking if we have run the approximator long/truly enough: specialized methods for each algorithm class.                                                                                                                                                   |
| **3.8.1 Convergence of Markov Chain Monte Carlo**            | How to check MCMC chain stationarity?                                        | *R-hat (Gelman-Rubin)*<br>*ESS, MCSE*<br>*Divergent transitions in HMC*                                         | Ensures draws are valid samples from the true stationary distribution; partial or poor convergence can bias all inferences.                                                                                                                                 |
| **3.8.2 Convergence of Optimization-Based Algorithms**       | How to detect local minima or insufficient optimization?                     | *Gradient-based solvers (LBFGS, ADAM)*<br>*Convergence criteria*                                                | Minimal gradient norms, stable objective, plus regular checks for partial solutions.                                                                                                                                                                        |
| **3.8.3 Convergence of Sequential Monte Carlo**              | How to confirm SMC approximates the target posterior well?                   | *Particle filters*<br>*SMC samplers*<br>*ESS of weighted particles*                                             | Each iteration‚Äôs convergence can be tested (ESS, kernel mixing). Diagnosing global correctness is trickier.                                                                                                                                                 |
| **3.8.4 Convergence of Approximate Bayesian Computation**    | Rejection/ABC-MCMC adjustments vs. too large distances?                      | *Distance thresholds*<br>*Sufficiency of summary statistics*                                                    | Minimal residual distance does not guarantee zero approximation error, especially in high dimensions.                                                                                                                                                       |
| **3.8.5 Convergence of Amortized Approximators**             | How to ensure neural approaches converge globally?                           | *Simulation-based training*<br>*Maximum mean discrepancy for data coverage*                                     | Diagnosing partial/mis-specified coverage: can pass local checks but fail real data if model mismatch (simulation gap).                                                                                                                                     |
| **3.9 Estimation Speed**                                     | How long does it take to reach a converged posterior approximation?          | *Implementation factors (parallelization, GPU)*<br>*Warm-up vs. main runs*                                      | Speed vs. accuracy trade-offs: faster methods (VI, surrogates) may be less accurate or have fewer guarantees.                                                                                                                                               |
| **3.9.1 Sampling Efficiency**                                | For samplers, how many effectively independent draws per second?             | *ESS / second measures*<br>*Chain mixing diagnostics*                                                           | Key metric for comparing MCMC and SMC approaches. Must also check correctness.                                                                                                                                                                              |
| **3.9.2 Estimation Speed of Amortized Approximators**        | How do we measure time cost with pre-trained networks?                       | *Training vs. inference time*<br>*Amortization break-even point*                                                | Expensive upfront training, then extremely quick inference. Useful when analyzing many data sets or repeated queries.                                                                                                                                       |
| **3.10 Robustness**                                          | How stable are results under small perturbations?                            | *Sensitivity analysis*<br>*Power-scaling priors*<br>*Omitted data checks*                                       | Investigate how changing priors, data subsets, or hyperparameters affects conclusions.                                                                                                                                                                      |
| **4. Utility Hierarchies and Trade-offs**                    | How to combine these 10 dimensions for holistic comparison?                  | *Ranking vs. weighting approaches*<br>*Application-specific priorities*                                         | Summaries with ‚Äútrees‚Äù of utilities, focusing on either observable or latent goals.                                                                                                                                                                         |
| **4.1 Utility Tree for Observable Inferential Goals**        | When the main purpose is predicting real outcomes                            | *Predictive modeling contexts*<br>*Machine learning tasks*                                                      | Primary: fairness if relevant ‚Üí predictive performance<br>Secondary: speed, interpretability, robustness<br>Tertiary: causal, parsimony, structural faithfulness, parameter recoverability, convergence.                                                    |
| **4.1.1 Primary Utilities**                                  | Which are absolutely required before all else?                               | *Ethical constraints (fairness)*<br>*Predictive accuracy*                                                       | A model failing fairness or predictive success is unacceptable.                                                                                                                                                                                             |
| **4.1.2 Secondary Utilities**                                | What is next in importance for predictions?                                  | *Estimation speed*<br>*Interpretability*<br>*Robustness*                                                        | May trade some predictive performance for faster or more transparent results.                                                                                                                                                                               |
| **4.1.3 Tertiary Utilities**                                 | Which utilities can be used as proxies or tie-breakers?                      | *Causal consistency*<br>*Parameter recoverability*<br>*Parsimony*<br>*Structural faithfulness*<br>*Convergence* | If multiple models are equally strong on primary and secondary, these help refine final choice.                                                                                                                                                             |
| **4.2 Utility Tree for Latent Inferential Goals**            | If the key task is to recover hidden parameters                              | *Scientific explanation, theory-driven models*                                                                  | Primary: fairness if relevant ‚Üí causal consistency ‚Üí convergence<br>Secondary: parameter recoverability, interpretability, speed, robustness<br>Tertiary: parsimony, structure, predictive performance.                                                     |
| **4.2.1 Primary Utilities**                                  | Non-negotiable for latent inquiries                                          | *Causal claims*<br>*Posterior correctness (convergence)*                                                        | Must ensure the model is structurally valid re: causal assumptions & numerically stable.                                                                                                                                                                    |
| **4.2.2 Secondary Utilities**                                | Practical constraints or high-level goals?                                   | *Parameter recoverability*<br>*Interpretability*<br>*Estimation speed*<br>*Robustness*                          | Focus of interest: can trade some for more speed or interpretability, depending on context.                                                                                                                                                                 |
| **4.2.3 Tertiary Utilities**                                 | Additional supportive factors                                                | *Parsimony*<br>*Structural faithfulness*<br>*Predictive performance*                                            | They can guide model building but are lower priority if the goal is purely latent understanding.                                                                                                                                                            |
| **5. Conclusion**                                            | How does the PAD taxonomy help future Bayesian workÔ¨Çows?                     | *Unified language for modern Bayesian models*<br>*Comprehensive utility dimensions*                             | Emphasizes that Bayesian models are more than prior √ó likelihood ‚Üí includes approximators & data. Ten utility dimensions unify their holistic evaluation.                                                                                                   |
