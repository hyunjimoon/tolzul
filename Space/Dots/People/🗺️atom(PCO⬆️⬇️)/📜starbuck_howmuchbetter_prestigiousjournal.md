---
collection:
- '[[People]]'
field:
- 🐙ops
- 🐢inv
- 👾cog
atom: 🗺️atom(PCO⬆️⬇️)
created: '2025-04-09'
---

### 🧱🗄️3🖼️3 Key Resources Summary:

#### 🗄️2: Comparison with existing literature

This table synthesizes how the findings on journal stratification compare across disciplines (economics, management, psychology, sociology), showing decreasing stratification over time in most fields contrary to Ellison's (2002) findings in economics. This challenges fundamental assumptions behind increasing emphasis on top-tier publications.

#### 🗄️3: Tradeoff for application (Confidence limits for review quality)

This crucial table quantifies the relationship between reviewer agreement (measured in multiple journals) and ability to assess manuscript quality, establishing a theoretical framework for evaluation reliability. It demonstrates the statistical limits of peer review with important implications for institutional evaluation practices.

#### 🖼️💜💚 1: Need-Solution Mapping (Journal prestige vs. article quality)

Visualizes the fundamental disconnect between journal prestige and actual article quality, showing substantial overlap between tiers. The purple component illustrates the problem (unreliable quality signals) while the green component demonstrates the statistical framework that explains this mismatch.

#### 🖼️💜💚 6: Distribution of Manuscripts (Need-Solution Visualization)

Shows the actual distribution of manuscript quality after review decisions, visually demonstrating how even high-quality papers face significant rejection rates while many lower-quality papers are accepted. This compelling visualization shows the fundamental problem (purple) and quantifies the statistical limitations (green).

#### 🖼️🔴(💜💚) 11: Quality Distribution Across Journal Tiers (Application)

This key implementation visual shows the practical implications of the statistical analysis - how manuscript quality is actually distributed across journal tiers after the full submission process. The red component demonstrates the practical outcomes of the current review system, showing substantial overlap of quality across journal tiers.

This analysis systematically dismantles assumptions about journal prestige through statistical modeling, revealing substantial quality overlap across journal tiers and demonstrating the limitations of using publication venue as a primary measure of research quality.

#### 🗄️1 Table of Contents

|Section/Subsection|Mode|Questions|🧱 Literature Brick|Answer|Tables/Figures|
|---|---|---|---|---|---|
|1. Introduction|💭 Theorizing|Why do social science departments and business schools place extreme emphasis on top-tier (A) journal publications despite apparent limitations?|• Gans & Shepherd (1994): Documented rejection of influential papers<br>• Peer review literature showing high variability<br>• Social construction of journal prestige|The emphasis on A-journals appears to be an administrative choice rather than evidence-based practice. While higher-prestige journals generally publish more high-value articles, substantial quality overlap exists between journal tiers. This makes judging article quality solely by publication venue problematic and requires statistical analysis to understand.|🖼️💜💚 1: Relationship between journal prestige and article value overlap|
|2. Citation Trends in Social Sciences|📐 Productizing|Have citation patterns become increasingly concentrated in top-tier journals, potentially justifying emphasis on them?|• Ellison (2002): Finding that economics journals show increasing stratification<br>• Literature on impact factors (Amin & Mabe, 2000)<br>• Research on departmental prestige dynamics|Contrary to expectations, citation data show decreased stratification across journal tiers in management, psychology and sociology over time, with only economics showing increased stratification. This contradicts the assumption that top journals are becoming more dominant, suggesting focus on top-tier publications is driven by status-seeking behavior rather than citation patterns.|🖼️2: Ratios of Impact Factors across journal fields (1981-2002)<br>🗄️2: Comparison of citation patterns across disciplines|
|3. Statistical Theory of Review Processes|💭 Theorizing|How can we statistically model peer review to assess relationship between reviewer judgments and manuscript quality?|• Rousseeuw (1991): Statistical analysis of review errors<br>• Cicchetti (2003): Analysis of editorial decisions<br>• Peters & Ceci (1982): Study showing high rejection of previously published papers|A statistical model can estimate correlations between reviewers' judgments and manuscripts' "true value" (Rho). Analysis of interrater agreement across journals suggests Rho likely falls between 0.15-0.45, implying significant randomness in selection processes. This explains why excellent manuscripts face high rejection probabilities and mediocre papers get published in top journals.|🗄️3: Confidence limits for correlations between reviewer judgments and manuscript quality<br>🖼️3: Relationship between reviewer opinions and true manuscript values|
|4. Outcomes of First Reviews|🛠️ Skill|What percentage of truly valuable manuscripts are accepted or rejected after initial review?|• Stinchcombe & Ofshe (1969): Statistical modeling of review process<br>• Lindsey (1978): Documented review limitations<br>• Gottfredson (1978): Reviewer accuracy studies|The review process produces substantial errors: With Rho=0.15, only 26% of accepted manuscripts belong in highest-value 20%, while 12% of rejected manuscripts are high quality. Even at Rho=0.45 (unlikely), 6% of rejected manuscripts belong in highest-value 20%. The selection process discriminates so poorly that "blind review" has unintended meaning.|🖼️5: Percentages of manuscripts receiving accepts/rejects<br>🖼️💜💚 6: Distribution of high-value manuscripts after review (showing poor selection efficiency)|
|5. Editorial Decisions After Multiple Reviews|🛠️ Skill|How do revisions, resubmissions, and editorial intervention affect final selection outcomes?|• Bedeian (2003, 2004): Author compliance with reviewers<br>• Frey (2003): Critique of publishing incentives<br>• Cicchetti (1991): Editor decision patterns|Even after multiple reviews and editor intervention, the process cannot overcome inherent judgment limitations. With the most plausible Rho=0.3, over 50% of truly excellent manuscripts are rejected while many mediocre manuscripts are published. Editorial intervention allows journals to meet target acceptance rates (15-25%) but doesn't significantly improve quality selection.|🖼️7: Distribution of manuscripts after multiple review rounds<br>🖼️8: Selection outcomes showing error rates|
|6. Quality Differences Between Journal Tiers|📐 Productizing|What is the actual distribution of article quality across journal prestige tiers in a sequential submission system?|• Campanario (1996, 1998): Rejections of influential papers<br>• Work on journal stratification<br>• Social construction of academic value|Models assuming sequential submission to journals in descending prestige tiers show substantial quality overlap between journal ranks. With Rho=0.3 (most plausible estimate), first-quintile journals contain only 43% articles from highest-value 20%, second/third-quintiles contain 29%, and fourth/fifth-quintiles contain 13%. This reveals significant quality overlap across journal tiers.|🗄️5: Distributions of manuscripts' true values across journal quintiles<br>🖼️🔴(💜💚) 11: Quality distribution visualization across journal tiers|
|7. Implications for Faculty Evaluation|💸 Evaluating|How reliable are counts of top-tier publications for evaluating faculty research quality?|• Trieschmann et al. (2000): Publication patterns in business schools<br>• DiMaggio & Powell (1983): Institutional isomorphism<br>• Literature on academic evaluation systems|Publication venue provides only a noisy signal of research quality, creating wide confidence intervals for quality inferences. For a faculty with 20 publications, having 5 in top-tier journals could indicate author quality ranging from 35th-80th percentile. Using only publication venue introduces significant randomness into personnel decisions, potentially benefiting elite institutions while disadvantaging others.|🖼️14-15: Percentile of author's quality vs. publication venue<br>🗄️3: Trade-offs in evaluation approaches - confidence intervals for quality assessment|
|8. Conclusion|👁️ Vision|What are the broader implications for academic evaluation systems and knowledge development?|• Gottfredson et al. (1977): Citation patterns<br>• Social construction of scientific quality<br>• Matthew effect in science literature|Emphasis on top-tier publications introduces substantial randomness into evaluation processes while benefiting elite institutions. Knowledge development may be impeded when mediocre research receives the endorsement of high visibility (57% of top-tier articles don't belong in highest-value 20%). Institutions should triangulate multiple sources of evidence rather than rely solely on publication venue.|🗄️2: Comparison with alternative evaluation approaches showing their relative advantages|


---


|Section/Subsection|🔐Research Question|🧱Literature Brick|🔑Key Message|📊Empirical Evidence|
|---|---|---|---|---|
|1. Introduction|Why do some business schools and departments place extreme emphasis on top-tier (A) journal publications despite apparent flaws in this approach?|• Gans & Shepherd (1994): Stories from leading economists about rejected influential papers<br>• Personal editorial experience on 15 journals across 4 fields|🧠While journals with higher prestige generally publish more high-value articles, there is substantial value overlap between journal tiers that makes judging articles solely by publication venue problematic|Anecdotal evidence of prestigious journals rejecting highly influential papers and lower-tier journals publishing excellent work|
|2. Citation Trends in Social Sciences|Have citations become increasingly concentrated in top-tier journals across social sciences, potentially justifying increased emphasis on them?|• Ellison (2002): Observation of increasing concentration of citations in top economics journals<br>• Data on impact factors from 1981-2002 for journals in economics, management, psychology, and sociology|🧠🌏Except in economics, citation data show decreased stratification across journal tiers over time, suggesting administrative choice rather than adaptation to field-wide trends drives the focus on top-tier publications|Fig 1-2: Impact factors showing economics journals becoming more stratified while management, psychology and sociology became less stratified<br>• Data from 102 economics journals, 95 management journals, 90 psychology journals, and 47 sociology journals|
|3. Statistical Theory of Review Processes|How accurately do editorial review processes identify manuscripts with high true value?|• Rousseeuw (1991): Analysis of review errors<br>• Cicchetti (2003): Observations about editorial decisions<br>• Lindsey (1978): Variance in reviewer qualification<br>• Peters & Ceci (1982): Resubmission experiment|🧭A statistical model can estimate correlations between reviewers' judgments and manuscripts' true value (Rho) by analyzing interrater agreement data from journals<br>• Low Rho (0.15-0.45) implies significant randomness in selection|Table 1-2: Frequencies of reviewer recommendations<br>Table 3-4: Confidence limits for correlations between reviewers' opinions and manuscript true values<br>Fig 3-4: Ratios of correlations between reviewers' opinions and manuscripts' true values|
|4. Outcomes of First Reviews|What percentage of truly valuable manuscripts are rejected or accepted after initial review?|• Bedeian (2003, 2004): Analysis of review processes<br>• Frey (2003): Critique of publishing incentives|🧭Given low correlations between reviewers' judgments and true quality, review processes result in both type I errors (accepting low-value manuscripts) and type II errors (rejecting high-value manuscripts)<br>• When Rho=0.3 (middle estimate), about half of accepted manuscripts do not belong in highest-value 20%|Fig 5-6: Distributions showing that with Rho=0.15, 12% of rejected manuscripts belong in highest-value 20%, while with Rho=0.45, only 6% do<br>• With Rho=0.15, only 26% of accepted manuscripts belong in highest-value 20% vs. 78% with Rho=0.45|
|5. Editorial Decisions After Multiple Reviews|How do revisions and editorial intervention affect manuscript selection outcomes?|• Bedeian (2003): Review compliance pressure<br>• Mahoney (1977, 1979): Confirmatory bias in reviewing|🧭Even after revisions and editor intervention, selection processes cannot overcome inherent limitations of reviewers' judgments<br>• When Rho=0.3, over half of truly excellent manuscripts get rejected and many suboptimal manuscripts get published|Fig 7-8: Distributions of manuscript acceptance/rejection after revisions<br>• After full review process with Rho=0.3, 43% of manuscripts in highest-value 20% receive accepts|
|6. Quality Differences Between Journal Tiers|What is the true distribution of article quality across journal prestige tiers?|• Data on impact factors across journal quintiles<br>• Analysis of citation patterns in different fields|🧍‍♀️🗺️Models assuming sequential submission to journals in descending prestige tiers show substantial quality overlap<br>• With Rho=0.3, first-quintile journals contain 43% articles from highest-value 20%, second/third-quintiles contain 29%, and fourth/fifth-quintiles contain 13%|Fig 9-11: Distributions of manuscript quality by journal tier<br>Table 5: Distributions of manuscripts' true values for journals in different quintiles<br>Table 6-7: Quality stratification compared with observed impact factor ratios|
|7. Implications for Faculty Evaluation|How reliable are counts of top-tier publications for evaluating faculty quality?|• Trieschmann et al. (2000): Research performance in business schools|👓Limited sample sizes in faculty evaluation create wide confidence intervals for quality inferences based solely on publication venues<br>• Using only publication counts in top journals introduces significant randomness into personnel decisions<br>• Elite institutions may benefit while others disadvantage themselves|Fig 12-15: Expected distributions of manuscripts by authors with different average quality levels<br>Table 8: Means and probable ranges of true values of journals<br>• For 20 publications, having 5 in top-tier journals could indicate author quality ranging from 35th to 80th percentile|
|8. Conclusion|What are the broader implications for research evaluation and knowledge development?|• DiMaggio & Powell (1983): Institutional isomorphism<br>• Starbuck (2003): Value in peer reviews|🗺️🌏Extreme emphasis on top-tier publications likely introduces randomness into most evaluation contexts while benefiting elite institutions<br>• Knowledge development may be impeded when mediocre research receives endorsement of high visibility<br>• Triangulating multiple sources of evidence produces more reliable evaluation|Summary of key findings:<br>• 57% of articles in top-tier journals don't belong in highest-value 20%<br>• Over half of truly excellent manuscripts get rejected<br>• Citation behavior doesn't support exclusive focus on high-prestige journals|

This table presents a comprehensive analysis of Starbuck's statistical examination of academic publishing, showing how editorial review processes generate substantial overlap in article quality across journal prestige tiers. The key insight is that when reviewer judgments correlate only moderately with manuscripts' true quality (Rho≈0.3), publication venue becomes a noisy signal of research value, with implications for faculty evaluation, institutional status, and knowledge development.