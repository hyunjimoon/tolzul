---
collection:
  - "[[Space/Lab/Papers]]"
author_ids:
  - John Sillince
  - Paula Jarzabkowski
  - Duncan Shaw
field:
  - ğŸ‘¾cog  # Cognition
  - ğŸ¯str  # Strategy
  - ğŸ”¬mth  # Methodology
thesisPaper: U
thesisChapter: T
year: 2012
rank: 9  # Critical for measurement
research_stream:
  - Strategic Ambiguity
  - Rhetorical Construction
  - Measurement
tags:
  - rhetoric
  - language-analysis
  - operationalization
  - measurement
  - empirical-qualitative
  - oil-framework
  - qualifying-exam
created: 2025-11-05
modified:
  - 2025-11-05T00:00:00-05:00
connections:
  extends:
    - "[[ğŸ“œEisenberg1984_Ambiguity_Communication]]"  # Ambiguity concept
  applied_in:
    - Your empirical measurement strategy
  related_to:
    - "[[ğŸ“œAbdallah2014_DoubleEdge_Ambiguity]]"  # What they measure
---

# ğŸ“œ Shaping Strategic Action Through Rhetorical Construction of Ambiguity

## ğŸ—„ï¸1: Core Framework (Q&A Format)

| Section | ğŸ”Research Question | ğŸ”‘Key Message & Framework | ğŸ“Formal Concept | ğŸ§±Literature Brick |
|---------|-------------------|-------------------------|-----------------|-------------------|
| **Main Thesis** | How is ambiguity actively constructed? | Ambiguity is NOT absence of clarity but ACTIVE RHETORICAL ACHIEVEMENT through specific linguistic devices | Ï„ as measurable linguistic property | â€¢ Rhetoric theory<br>â€¢ Green (1995) rhetoric strategy |
| **Operationalization** | How to measure Ï„? | COUNT linguistic features: abstract terms, hedging, modal verbs (â†’ low Ï„) vs concrete terms, specifics (â†’ high Ï„) | **Ï„-index**: (Concrete + Specific) / (Abstract + Hedges) | â€¢ Computational linguistics<br>â€¢ Content analysis methods |
| **Strategic Use** | Why construct ambiguity? | Enable action by allowing multiple stakeholder groups to see their interests while maintaining flexibility | Active construction (not accident) | â€¢ Burke (1969) rhetoric<br>â€¢ Suddaby & Greenwood (2005) rhetoric |

## ğŸ—„ï¸2: Theoretical Position

### Extends
- [[ğŸ“œEisenberg1984_Ambiguity_Communication]]: From "ambiguity exists" â†’ "how it's made"
- **Key Addition**: OPERATIONALIZATION - how to measure ambiguity

### **CRITICAL for Your Empirical Paper**
This paper provides METHOD to measure Ï„ from text

### Related Work
- [[ğŸ“œAbdallah2014_DoubleEdge_Ambiguity]]: Provides theory of what to measure; Sillince shows HOW

## ğŸ—„ï¸3: Linguistic Devices for Ambiguity

| Device Type | Increases Ambiguity (â†“Ï„) | Decreases Ambiguity (â†‘Ï„) |
|-------------|------------------------|-------------------------|
| **Vocabulary** | Abstract nouns ("value", "transformation") | Concrete nouns ("revenue", "product X") |
| **Specificity** | General statements | Specific numbers/metrics |
| **Modality** | "might", "could", "possibly" | "will", "is", "must" |
| **Hedging** | "arguably", "potentially" | Direct assertions |
| **Metaphors** | Rich metaphors (multiple mappings) | Literal language |
| **Quantification** | Vague quantities ("several", "many") | Exact quantities ("12", "50%") |

## ğŸ’­ Critical Insights for OIL Framework

### THE Measurement Paper

**Your Challenge**: How to measure Ï„ (promise precision) in venture data?

**Sillince's Answer**: COUNT linguistic features in company descriptions

### Computational Approach

```python
def measure_tau(text):
    """
    Measure promise precision from company description
    Higher score = Higher Ï„ (more precise)
    Lower score = Lower Ï„ (more vague)
    """
    # Precision indicators (increase Ï„)
    concrete_terms = count_concrete_nouns(text)
    specific_metrics = count_numbers_percentages(text)
    definitive_language = count_modal_certainty(text)  # will, is, must
    
    # Vagueness indicators (decrease Ï„)
    abstract_terms = count_abstract_nouns(text)
    hedging = count_hedge_words(text)  # potentially, arguably
    modal_uncertainty = count_modal_possibility(text)  # might, could
    metaphors = count_metaphorical_language(text)
    
    # Calculate Ï„-index
    precision_score = concrete_terms + specific_metrics + definitive_language
    vagueness_score = abstract_terms + hedging + modal_uncertainty + metaphors
    
    tau_index = precision_score / (vagueness_score + 1)  # +1 to avoid division by zero
    
    return tau_index
```

### Example Analysis

**High Ï„ (Precise Promise)**:
> "We will capture 15% market share in cloud storage by delivering 99.99% uptime through our patented compression algorithm."

- Specific metric: "15% market share"
- Concrete deliverable: "99.99% uptime"
- Definitive: "will capture"
- Technical specificity: "patented compression algorithm"
- **Ï„-index: HIGH**

**Low Ï„ (Vague Promise)**:
> "We're transforming how organizations think about data, potentially enabling new paradigms of information management through innovative approaches."

- Abstract: "transforming", "paradigms"
- Hedging: "potentially"
- Vague: "innovative approaches"
- Metaphorical: "think about data"
- **Ï„-index: LOW**

## ğŸ¯ Research Implications

### For Your Empirical Paper

**CRITICAL ENABLER**:
Without Sillince's framework, you cannot measure Ï„
With it, you can process large-scale text data

### Measurement Strategy

```python
# For each venture in dataset
for venture in ventures:
    # Get company descriptions over time
    desc_t0 = venture.description_at_founding
    desc_t1 = venture.description_at_series_a
    desc_t2 = venture.description_at_growth
    
    # Measure Ï„ trajectory
    tau_0 = measure_tau(desc_t0)
    tau_1 = measure_tau(desc_t1)
    tau_2 = measure_tau(desc_t2)
    
    # Calculate evolution
    delta_tau_early = tau_1 - tau_0
    delta_tau_late = tau_2 - tau_1
    
    # Test hypotheses
    # H1: tau_0 â†’ early_funding
    # H2: delta_tau_late â†’ later_success
```

### Validation Approaches

**1. Inter-rater Reliability**:
- Have humans rate sample on "vague" vs "precise"
- Compare with computational Ï„-index
- Should correlate r > 0.7

**2. Known Cases**:
- Tesla early (expect low Ï„): "sustainable transportation"
- vs later (expect high Ï„): "500,000 Model 3s/year"
- Measure Ï„ trajectory matches expected

**3. Cross-sectional Validation**:
- Compare ventures in different industries
- Tech should have lower Ï„ than manufacturing?
- Mission-driven lower Ï„ than profit-focused?

## ğŸ”¬ Detailed Operationalization

### Dictionary Development

**Concrete Terms** (increase Ï„):
- Products: "smartphone", "software", "vehicle"
- Metrics: "units", "revenue", "users"
- Specifications: "algorithm", "patent", "protocol"

**Abstract Terms** (decrease Ï„):
- Concepts: "innovation", "transformation", "paradigm"
- Values: "sustainability", "empowerment", "impact"
- Processes: "enabling", "facilitating", "disrupting"

**Hedge Words** (decrease Ï„):
- "potentially", "arguably", "possibly"
- "might", "could", "may"
- "somewhat", "relatively", "partially"

### Contextual Coding

**Important**: Same word can be concrete or abstract

"Platform" in context:
- Concrete: "Our iOS platform handles 1M requests/second"
- Abstract: "We're building a platform for the future of work"

**Solution**: Use NLP to capture context (not just word counting)

## ğŸ–¼ï¸ Rhetorical Ambiguity Framework

```
Low Ï„ Construction:           High Ï„ Construction:
                             
Abstract Nouns               Concrete Nouns
    +                            +
Modal Uncertainty            Definitive Statements
    +                            +
Hedging Language             Specific Metrics
    +                            +
Metaphors                    Literal Description
    â†“                            â†“
AMBIGUOUS MESSAGE            PRECISE MESSAGE
    â†“                            â†“
Multiple Interpretations     Single Interpretation
    â†“                            â†“
Unified Diversity            Narrow Focus
```

## âœ… Action Items for ä¸­êµ°ë‹˜

- [ ] **CRITICAL FOR EMPIRICS**: This paper enables your measurement
- [ ] **Build Ï„-index**: Implement computational measure
- [ ] **Validate**: Test on known cases (Tesla, Better Place)
- [ ] **Scale**: Apply to all 29 ventures in dataset
- [ ] **Test H1 & H2**: With measured Ï„ values

## ğŸ“š Implementation Sequence

**Week 1**: 
1. Read Sillince carefully (focus on linguistic devices)
2. Build dictionary of concrete/abstract/hedge terms

**Week 2**:
3. Implement Ï„-index calculator in Python
4. Test on 5 sample ventures
5. Validate against human raters

**Week 3**:
6. Apply to full dataset
7. Check distribution (any outliers?)
8. Run H1 and H2 regressions

---

**í•µì‹¬ for í•„ì‚¬ì¦‰ìƒ**: 
ì´ ë…¼ë¬¸ = Your empirical paperì˜ í•µì‹¬ infrastructure
Without Sillince: Cannot measure Ï„
With Sillince: Can process all 29 ventures + test hypotheses
