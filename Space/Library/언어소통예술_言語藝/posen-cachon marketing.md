---
modified:
  - 2025-11-30T03:25:17-05:00
---

# WHAT MAKES A THEORETICAL CONTRIBUTION? A FRAMEWORK FOR EVALUATING AND FRAMING RESEARCH

[[hart_posen]] Dartmouth College, Tuck School hart.posen@tuck.dartmouth.edu

Version: 2025-07-23

In evaluating whether a paper makes a theoretical contribution, the core question is not whether the idea is novel or well-executed, but whether it changes how a specific audience thinks. The value of a scholarly contribution lies not in the ideas themselves, but in what they do for readers. This perspective, that writing is about readers, not writers, places the audience at the center of the contribution equation. All three elements of contribution—interestingness, importance, and face validity—are defined relative to a specific scholarly audience.

This framework provides a structured approach to evaluating and developing theoretical contributions, particularly in contexts where the goal is to challenge assumptions and shift understanding within a scholarly audience. It emphasizes the interdependence of surprise, consequence, and plausibility—anchored in the beliefs and expectations of a defined readership.

I think of it as the contribution triangle. The key elements of the triangle are articulated below.

Audience: The Anchor of All Contribution

Before any idea can be said to make a contribution, it must be clear to whom it contributes. A paper must identify its audience and demonstrate familiarity with that audience’s assumptions, expectations, and theoretical conversations. Contribution is inherently relational—it is not a property of the idea alone, but of the idea’s effect on a community of thought. Thus, authors must ask: What does my audience currently believe? And what would it take to change that belief?

Identifying your audience means asking: What community are you speaking to? What do they believe or assume about the phenomenon you’re studying? What literatures do they rely on, and what would surprise them? A contribution is never general; it is always relative to an audience. The same idea may be boring to one audience and paradigm-shifting to another.

The contribution triangle—Interesting, Important, Valid—only takes on meaning once the audience is specified. What surprises one community may be obvious to another. What one field sees as trivial, another may view as a foundational challenge. Audience is not one leg of the triangle—it is the ground the triangle sits on.

1 Interestingness: Breaking a Null Assumption

Surprise is the operational test of interestingness: if the audience is not surprised, the idea is not interesting.

To frame a contribution as interesting, it must be positioned against a null expectation—something your audience assumes to be true unless shown otherwise. However, many papers fail to specify the null clearly, leaving readers unsure of what is being challenged.

A well-formulated null typically takes the form:

“The audience would expect X to be true, but under certain conditions, we show that Y is actually the case.”

To surface and articulate your null, ask:

● What would my target audience assume to be true in this situation?

● What implicit prediction would existing theory make?

● What prior finding or belief am I inviting the reader to question?

Examples:

● Null: “You thought R&D was done to create new knowledge.”

○ Null Breaking: “In fact, R&D is often done to be learn from others’ existing knowledge”

● Null: “You thought that individuals commit suicide for psychological reasons.”

○ Null Breaking: “In fact, there are conditions under which suicide is driven by social (structural) reasons.”

● Null: “You thought the rise in ADHD diagnosis is due to modern childhood issues (video games, too much sugar, too little outdoor play)” ○ Null Breaking: “In fact, the rise in ADHD might be causes by changing regulatory considerations (standardized school testing)”

A clear null not only creates space for surprise, it also anchors the entire theoretical narrative of the paper.

A paper is interesting when it breaks a null assumption held by the audience. As Murray Davis (1971) argued in his essay 'That’s Interesting!', a paper is interesting when it challenges a belief the audience holds to be true, not simply when it is novel. Interestingness arises from surprise, not novelty. An idea is not interesting because it is new; it is interesting because it violates an expectation. This makes interestingness conditional on what the audience already believes—their orthodoxy. An idea with “interesting potential” becomes a contribution only when it is framed against a specific null. The same idea can be framed as interesting or uninteresting depending on how it is positioned.

This distinction between novelty (newness) and surprise (unexpectedness) is crucial. Surprise requires a prior belief; novelty does not. A finding that LLMs behave like humans in a decision task is only interesting if the audience assumed they would not. If they already expect

2 convergence between human and machine behavior, then the result, however novel, is not surprising—and therefore not interesting.

This is why "gap-spotting" is not enough. Simply identifying something that has not yet been studied does not make a paper interesting. There is an infinite number of things that haven’t been done yet. But most of them probably don’t matter. Many papers fail to contribute because they spot a gap in the literature without problematizing existing assumptions. Gap-spotting answers the question “What’s missing?” Problematization asks, “What might be wrong with what we think we know?” A true contribution challenges what the audience believes. not just what they’ve already covered.

Moreover, not all assumptions are created equal. Drawing from Alvesson and Sandberg (2011), who argued that contributions should be generated by problematizing existing assumptions, we can distinguish between different levels of assumptions. These range from in-house assumptions (least impactful) to root metaphors, paradigmatic, ideological, or field-level assumptions (most impactful). Challenging deeper, broader assumptions is harder, but it is also what often leads to high-impact theories. For example, Williamson’s transaction cost economics challenged the field-wide assumption that markets were frictionless, reshaping how scholars understood the boundaries of the firm.

Framing the right null is itself an act of intellectual search. You are navigating a rugged landscape of possibilities, trying to locate a framing that breaks an assumption your audience did not expect to be broken.

Importance: The Consequence of the Surprise

While surprise is what makes a claim interesting, importance is what makes it matter. A paper must show that its surprising insight has meaningful implications—for theory, for practice, or for how we understand the world. An interesting result becomes a contribution only if it allows the audience to explain something they previously could not, predict something in a new way, or reframe a foundational assumption.

Bartunek et al. (2006), in their work on what makes research interesting, emphasize importance as a criterion, though they do not define it precisely. Importance emerges when a finding changes what an audience is able to do or understand. For instance, a new theory that explains why firms invest in R&D despite high spillovers (because R&D helps absorb others’ knowledge) matters because it alters our interpretation of firm behavior—a theoretical consequence with explanatory and predictive weight.

If a paper is surprising but does not appear to change anything of significance, it tends to be dismissed as merely cute. Reviewers and editors may acknowledge that the paper “made them think,” but without a consequential takeaway, the reaction remains intellectual amusement, not scholarly impact. Cute papers are interesting but not important. Important contributions make the audience ask not just “Really?” but “So what?”

Consider one of the examples from earlier:

● Null: “You thought R&D was done to create new knowledge.”

3 ○ ○

Null Breaking: “In fact, R&D is often done to be learn from others’ existing knowledge” Importance: Explains the well-known empirical anomaly that firms do R&D even in industries with very high spillovers (i.e., their knowledge flows rapidly to rivals).

Face Validity: Credibility and Persuasiveness

Even a surprising and important idea fails as a contribution if it lacks face validity. This is not empirical validity per se, but the plausibility of the logic that connects the null-breaking insight to its implications. Face validity is what allows a skeptical but informed reader to suspend disbelief—to say, “I see how that could be true.”

Face validity is not the same as intuition or familiarity. It is the audience's judgment that the logical steps connecting the surprising claim to its implications are reasonable, even if unproven.

Bartunek et al. (2006) acknowledge the importance of validity in theoretical contributions, although they do not fully articulate what constitutes it. In this framework, validity rests on the theoretical logic that connects the surprise to the implication.

For example, in Cohen and Levinthal (1989), the claim that R&D enables firms to absorb external knowledge is not just asserted but justified by a conceptual logic: learning from spillovers is not costless. Some portion of R&D spending can thus be understood as investment in absorptive capacity. This argument persuaded readers not because it was proven empirically, but because it made sense.

Similarly, Daniel Kahneman’s experiments on loss aversion had face validity not because of statistical significance, but because readers could see themselves choosing $50 for sure over $100 with a 50% probability. The effect was psychologically and intuitively plausible.

Consider one of the examples from earlier:

● Null: “You thought R&D was done to create new knowledge.”

○ Null Breaking: “In fact, R&D is often done to be learn from others’ existing knowledge” ○ Importance: Explains the well-known empirical anomaly that firms do R&D even in industries with very high spillovers (i.e., their knowledge flows rapidly to rivals).

○ Validity: As scholars, we are all aware that when we embark on our research journey, the process of writing papers (akin to R&D) significantly enhances our ability to read and learn from others’ papers.

Diagnosing Contribution: Questions to Ask

To surface audience assumptions, ask: What has been taken for granted in recent literature reviews? What would seem implausible or controversial if stated plainly in a seminar? What would contradict a widely taught framework or canonical model?

4 To apply this framework, scholars can use the following diagnostic prompts to test whether their work meets the standard of contribution:

Audience

● Who is the audience for this paper?

● What does this audience currently believe that I’m asking them to reconsider?

Interestingness

● What is the null expectation I am breaking?

● Would my audience actually be surprised by this result or claim?

● How deep or foundational is the assumption I am challenging?

Importance

● What changes if my audience accepts this finding?

● Does it enable a new explanation, prediction, or theoretical shift?

Face Validity

● Is the logic connecting the surprise to its consequence clear and believable?

● Can an informed, skeptical reader accept this claim as plausible, even if not proven?

Putting It All Together: A Composite Function

While this framework primarily focuses on theoretical contributions, it can also be applied to methodological or empirical work that challenges assumptions about how we generate knowledge, measure constructs, or interpret evidence.

These three elements—interesting, important, and valid—form the vertices of a triangle, anchored in the audience. A strong contribution sits inside that triangle, balancing all three in relation to what the audience believes and values.

We can think of contribution as a function:

Contribution = f(audience | interesting + important + valid + interactions)

Each element is necessary but not always sufficient on its own. There can be compensating variation: a highly surprising and persuasive idea may compensate for modest importance; a profoundly important claim may justify more interpretive flexibility in the logic. These attributes also interact: surprise heightens importance; importance raises the stakes of face validity; face validity enables the uptake of surprise.

For example, a paper that offers a novel insight into why a widely accepted theory fails under certain conditions may seem unimportant at first. But if the logic is airtight and the claim is surprising enough, it may prompt readers to rethink how robust that theory really is. Conversely,

5 a less surprising insight may still matter deeply if it resolves a critical theoretical contradiction or opens a new domain of application.

It is critical to recognize that, as the author, you must choose what content belongs in each vertex of the contribution triangle: interesting, important, and valid. This is not a mechanical task. It is a creative challenge that involves search, evaluation, and judgment. Authors often become prematurely locked into a specific formulation for each corner, limiting the potential impact of the paper. Be deliberate in exploring multiple ways to frame your contribution before settling on a final configuration. Equally important, each vertex must contain distinct content. You cannot assign the same substantive idea to more than one corner. A contribution is only compelling when it offers a unique surprise, a separate reason it matters, and a credible justification for believing it.

This framework also implies that non-contributions can be diagnosed along a spectrum. While offered partly tongue-in-cheek, the following typology encourages scholars to reflect on which dimension their work might be lacking:

● Interesting + Important + Valid = Contribution

● Not Interesting + Important + Valid = Boring (technically sound but fails to capture attention)

● Interesting + Not Important + Valid = Cute (provocative but inconsequential)

● Interesting + Important + Not Valid = Unsubstantiated Insight (compelling idea without credible support)

● Interesting + Not Important + Not Valid = Speculative Fluff (clever but implausible and trivial)

● Not Interesting + Important + Not Valid = Consulting (practically motivated but theoretically weak)

● Not Interesting + Not Important + Valid = Self-Referential Formalism (formally tight but purposeless)

● Not Interesting + Not Important + Not Valid = No Clear Contribution (no meaningful surprise, implication, or persuasive logic)

Ultimately, to make a contribution is to change what an audience believes, and to do so by telling them something they didn’t expect, showing why it matters, and persuading them that it could plausibly be true.

## Strengthening a Contribution

To improve a paper’s contribution:

● Reframe your core claim relative to a clear null. Ask: What does your audience believe that you are challenging?

● Make the implications of the surprise explicit. What changes if your result is true?

● Tighten the theoretical logic linking your surprise to its consequence to increase face validity. Crafting a contribution is both an intellectual and rhetorical task: it requires

6 knowing what to challenge, why it matters, and how to convince others that your insight deserves their belief.

# What Is Interesting in Operations Management?
Gérard P. Cachon

This essay discusses my view of the essential characteristics of interesting research in general and in operations management in particular. It is based on my Manufacturing and Service Operations Management Distinguished Fellows presentation given at the University of Michigan, June 27, 2011.

Key words: operations management; interesting; impact History: Received: September 5, 2011; accepted December 8, 2011. Published online in Articles in Advance February 28, 2012.

What is interesting research? Is it merely in the eye of the beholder? Or is there something more systematic about what is interesting? We all want to write, and editors should only want to publish, interesting research. And we all (hopefully) think our own research is interesting. But what will others ﬁnd interesting? I claim that there is one key ingredient to “interesting.” Interesting means unexpectedinteresting research piques your curiosity, it induces a pause for contemplation, and most importantly, it contradicts how you think about the world. In this essay, I develop this idea further and apply it to the literature in operations management. For other work that identiﬁes “unexpected” as a critical component of “interesting,” applied in the domain of sociology, see Davis (1971).

## Unexpected and Expected

Interesting research reveals a new perspective on the familiar. It poses a question that has not been asked before, or it follows an accepted question with a new answer, an answer that is orthogonal to those that preceded it. For example, to the question “What is the shape of the Earth?” Columbus countered with “round,” a sharp departure from the common wisdom of the time.

Conﬁrming what is expected to be true is simply not interesting. For example, do we care or should we care if a paper reports, even if through sophisticated means, that callers will balk more frequently if they have to wait longer? Or that employees work faster if they are paid more? How could it be any other way? One might retort with “But isn’t it important to show that people actually put in more effort when they are

paid more?” I do not think so. It would indeed be interesting to show the opposite because that would violate our presumptions about worker motivation. But demonstrating or proving the expected, no matter how hard it is to do, remains bland, pedestrian, and just ﬂat out uninteresting.

Given that interesting equals unexpected, it is important to be clear about what is meant by “expected.” Expected is what is assumed to be “true,” “known” or “given” at the time the research is developed, or what could be known with minimal effort. For example, if the expectation is that a linear program is hard to solve, then the invention of a method that reduces the necessary search space and is guaranteed to ﬁnd the optimal solution (e.g., George B. Dantzig’s simplex method) is unexpected. After that, if the expectation is that the solution method must traverse the boundaries of the feasible region (as in the simplex method), then a method that approaches the optimal solution from the interior would be unexpected (as in interior point methods; e.g., Karmarkar 1984).

## A Formula for Interesting

How does one create an unexpected, and therefore interesting, result? It has been my experience that the main idea of an interesting paper can be described with the following template:

“What was thought to be X is really Y 0′′ 

Uninteresting papers are unable to offer a short, simple, and precise version of the above template. The “X” and “Y ” are apparent in an interesting paper (and they are apparent due to the careful writing of the authors—interesting papers do not just happen, they are crafted so that it is clear to the reader why they are interesting). Fortunately, as the following examples demonstrate, there are many to choose from. The list below is by no means exhaustive (nor meant to be).

What Was Thought to Be Exogenous Is Really Endogenous This is a powerful and often-used approach to developing interesting research. To illustrate, consider demand uncertainty at each level of a supply chain (e.g., manufacturers, wholesalers, and retailers). Where does that uncertainty come from? It generally was assumed to be exogenous, but Lee et al. (1997) provide four reasons why it can be endogenous. Not only endogenous, it ampliﬁes as one moves up the supply chain, a phenomenon labeled the “bullwhip effect.” (What was thought to not exhibit a pattern does have a pattern, i.e., variances increase in a particular way.) Hence, much of the demand uncertainty within a supply chain is actually self-inﬂicted, caused by the actions of the ﬁrms themselves, and therefore can be reduced by prudent management.

Porteus (1985) provides an early example of this approach with the economic lot-sizing problemchoose a set of production quantities to minimize holding and setup costs, where a ﬁxed setup cost is incurred whenever a production lot is started. Rather than assume the setup cost is given (i.e., exogenous), Porteus (1985), motivated by Japanese manufacturing practices, argues that a ﬁrm can exert effort to reduce its setup costs (i.e., the setup cost can be endogenous).

Recent examples of converting “exogenous” into “endogenous” include the highly inﬂuential work on strategic consumers: although it was assumed that operating decisions had no direct impact on demand, in fact, consumer demand responds directly to the operational choices the ﬁrm makes. With this new perspective we learn that ignoring strategic behavior can lead a ﬁrm to carry too little inventory (Dana and Petruzzi 2001), or that optimal prices may rise over a selling season (Su 2007), or that committing to never mark down prices may help a ﬁrm (Su and Zhang 2008).

What Was Thought to Be Complex Is Really Simple Return to the lot-sizing problem but consider a more complex version, the multiechelon version with one warehouse and N retailers. This is a very difﬁcult problem, and the optimal policy is unknown. However, Roundy (1985) provides a different view on the problem—instead of trying to identify the optimal policy, identify a simple policy that has provably good performance. In other words, what was thought to be complex (a problem with an unknown optimal policy) is really simple (a simple policy works quite well).

Although the work of Roundy (1985) exempliﬁes this approach, it is by no means the only example.

In fact, many papers in operations management follow this path. For instance, Lariviere and Porteus (2001) take a problem (selling to a newsvendor) that is ill behaved in general (i.e., complex) and show that it is well behaved (i.e., simple) for a large class of distribution functions. Gallego and van Ryzin (1994) show that a complex stochastic dynamic problem (what sequence of prices should be chosen to maximize revenue from a ﬁnite set of inventory sold over a ﬁxed period of time) can be well approximated by a deterministic counterpart. Their insight led to the unexpected conclusion that simple, ﬁxed price polices may perform nearly as well as a fully dynamic pricing policy.

What Was Thought to Be Simple Is Really Complex 
In the newsvendor problem a decision maker chooses a quantity of inventory to order before facing stochastic demand—order too little, and there will be lost sales, but order too much, and there is costly leftover inventory. The optimal policy is known, but how do people actually make newsvendor-type choices? This is a choice task with random outcomes, and there are a number of established theories for how people make choices under uncertainty. For example, they could be risk averse or risk seeking. Unfortunately, these theories do not describe actual decisions in this setting very well (see Schweitzer and Cachon 2000). (I fully recognize that it is presumptuous to assume that my papers are interesting, so I only mention a few.) Another theory is needed. What was thought to be simple (an established theory can provide the answer) is really complex (a new theory is needed).

What Was Expected to Be a Small Effect Is Really a Large Effect Say a paper demonstrates that what you would think would be a small effect is actually a large effect. That’s interesting. And there are number of good examples of this approach.

What is the impact of trafﬁc congestion on public health? To answer this question, Currie and Walker (2011) exploit the adoption of the E-ZPass toll system on highways in Pennsylvania and New Jersey. E-ZPass is a radio-frequency identiﬁcation tag that reduces toll plaza congestion by allowing vehicles to pay their toll without stopping. Less congestion leads to less pollution, which can lead to better health. Speciﬁcally, does the reduction in congestion in a toll plaza have a measurable impact on pregnancy outcomes of people who live within two kilometers of the toll plaza? One could easily assume (as I would) that this effect, if it exists, would be too small to measure, but they ﬁnd a substantial effect: premature births decreased by 10.8% and low-weight births decreased by 11.8%.

As a Ph.D. student, my ﬁrst mentor, Colin Camerer, told me that there are three kinds of papers in this world. The ﬁrst, and maybe the majority, are the ones that should never have been written—the question is not interesting, and the answer is not compelling. The second are the papers that you are glad somebody else wrote. You wanted to know the answer to the question, but you are content that somebody else was willing to do all of the work to provide it. These can be interesting, just not worth the personal effort to achieve the answer. And the third are the papers that you wish you wrote. There are not many of those. Currie and Walker (2011) is deﬁnitely in my “I wish I wrote it” category. The next one is as well.

Jordan and Graves (1995) study the question of production ﬂexibility. Say you have a set of production facilities and a set of products to produce. Demand is uncertain, and each facility has a ﬁxed (and limited) capacity. You can invest to give a plant the ability to make more than one product. But this ﬂexibility is expensive, so how much do you need? An intuitive answer is that a lot of ﬂexibility is worth much more than a little bit. It turns out, we now know, thanks to Jordan and Graves (1995), that what was assumed to be a small effect (a little bit of ﬂexibility provides only a little beneﬁt) is actually a very large effect (a little bit of ﬂexibility, done right, provides essentially the same beneﬁt as complete ﬂexibility).

There are other examples of this approach. Fisher and Raman (1996) show that smartly exploiting a little bit of early season sales can generate a substantial proﬁt increase (whereas it was assumed that a small sample of data would have little value, it can actually have a large value). Next, although it was assumed that there would be no value to operational ﬂexibility when demands across products are perfectly correlated, Van Mieghem (1998) shows that the value in this case can be positive (when the products have different margins).

What Was Thought to Be a Large Effect Is Really a Small Effect In the early 1990s there was considerable interest in the use of information technology to improve supply chain performance. The key technologies of the time were bar coding and electronic data interchange. (The internet was only starting to gain momentum.) Everyone assumed (including myself), that exploiting the data these systems created and shared across the supply chain would lead to tremendous improvements in inventory performance. But Cachon and Fisher (2000) show that what was expected to be a large effect (the reduction in inventory through the use of shared information across the supply chain) was actually a small effect (it is much more effective to move goods faster than it is to be smart with how you move them).

A corollary could be “What was thought to be common is not so common.” Cachon et al. (2007) take this approach with the bullwhip effect.

What Was Thought to Be a Large Effect Is Really Much Larger Quick response has been shown to provide substantial value to ﬁrms (Iyer and Bergen 1997) because of its ability to better match supply with demand. However, Cachon and Swinney (2009) show that ignoring strategic consumer behavior underestimates the value of quick response (what was thought to already be a substantial amount) by as much as 500%.

What Was Thought to Be Easy Is Really Hard Ask students in teams of four to make inventory decisions in a serial supply chain. Demand is constant, and lead times are constant. This should not be terribly hard, yet, as ﬁrst observed by Sterman (1989) and has been demonstrated countless of times, this is a very difﬁcult task—subjects make a number of errors that lead to large mistakes and terrible operating performance.

What Was Assumed to Not Be a Problem Is Really a Problem Nearly the entire literature on inventory management assumes the decision maker knows how much inventory they have. But is that true? In fact, DeHoratius and Raman (2008) show that what was assumed to not be a problem (inventory records are accurate) is really a problem (in fact, errors are common enough to lead to poor decisions).

What Should Improve Performance Really Harms Performance Consider a large number of commuters who want to drive to work and have several possible routes to choose from in a network of roads. The congestion on a particular route depends on how many drivers choose the route, so one would think that things can only get better when you add capacity to the network (another route) while holding the number of commuters constant. But Braess (1968) shows that this might not be the case—due to self interested behavior, adding capacity may actually increase commuting times for all.

## Is “Interesting” Everything?

Are we done? Is “interesting” sufﬁcient, the exclusive goal, everything that we want? Although interesting is desirable, and surely necessary, I contend that it is not sufﬁcient. We want research to also be “important”—important research is useful, either for the creation of more research or, better still, for the utility of society. To be useful, research generally must be broadly applicable, or at least applicable in a domain of signiﬁcance (e.g., healthcare rather than just dog grooming). A result is not important if it applies only in a special case or for unlikely or unreasonable parameters. When a result is useful and broadly applicable, it is likely to change behavior, the behavior of other scholars (e.g., they build upon the research or they change the questions they work on) or, ideally, the behavior of those outside academia (managers, policy makers, etc.).

Is that it? We only want interesting and important research? What about being correct? Shouldn’t a result also be “true”? Strictly speaking, interesting does not need to be true. Take the famous Hawthorne effect. In the 1920s a series of experiments were conducted at Western Electric’s Hawthorne plant to determine whether better lighting improves worker productivity. The surprising ﬁnding was that productivity improved whenever there was a change in lighting—lighting did not matter per se, but rather that workers knew that management was paying attention to them. This is indeed an interesting result, maybe even a fascinating result, but it may not be correct (see Levitt and List 2011 for details).

But if a result need not be true, doesn’t that open up the possibility that every cockamamie idea can be labeled “interesting”? No. Although an interesting idea does not need to be true, it does need to be plausible. The Hawthorne effect gained notoriety because it was both unexpected and plausible—it had some prima facie validity. Most importantly, it had data (albeit, poorly collected and analyzed data). So, by any yardstick, the Hawthorne effect was interesting and important—it surely changed behavior of scholars and practitioners.

## Conclusion

Interesting research raises more questions than it answers. It is controversial. It invokes responses like “that can’t be true” or “this is obviously incomplete.” Interesting research should initially leave the reader a little discontent, unnerved, or motivated to prove it wrong or at least incomplete. This is why it can be hard to publish interesting research, and really interesting research is rarely accepted immediately.

Interesting research that is also “important” changes behavior, i.e., it yields the ever so desirable “impact”:

Interesting × Important = Impac