This is a special chapter discussing the Curry-Howard Correspondence. It's currently work-in-progress, due to the vast amount of background and related knowledge to properly understand the correspondence.

This page includes content from chapter 10, 15 of Reynolds' "Theories of Programming Languages" book, [notes on CHC](http://disi.unitn.it/~bernardi/RSISE11/Papers/curry-howard.pdf), and various other sources.

#operator #operand #ProgrammingLanguages 

## Preliminaries
We take a look at some preliminary topics required to understand what the correspondence means.

### Untyped(vanilla) $\lambda$-calculus

In simple terms, $\lambda$-calculus is a way to specify *function applications* without actually defining their names. Normally in programming, we can define a function like so:
```
let f(x) = x * x + 1 in B
```

where B is a command. This defines what `f` does, given some variable `x` that's bound to the function body, `x * x + 1`. Equivalently, we can denote this binding of `x`:

$$
\mathbf{let} \ f = \lambda x. x \times x + 1 \ \mathbf{in} \ B
$$

The "function declaration" in the above statement, $\lambda x. x * x + 1$ is called a *lambda expression* or an *abstraction*, which is saying "when this is applied to some $x$, it returns $x \times x + 1$". Note that a lambda expression/abstraction doesn't give notions on what $x$ is. The only thing it implies is the relationship between some operand $x$ and operator $x \times x + 1$.

**Note that abstractions are not functions!!** Abstractions can be used to denote a function application, but doesn't always define a function by itself.

This is why unnamed functions in typical programming languages are called *lambda functions*, because a lambda function yields some value defined by relationships between its arguments and the function body. Hence it's an "unnamed function".

Suppose lambda expressions didn't exist. All function calls must be declared with let terms. For example, say we wanted to calculate $\int_0^1 x^2 + 1$ with a defined integration function `integrate(lb, ub, f)`. This can be written as:
$$$
\mathbf{let} \ f(x) = x^2 + x \ \mathbf{in} \ \text{integrate}(0, 1, f)
$$$

But with lambda expressions, this gets simplified into:
$$
\text{integrate}(0, 1, \lambda x. x^2 + 1)
$$

the $\lambda$-calculus we will be looking at in this chapter only works with *untyped* $\lambda$-calculus. This means we don't consider the types of variables and expressions. Untyped $\lambda$-calculus is simpler to describe, but isn't sufficient in describing mathematical logic. We need *typed* $\lambda$-calculus as a stronger tool.

#### Syntax and Basic Rules
$\lambda$-calculus is composed of expressions called *$\lambda$-terms*. We will define the set of all lambda terms $E$, inductively, like from [[2. Formal Logic#]]:
$$
\begin{aligned}
E &\rightarrow x, \ x \in V \\
& \ \ | \lambda x. E, \ x \in V \\
& \ \ | E \ E
\end{aligned}
$$

1. A term of form $x$, where $x$ is in some set of variables $V$, denotes a *variable*.
2. A term of form $\lambda x. E$, is called a *lambda expression* or to reduce confusion, also called an *abstraction*.
3. A term of form $E_0 \ E_1$ is called *application* of $E_0$ to $E_1$. Equivalently, we see $E_0$ as the *operator* and $E_1$ as *operand*.

Application is left associative, meaning
$$
E_0 \ E_1 \ E_2 = ((E_0 \ E_1) \ E_2)
$$
and in abstractions bounded variables are bound to the closest binder, meaning:
$$
\lambda x. (\lambda y. x \ y \ x) \ x = \lambda x. ((\lambda y. ((x \ y) \ x)) \ x)
$$

----

##### $\alpha$-Conversion

Since every abstraction $\lambda x. E$ binds some variable $x$ in scope $E$. We need to identify the set of *free variables* that's allowed in an expression. The set $\text{FV}(E)$ of free variables in expression E is defined as:
$$
\begin{aligned}
\text{FV}(x) &= {x}, \ x \in V \\
\text{FV}(E_0 \ E_1) &= \text{FV}(E_0) \cup \text{FV}(E_1) \\ 
\text{FV}(\lambda x. E) &= \text{FV}(E) - \{x\}, \ x \in V \\
\end{aligned}
$$

Once we identifed the set of free variables in an expression $E$, we can try and *substitute* all free variables in $E$ with another expression. Let $E/\delta$ denote substituting occurences of variable $x$ in $E$, with $\delta \ x$. $E/\delta$ is defined as:
$$
\begin{aligned}
x/\delta &= \delta \ x \\
(E_0 \ E_1)/ \delta &= (E_0 / \delta) \ (E_1 / \delta) \\
(\lambda x. E)/ \delta &= \lambda x_{new}. (E / [\delta \ | \ x:x_{new}])
\end{aligned}
$$
where
$$
x_{new} \notin \bigcup_{\omega \in \text{FV}(E) - \{x\}} \text{FV}(\delta \ \omega)
$$

Let's go through this step by step:

1. If the expression $E$ is just a variable($x$) we can just replace $x$ with $\delta$ applied to $x$, $\delta \ x$
2. If $E$ is an application, we first perform substitution on both operator and operands: $(E_0 / \delta)$ and $(E_1 / \delta)$. And finally we just apply the substituted operator to the substituted operand.
3. If $E$ is an abstraction , $x$ is not a free variable. So we cannot just substitute $\delta \ x$. Instead, we can *rename* $x$ into some other variable $x_{new}$, and then perform substitution on $x_{new}$. However, if we were to change $x$ into some variable that's being used inside E, this can change the meaning of the abstraction. For example, suppose we have $\lambda x. y \ x$. It will always return some variable $y$, regardless of x. But if you changed the bounded variable $x$ to $y$: $\lambda y. y \ x$, it now becomes the identity function. When we are substituting variables in an abstraction, we are free to change the bound variable $x$ to $x_{new}$, *given that the new variable $x_{new}$ isn't being used in the abstraction.* This is equivalent to $x_{new} \notin \bigcup_{\omega \in \text{FV}(E) - \{x\}} \text{FV}(\delta \ \omega)$, since free variables are variables being used inside the terms.

Applying the substitution rule to $\lambda x. E$, resulting in $\lambda x_{new}. (E/x \rightarrow x_{new})$ is called *renaming*. And if some expression $E'$ is obtained after applying multiple renamings to $E$, we say that $E$ and $E'$ are *$\alpha$-equivalent*. Then, it can be shown that:
$$
E \equiv E'
$$

This is simply because all we have done were renaming bound variables to some other variable, that's not being used with the expression. In code form, we can see that
```
def my_function(a, b):
	return a + b + c  # c is some other variable
```
and
```
def my_function(renamed_a, renamed_b):
	return renamed_a + renamed_b + c  # c is some other variable
```
are functionally equivalent.

----

##### Reduction
Recall that $\lambda$-calculus is just a way to represent repeated function applications. This leads us to thinking, if we compose applications, couldn't we *reduce* them so that we can sequentially apply the function? The answer to this is yes. Consider the following lambda term:
$$
(\lambda x. E) E'
$$
A term of this form is called a *redex*, which represents applying the function $(\lambda x. E)$ to the argument $E'$. This means $(\lambda x. E)$ should yield the value of $E$ when $x$ in $E$ denotes the value of $E'$. In other words, we *substitute* $E'$ for $x$ in $E$. 

Suppose we have an expression $E_0$ that contains one or more occurences of $(\lambda x. E) E'$. Let $E_1$ be obtained from $E_0$ by replacing $(\lambda x. E) E'$ with $E/x \rightarrow E'$. Then we write $E_0 \rightarrow E_1$, and say that $E_0$ *contracts* to $E_1$. Using this notation, we can write the $\beta$-reduction rule:
$$
\frac{}{(\lambda x. E) E' \rightarrow (E/x \rightarrow E')}
$$

This is a very intuitive rule - we can convert function applications by just *substituting function calls with its function body applied to the arguments*.

The following rules also hold:
Renaming:
$$
\frac{E_0 \rightarrow E_1 \qquad E_1 \equiv E_1'}{E_0 \rightarrow E_1'}
$$

Contextual Closure
$$
\frac{E_0 \rightarrow E_1}{E_0' \rightarrow E_1'}
$$
where $E_1'$ is obtained from $E_0'$ by replacing one occurence of $E_0$ in $E_0'$ in $E_1$

If $E_1'$ is obtained from $E$ by zero or more contractions, then we say *$E$ reduces to $E'$* and write that as $E \rightarrow^* E'$. From this we can derive more rules:

Transitive and Reflexive Closure
$$
\frac{E_0 \rightarrow E_1}{E_0' \rightarrow^* E_1'}
$$

$$
\frac{E_0 \rightarrow^* E_1 \qquad E_1 \rightarrow^* E_2}{E_0 \rightarrow^* E_2}
$$

$$
\frac{E_0 \equiv E_1}{E_0 \rightarrow^* E_1}
$$

----

For an expression, we can continuously apply $\beta$-reduction (and $\alpha$-conversion if needed) until no redexes exist in the expression. Thus at a certain point we can no longer apply $\beta$-reduction, reaching a terminal state. This type of terminal expression is called a *normal form*, and say that "$E'$ is a normal form of $E$", if $E$ reduces to normal form $E'$.

As an example, let's try reducing $(\lambda x. (\lambda y. y \ x)z)(z \ w)$:
$$
(\lambda x. (\lambda y. y \ x)z)(z \ w) \rightarrow
(\lambda x. x \ z)(z \ w) \rightarrow z \ (z \ w)
$$
However, if we perform $(\lambda y. y \ x)z/x \rightarrow (z \ w)$ first, we get a different order of reduction:
$$
(\lambda x. (\lambda y. y \ x)z)(z \ w) \rightarrow
(\lambda y. y \ (z \ w))z \rightarrow z \ (z \ w)
$$
but end up with the same normal form, $z \ (z \ w)$.

This is the *Church-Rosser Theorem*, which states that the order of reduction does not matter. More formally, if $a \rightarrow^* b$ and $a \rightarrow^* c$, then there exists some $d$ such that $b \rightarrow^* d$ and $c \rightarrow^* d$.
<span class='centerImg'>![[church_rosser.png]]</span>

#### Church-Turing Thesis and the Church Numerals
From wikipedia:
> In computability theory, the Church–Turing thesis (also known as computability thesis, the Turing–Church thesis, the Church–Turing conjecture, Church's thesis, Church's conjecture, and Turing's thesis) is a thesis about the nature of computable functions. It states that a function on the natural numbers can be calculated by an effective method if and only if it is computable by a Turing machine.

Church shows that indeed natural numbers and arithmetic are representable solely with $\lambda$-calculus, proving that $\lambda$-calculus is turing-complete. The representation of the natural numbers with $\lambda$-terms, is called the *Church Numerals*. In fact, you can represent any arbitrary data type and calculations involving them in $\lambda$-calculus, including,  but not limited to integers, finite-precision real numbers, booleans, arrays, and so on. This is the Church-Turing thesis.

Note: this section is for those who have keen curiosity as to what $\lambda$-calculus can and cannot represent. You may skip this section if you don't want to read it.

----

The key idea is to represent natural numbers, and their succesive numbers as a self-composite(recursive) function that "does something":

<span class='centerImg'>![[church_numerals_composition.svg]]</span>

For any $n \in \mathbb{N}$, define $f^n(x)$ as the following:
$$
\begin{aligned}
f^0(x) &= x \\
f^1(x) &= f(x) \\
f^2(x) &= f(f(x)) \\
\vdots \\
f^n(x) &= f(f^{n - 1}(x))
\end{aligned}
$$
Then, for any $n \in \mathbb{N}$, the $n$th Church Numeral $c_n$ is the $\lambda$-term
$$
\begin{aligned}
c_0 &= \lambda f.\lambda x. \ x \\
c_1 &= \lambda f.\lambda x. (f \ x) \\
c_2 &= \lambda f.\lambda x. (f \ (f \ x)) \\
\vdots \\
c_n &= \lambda f.\lambda x. f^n(x)
\end{aligned}
$$

You might look at this and ask, "so where is the number $n$?". This may sound confusing, but the Church Numerals does not mean to return an output number $n$. Instead, it holds the meaning of applying some function $f$, to some argument $x$, $n$ times. Thus *this function form itself is the Church Numeral, not its result*. It implys, "do something $n$ amount of times".

----

### Intuitionistic Proposition Logic

#### Introduction

Logic, the study of what is "right", is what I consider one of the greatest discoveries of science. The formal treatment of logic enabled amazing developments related to mathematics, computational science, and many more fields. 

I'm sure you heard of the "chicken or egg" paradox. What comes first, the chicken or the egg? In the same metaphorical context, what comes first, a program or a computer? It's well known that mathematicians were developing actual programs before the invention of the first computer and hence computers were invented as a tool to realize programs.

In traditional, classic logic we reasoned about whether a given proposition is true or false. The basis of classic logic lies in the *Law of the Excluded Middle*, which states that $p \vee \neg p$ must be always true. In other words, a proposition's truth value must be either true or false, independent of any reasoning. It turns out that it's quite limiting in what it's possible to express. For example, take this famous problem:

> There are two irrational numbers $x$ and $y$, such that $x^y$ is rational.

The well-known proof to this is setting $x = \sqrt{2}^{\sqrt{2}}, y = \sqrt{2}$ when $\sqrt{2}^{\sqrt{2}}$ is irrational. Except we do not know if $\sqrt{2}^{\sqrt{2}}$ is rational. This is an example of a *non-constructive* proof, meaning the proof does not specify in which way a mathematical object satisying the problem's statement. This is very much similar to the inductive definition of sets from [[1. Inductive Definition#Set-generating function]] which only shows *what* elements are in a set, requiring set-generating function(s) that explain *how*  to generate a set.

Intuitionistic(constructive) logic attempts to find *constructive* proofs. This is the first point where you may get a very faint grasp of what the Curry-Howard Correspondence says: A constructive proof is an algorithm that defines how a certain type of mathematical object is constructed. A program is a set of instructions specifying how a computer should carry out a specific task, an *algorithm*. Do you get the feeling that they are very closely related, or perhaps even the same?

#### Intuitive Semantics

In intuitionistic logic, a proposition is only considered true if a *construction*(proof) exists. The Brouwer, Heyting and Kolmogorov interpretation(BHK-interpretation) shows how classic logical connectives are viewed in intuitionistic logic:

- A construction of $\phi_1 \wedge \phi_2$ consists of a construction of $\phi_1$ and a construction of $\phi_2$.
- A construction of $\phi_1 \vee \phi_2$ consists of a number $i \in \{1, 2\}$ and a construction of $\phi_i$. I.E. we must have the construction of either $\phi_1$ or $\phi_2$.
- A construction of $\phi_1 \rightarrow \phi_2$ is a method(function) transforming every construction of $\phi_1$ into a construction of $\phi_2$.
- There is no possible construction of $\bot$ where $\bot$ denotes falsity.
- A construction of $\neg \phi$ is a method(function) that transforms every construction of $\phi$ into $\bot$. Hence $\neg \phi \equiv \phi \rightarrow \bot$

#### Natural Deduction

The way in which we write intuitionstic propositions are the same as classical logic. Let $PV$ denote an infinite set of propositional variables. Then the set of intuitionistic formulas, $\Phi$ is defined as:
$$
	\Phi ::= \bot \ |  \ PV \ | \ (\Phi \rightarrow \Phi) \ | \ (\phi_1 \wedge \phi_2) \ | \ (\phi_1 \vee \phi_2)
$$
Remember that $\wedge, \vee, \rightarrow$ are called *connectives*.

We now explain some notation on how proofs are written in natural deduction. Don't worry, they are very simple things written in a cryptic way for rigorousness:

1. A *context* is a finite subset of $\Phi$	. We use $\Gamma, \Delta$, etc. to range over contexts.
2. The relation $\Gamma \vdash \phi$ means that "from $\Gamma$ we can give a proof of $\eta$.
3. $\Gamma, \Delta$ means $\Gamma \cup \Delta$. This can be extended to single propositions like so: $\Gamma, \phi \equiv \Gamma \cup \{ \phi \}$

We can use the same format of inference rules, such that as from [[2. Formal Logic#Inference Rules]], to derive a proof system:

Axiom for intuitionistic logic propositional calculus:

$$
\Gamma, \phi \vdash \phi
$$

An *introduction rule*, $(\bullet \mathbf{I})$, shows how to derive the connective $\bullet$:

$$
\frac{\Gamma \vdash \phi \quad \Gamma \vdash \psi}{\Gamma \vdash \phi \wedge \psi} (\wedge \mathbf{I})
$$

$$
\frac{\Gamma \vdash \phi}{\Gamma \vdash \phi \vee \psi}\ , \ \frac{\Gamma \vdash \psi}{\Gamma \vdash \phi \vee \psi} (\vee \mathbf{I})
$$

$$
\frac{\Gamma, \phi \vdash \psi}{\Gamma \vdash \phi \rightarrow \psi} \ (\rightarrow \mathbf{I})
$$

Intuitionistic *elimination rules*, $(\bullet \mathbf{E})$ show how the connective $\bullet$ can  be used to derive other formulas.
$$
\frac{\Gamma \vdash \phi \wedge \psi}{\Gamma \vdash \phi}, \quad \frac{\Gamma \vdash \phi \wedge \psi}{\Gamma \vdash \psi} (\wedge \mathbf{E})
$$

$$
\frac{\Gamma, \phi \vdash \rho \quad \Gamma, \psi \vdash \rho \quad \Gamma \vdash \phi \vee \psi}{\Gamma \vdash \rho} (\vee \mathbf{E})
$$

$$
\frac{\Gamma \vdash \phi \rightarrow \psi \quad \Gamma \vdash \phi}{\Gamma \vdash \psi} (\rightarrow \mathbf{E})
$$

$$
\frac{\Gamma \vdash \bot}{\Gamma \vdash \phi} (\bot \mathbf{E})
$$

#### Related links
- https://math.stackexchange.com/a/2697035

### Simply Typed $\lambda$-calculus

## The Curry-Howard Correspondence