## 1. scaling behavior of intelligence vs machine learning 

### rational ai principle
üìêshallow to deep: structured ignorance priors enable robust inference by starting deliberately broad and refining incrementally as evidence accumulates. this approach maintains flexibility against unexpected cases while leveraging structure when appropriate, yielding systems that scale better than those with premature specificity.

### applying principle 1

| Module | Topic                                                | Rational AI Principle | How                                                                                                                        | Contrast                                                                                                                             | Example                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| ------ | ---------------------------------------------------- | --------------------- | -------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1      | Scaling behavior of intelligence vs machine learning | üìêShallow to Deep     | Define broad model space with structured ignorance priors, allowing models to evolve incrementally as evidence accumulates | Traditional ML fixes model architecture and optimizes parameters; shallow-to-deep allows structure itself to adapt based on evidence | **Time-series forecasting with a dynamic DSL**. We specify a generative model (e.g. linear trend, seasonal/periodic components, change-point jumps) with latent hyperparameters for amplitudes, frequencies, etc. The system samples _which_ model components to include, plus their parameters, instead of committing to a single fixed architecture. Then, using particle-based sampling (e.g. SMC), it _automatically_ refines and prunes these structural choices as new observations arrive. In **airline-traffic demo**, this approach successfully adapts to the sudden COVID drop, while a transformer or LSTM‚Äîpre-trained on stationary patterns‚Äîfails to track the abrupt change. The code can be implemented in Gen/GenJAX by writing a short ‚Äúkernel DSL‚Äù for time-series structure, assigning a broad prior over kernels, and applying sequential Monte Carlo so that the model _grows in complexity only if/when the data demand it_. |

üìêshallow to deep: structured ignorance priors enable robust inference by starting deliberately broad and refining incrementally as evidence accumulates. this approach maintains flexibility against unexpected cases while leveraging structure when appropriate, yielding systems that scale better than those with premature specificity.

- üß±extending #224  
- üó£Ô∏è[vikash seminar.txt](https://github.com/user-attachments/files/18817493/vikash.seminar.txt)

### more on üìêshallow to deep
- shallow as using "broad ignorance priors" that deliberately avoid making strong assumptions, allowing systems to be more robust. For example, in 3D perception, rather than using detailed CAD models (deep), they use simple voxelized approximations (shallow). The key insight is that "if you put in rationally uncertain knowledge, then you will scale better than if you didn't." The system progressively moves from shallow to deep understanding by maintaining flexibility to accommodate exceptions (like floating objects) while leveraging structure when it exists (like object contact).
- infer the structure of the world (rather than quantified uncertainty of known structure in science)
- "broad structure then  - get a sense of what the uncertainty is in the model structure"
- " it's perceptual failure and value function estimation. mathematically, what's going on is there's a gap between what the optimizer is able to achieve and the actual minimax solution to the game. In that gap, that's where the camouflage lives. it's important at the starting point to recognize, if we're interested in scaling intelligent systems or understanding natural intelligence, we need something that scales very differently in terms of data efficiency, compute efficiency and robustness from machine learning, spinning today's machine learning. So in the seminar, one of the approaches we'll be exploring in more detail is really just to bring back the idea that there's a world and we have to make decisions about it. these themes of perception and reasoning, which were central to multiple decades of AI research."

- "someone was asking, How broad is the prior? Here we're just showing that the prior is quite broad. I'll point out a interesting project direction for people who have kind of a theoretical that is that the prior is still paying as the trees get larger. So there's some very fundamental questions about, what would it mean to have like a really ignorant prior with no biases over this type of model class, like you know, if we only truncated at three level trees, then we could have a uniform prior over all 8000 structures. That's not what this prior does, though. recursion kind of favors shorter programs within that class. That's very technical issue that we'll see if, I think the third lecture actually on fundamental, really interested in very general phenomena of learning from sparse data or rationality how is it that we can find robust explanations? And also, when does that capacity fail like when we get sort of seduced by a conspiracy theory of there's a lot here in this, " (üôã‚Äç‚ôÄÔ∏è why does recursion favors shorter program? might this be relevant with occum's razor? - which i've never associated with recursion) (üôã‚Äç‚ôÄÔ∏è what does vikash mean by conspiracy theory?)


applying  üìêshallow to deep principle to entrepreneurship: "At the heart of Vikash Mansinghka's seminars lies the novel concept of "structured ignorance" in probabilistic inference - a methodical approach to embracing and encoding uncertainty in decision-making models. Rather than making narrow, overconfident assumptions, this approach advocates for beginning with broad "ignorance priors" that acknowledge the vast range of possibilities in an open-world system. This foundational principle suggests that by explicitly structuring what we don't know, we can create more robust and adaptable models that better reflect reality. The document introduces a progressive refinement methodology that moves from "shallow" to "deep" probabilistic programs. Starting with simple models that capture broad uncertainties, the approach gradually adds layers of detail and structure as more evidence becomes available. This hierarchical approach to uncertainty allows for a systematic reduction of ignorance while maintaining flexibility at each level. Like zooming into a blurry picture, the process reveals finer details only when warranted by data and evidence, preventing premature commitment to potentially wrong assumptions. These concepts find particular resonance in entrepreneurship, where decision-makers must navigate complex uncertainties while building new ventures. The framework suggests practical strategies: starting with deliberately broad assumptions, layering learning from coarse to fine details, updating beliefs iteratively with new evidence, and maintaining flexibility for unexpected developments. By treating business models as living probabilistic programs that can be refined through data and experience, entrepreneurs can make more informed decisions while remaining adaptable to change. This Bayesian approach to entrepreneurship transforms uncertainty handling from a vague art into a structured, computational process that can be systematically improved over time."

----

‚≠êÔ∏èrational inference
- "intelligence as emergent" is the foil of this lecture (üôã‚Äç‚ôÄÔ∏è this means vikash is against it? - why?)
- MENTAL SKILL: visualize the entire space - where the mass flows

‚≠êÔ∏èprogrammable inference
- modular, symbolic, programmable language with compiler receiving generating program"s" and data stream and outputs custom algorithms
- dsl program (example of deep - constrain model space) for time series (linear, squared ; e = ..9, ; compete with 
- robustness for `inference programs` (prababilty of unacceptable failure - only soundness guarante) and soundness for probabilistic programming platform  (sounness building the system building that platform; interpreter will encode; meaning preserving); NN fails to be sound/robust (soundness ~ verification test and robustness ~ validation test)

----
**didn't find the need to digest yet**

<img width="1469" alt="image" src="https://github.com/user-attachments/assets/718088af-8322-4473-8e4b-7a81eb82e10c" />

The fact that they're policy computing systems in a compositional way. So liquid model probabilistic programming, which is developed by, principally by Alex Lew is a probabilistic programming model for language models that give some interesting new capabilities. So here's one distinctive problem of generating poems that can be plausibly reversed. Let's say you could ask GPT4 to try to generate a first line of a poem that could possibly be reversed to form the ending line of the problem. You don't need to generate the whole poem just a phrase whose words can be reversed. It turns out that many strong models really struggle with this. So GPT4 might say, Dawn, silence, whispers, softly, evenings, Echo fades, reversing. It fades. Echo, evening, softly, whispers, silence, thoughts didn't really work, so this task was introduced in the sparks of AGI paper as an example of failures of planning in language models. 

But you can actually use a llama 2 family model, plus our language model problems to programming language to get solutions like cold and dark. Was it when winter last, which traverses to last winter when it was dark and cold? Or are we there yet? Yeah, there we are, or there was nothing but love in her eyes, eyes are in love, but nothing was there. the way we did this was by writing a language model, probabilistic program that combines forward generation process with a test that reverses what was for was generated forward and scores. It may be a pretty intuitive reasoning strategy for solving the problem. 

This is an instance of a much more general affordance that language model problems to programs, which is the ability to take complex task specifications and model at least some of the semantics of this task specifications. So we can't understand symbolically all the semantics of some code generation task, but we can't break it down into right x such that y, which we could model a sample from a distribution prompted by an LLM over exits, but actually re weighted by a likelihood score that assesses the probability of y given x. So this is an example, again, of going from shallower to deep interpretations here, though it's of the task. You just throw it all the language model, or you could try to parse the task a little bit into a compositional specification, which you could then solve with probabilistic inference. So again, it's this ü¶¶shallow to deep inference process. And over the course of this semester, we'll actually see some some recent results that actually do natural language probabilistic programming with language models, where you can use language model probabilistic programs to take instructions and parse them into language model probabilistic programs so that you can actually implement the process like whatever just described. And that turns out to have some interesting capabilities in practice, and it could be even competitive with four much cheaper than 01, with very, very small models. Now we'll spend a little time when we talk about language model probabilistic programming, situating it against sort of the recent flurry of interest in reasoning, quotes of various kinds, and the chain of thought and all that kind of stuff. 
But I do think it's worth highlighting that in the last year, two research awards, one at ICML and one went to sequential Monte Carlos for inference and reinforcement learning and language our language model promo super programming paper is actually going to be COVID and all that I clear this year. So I just want to point out that for people who are interested in language theme, but has some currency, but there's a wide range of approaches, some that are really based on machine learning, and some that may look a little bit more like üß†Rational inference.

üö®todo: extract every context with "rational"