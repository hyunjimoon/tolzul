ë˜˜ë˜˜(toltol) in korean means smart and bright.

2025-04-03
[[ğŸ—£ï¸(style)]] 


ì„¸ë ¨ë˜ë‹¤: ë§ì´ë‚˜ ê¸€, í–‰ë™ ë“±ì´ ì„œíˆ¬ë¥´ê±°ë‚˜ ì–´ìƒ‰í•˜ì§€ ì•Šê³  í›Œë¥­í•˜ê³  ëŠ¥ìˆ™í•˜ë‹¤.

> For everything that we do in OR, context is important, and that means that one cannot avoid the details of the domain. The best solutions typically combine good methodology with enough domain knowledge to amplify the impact of the methods. Besides, domain knowledge can be learned by talking to experts.

> If I may introduce my interest further,Â [this](https://arxiv.org/pdf/2103.10522.pdf)Â research requires a fundamental understanding of confidence interval, power, and sampling (Could you be generous enough to classify these concepts as domain knowledge? After all, the concept of 'power' is statisticians'Â inventionÂ for system efficiency like most otherÂ domainÂ knowledge). It was interesting to see how they cast the best confidence interval problem as an optimization problem.

I would not call this domain knowledge. It is definitely statistical/mathematical knowledge but not really domain knowledge -- at least the way I define it. To mean a "domain" is something physical, with physical constraints. A power grid, a queuing system, a logistics and distribution system, etc. are all domains. They have physical constraints that we (as mathematicians, statisticians, OR researchers) abstract out and create mathematical/statistical models. Within the model we can certainly have ways of transforming one question into another that is more tractable. - Garud Iyengar

Cronon essay 1995
> The only thing we have to preserve Nature with is Culture


> One thing that can be confusing in statistics is that similar analyses can be performed in different ways. #ROS

### ROS
From C.Robert's [review](https://xianblog.wordpress.com/2020/07/23/the-art-of-regression-and-other-stories/) on #ROS
> debate between Bayesian and likelihood solutions is quite muted, with aÂ recommendation for weakly informative priors superseded by the call for exploring the impact of oneâ€™s assumption.Â (Although the horseshoe prior makes an appearance, p.209!) [[_ref/QnA]]
> machine learning remains a form of regression, which can be evaluated by simulation of fake data and assessed by X validation, hence quite within the range of theÂ [book](https://amzn.to/2BO5ACB).

### BDA
From C.Robert's [review](https://xianblog.wordpress.com/2014/03/28/bayesian-data-analysis-bda3/) on #BDA
> unique feature of introducing weakly informative priors (Sections 2.9 and 5.7), like the half-Cauchy distribution on scale parameters. It may not be completely clear how weak a weakly informative prior, but this novel notion is worth including.
> Chp.5Â broaches on improper posteriors by suggesting to run a Markov chain that can exhibit improperness by enjoying an improper behaviour.Â When it happens as in the quote above, fine!, but there is no guarantee this is always the case! For instance, improperness may be due to regions near zero rather than infinity. [[_ref/QnA]]
> the creative choices that are required, first to set up a Bayesian model in a complex problem, then to perform the model checking and confidence building that is typically necessary to make posterior inferences scientifically defensible (p.139)
#### Chp.8 and 9 are <span style="color:red">time-symmetric</span>!
> Chapter 8 is about data collection, sample surveys, randomization and related topics and Chapter 9 is the symmetric in that it focus on the post-modelling step of decision making.


"I skate to where the puck is going to be, not where it has been." - Wayne Gretzky, a legendary ice hockey player

- fashionists get inspired from 4 history, geography, art, nature (hgan)


- [ë°•ì°¬ìš± ê°ë…ì€ ê¹€íƒœë¦¬ì™€ ë§ˆì£¼í•œ ìˆœê°„ì„ ë– ì˜¬ë¦¬ë©° â€œëˆˆë¹›ì´Â _ë˜˜ë˜˜_í–ˆë‹¤. ì–´ë””ì—”ê°€ ë”± ë°•íŒ ëŠë‚Œì´ì—ˆë‹¤. ì „ì²´ì ì¸ ì¸ìƒì´ ë§ˆì¹˜ ì°¨ëŒì²˜ëŸ¼ ë‹¨ë‹¨í–ˆë‹¤â€ê³  íšŒìƒí–ˆë‹¤](https://sports.donga.com/article/all/20160613/78636822/2)