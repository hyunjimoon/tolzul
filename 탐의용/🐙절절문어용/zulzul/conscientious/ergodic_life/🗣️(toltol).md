똘똘(toltol) in korean means smart and bright.

2025-04-03
[[🗣️(style)]] 


세련되다: 말이나 글, 행동 등이 서투르거나 어색하지 않고 훌륭하고 능숙하다.

> For everything that we do in OR, context is important, and that means that one cannot avoid the details of the domain. The best solutions typically combine good methodology with enough domain knowledge to amplify the impact of the methods. Besides, domain knowledge can be learned by talking to experts.

> If I may introduce my interest further, [this](https://arxiv.org/pdf/2103.10522.pdf) research requires a fundamental understanding of confidence interval, power, and sampling (Could you be generous enough to classify these concepts as domain knowledge? After all, the concept of 'power' is statisticians' invention for system efficiency like most other domain knowledge). It was interesting to see how they cast the best confidence interval problem as an optimization problem.

I would not call this domain knowledge. It is definitely statistical/mathematical knowledge but not really domain knowledge -- at least the way I define it. To mean a "domain" is something physical, with physical constraints. A power grid, a queuing system, a logistics and distribution system, etc. are all domains. They have physical constraints that we (as mathematicians, statisticians, OR researchers) abstract out and create mathematical/statistical models. Within the model we can certainly have ways of transforming one question into another that is more tractable. - Garud Iyengar

Cronon essay 1995
> The only thing we have to preserve Nature with is Culture


> One thing that can be confusing in statistics is that similar analyses can be performed in different ways. #ROS

### ROS
From C.Robert's [review](https://xianblog.wordpress.com/2020/07/23/the-art-of-regression-and-other-stories/) on #ROS
> debate between Bayesian and likelihood solutions is quite muted, with a recommendation for weakly informative priors superseded by the call for exploring the impact of one’s assumption. (Although the horseshoe prior makes an appearance, p.209!) [[_ref/QnA]]
> machine learning remains a form of regression, which can be evaluated by simulation of fake data and assessed by X validation, hence quite within the range of the [book](https://amzn.to/2BO5ACB).

### BDA
From C.Robert's [review](https://xianblog.wordpress.com/2014/03/28/bayesian-data-analysis-bda3/) on #BDA
> unique feature of introducing weakly informative priors (Sections 2.9 and 5.7), like the half-Cauchy distribution on scale parameters. It may not be completely clear how weak a weakly informative prior, but this novel notion is worth including.
> Chp.5 broaches on improper posteriors by suggesting to run a Markov chain that can exhibit improperness by enjoying an improper behaviour. When it happens as in the quote above, fine!, but there is no guarantee this is always the case! For instance, improperness may be due to regions near zero rather than infinity. [[_ref/QnA]]
> the creative choices that are required, first to set up a Bayesian model in a complex problem, then to perform the model checking and confidence building that is typically necessary to make posterior inferences scientifically defensible (p.139)
#### Chp.8 and 9 are <span style="color:red">time-symmetric</span>!
> Chapter 8 is about data collection, sample surveys, randomization and related topics and Chapter 9 is the symmetric in that it focus on the post-modelling step of decision making.


"I skate to where the puck is going to be, not where it has been." - Wayne Gretzky, a legendary ice hockey player

- fashionists get inspired from 4 history, geography, art, nature (hgan)


- [박찬욱 감독은 김태리와 마주한 순간을 떠올리며 “눈빛이 _똘똘_했다. 어디엔가 딱 박힌 느낌이었다. 전체적인 인상이 마치 차돌처럼 단단했다”고 회상했다](https://sports.donga.com/article/all/20160613/78636822/2)