[[966_hw2]]
using https://claude.ai/chat/10d2a46d-b0ed-4995-a03f-5e50be4138b5

| 1a. Distribution of final score | Understanding probabilistic outcomes in sequential dice rolls using a stopping condition                                                                                                                        | Basic recursive sampling: `generateFrom([])` with stop condition `sum(sequence) >= 10` | <br>var fairDice = Categorical({vs: [1, 2, 3, 4, 5, 6], ps: [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]})<br>var generateFrom = function(sequenceSoFar) {<br>  var roll = sample(fairDice)<br>  var sequence = sequenceSoFar.concat(roll)<br>  if(sum(sequence) >= 10) {<br>    return sequence<br>  } else {<br>    return generateFrom(sequence)<br>  }<br>}<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                           |
| ------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------- |
| 1b. Distribution of rolls       | Analyzing length of sequences in recursive processes                                                                                                                                                            | Track sequence length: `sequence.length`                                               | var generateFrom = editor.get("generateFrom")<br>var model = function() {<br>   generateFrom([]).length<br>}<br>viz(Infer({method:'enumerate'}, model))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                           |
| 1c. Conditional distribution    | Conditional reasoning with stopping criteria; conditional on the sum > 13 (larger than the other), distribution of the last number is greater than average; it should be at 4 as the last sum was lower than 10 | Use condition: `condition(sum(sequence) <= 13)`                                        | var generateFrom = editor.get("generateFrom")<br>var model = function() {<br>  var sequence = generateFrom([])<br>  condition(sum(sequence) >= 13)<br>  sequence.pop()<br>}<br>viz(Infer({method:'enumerate'}, model))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | ![[Pasted image 20241118210345.png\|100]] |
| 2a. Casino dealer sampler       | Implementing Hidden Markov Model (HMM) as recursive generator                                                                                                                                                   | Track two sequences (coins & faces) with transition prob 0.2 and emission prob 0.2/0.8 | var pHeads_1 = 0.3<br>    var pHeads_2 = 0.7<br>    var currentLength = outcomes.coins.length<br>    var previous_coin = outcomes.coins[currentLength-1]<br>    var other_coin = previous_coin == pHeads_1 ? pHeads_2 : pHeads_1<br>    var switch_prob = 0.15<br><br>    var coin = currentLength == 0 ? (flip() ? pHeads_1:pHeads_2) : <br>               (flip(switch_prob) ? other_coin : previous_coin)<br><br>    var coins = outcomes.coins.concat(coin)<br>    var face = flip(coin) ? "H" : "T"<br>    var faces = outcomes.faces.concat(face)<br>    <br>    var newOutcomes = {coins:coins, faces:faces}<br>    <br>    if(newOutcomes.faces.length == n){<br>        return newOutcomes <br>    }<br>    else{<br>        return generateFrom(newOutcomes, n)<br>    }<br>}                                                 |                                           |
| 2b. Short sequence inference    | Inferring hidden states from short observation sequence                                                                                                                                                         | Condition model on faces [H,H,H,H,T,H,H,T] to infer coins                              | var observations = ['H', 'H', 'H', 'H', 'T', 'H', 'H', 'T']<br>var model = function() {<br>  var outcomes = generateFrom({coins:[], faces:[]}, observations.length)<br>  condition(_.isEqual(outcomes.faces, observations))<br>  return outcomes.coins<br>}<br>var dist = Infer({method:"enumerate"}, model)<br>viz.casino(observations, dist)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                           |
| 2c. Long sequence inference     | Understanding limitations of exact inference                                                                                                                                                                    | Same as 2b but longer sequence shows enumeration limits                                | var observations = ['H', 'H', 'H', 'H', 'H', 'T', 'T', 'T', 'T', 'H', 'T', 'H', 'H', 'H', 'H']<br>var model = function() {<br>  var outcomes = generateFrom({coins:[], faces:[]}, observations.length)<br>  condition(_.isEqual(outcomes.faces, observations))<br>  return outcomes.coins<br>}<br>var dist = Infer({method:"MCMC", samples:50000}, model)<br>viz.casino(observations, dist)                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                           |
| 2d. Using observe()             | Making inference tractable with likelihood weighting                                                                                                                                                            | Replace condition with `observe(Bernoulli(theta[coin]), face=='H')`                    | var generateFrom = function(coins, observations) {<br>  var pHeads_1 = 0.3<br>  var pHeads_2 = 0.7<br>  var n = observations.length<br>  var currentLength = coins.length<br>  var observedFace = observations[currentLength]<br>  var previous_coin = coins[currentLength-1]<br>  var other_coin = previous_coin == pHeads_1 ? pHeads_2 : pHeads_1<br>  var switch_prob = 0.15<br>  <br>  var coin = currentLength == 0 ? (flip() ? pHeads_1:pHeads_2) : <br>             (flip(switch_prob) ? other_coin : previous_coin)<br>  var coinToss = Categorical({vs: ["H", "T"], ps: [coin, 1-coin]})<br>  <br>  observe(coinToss, observedFace)<br>  <br>  var newCoins = coins.concat(coin)<br>  <br>  if(newCoins.length == n){<br>    return newCoins <br>  }<br>  else{<br>    return generateFrom(newCoins, observations)<br>  }<br>} |                                           |
| Question                        | Key Takeaway                                                                                                                                                                                                    | Core Code Concept                                                                      | answer or code                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                           |
| 3a. Parameter sensitivity       | Understanding parameter effects on inference                                                                                                                                                                    | Test different switching probabilities and coin weights                                | - Higher pswitch (0.85 vs 0.15) leads to more frequent switching predictions<br>- When coins are strongly biased (0.3/0.7), predictions are more confident<br>- Equal pswitch values (0.5/0.5) lead to less structured predictions<br>Code shows tests with:<br>- Table 1 params: pswitch=0.15/0.15, pHeads=0.3/0.7<br>- Modified params: pswitch=0.5/0.5, pHeads=0.5/0.45<br>- Another set: pswitch=0.15/0.15, pHeads=0.5/0.45                                                                                                                                                                                                                                                                                                                                                                                                         |                                           |
| 3b. Human data comparison       | Validating model against human judgments                                                                                                                                                                        | Plot subject ratings vs model predictions                                              | 1. When model posterior is near 0 or 1, humans exaggerate to exactly 0 or 1<br>2. Surprising agreement between human and model predictions for Flips 8-9<br>3. Model shows "considerable qualitative agreement" with human predictions<br>4. Question of whether to analyze average or individual human predictions depends on research question (shared vs individual priors)                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                           |
| 3c. Parameter learnability      | Identifying hard-to-learn parameters                                                                                                                                                                            | Demonstrate with example sequences                                                     | 1. Easy to predict when:<br>- Coins are strongly biased (0.3 vs 0.7)<br>- Frequent switching (p=0.85)<br>2. Hard to predict when:<br>- Moderately biased coins (0.4 vs 0.6)<br>- Infrequent switching (p=0.05)<br>3. Model outperforms humans when:<br>- p(switch)=0.5 both directions<br>- pHeads close (0.45 vs 0.55)<br>Because model better handles sequential dependencies                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                           |
| 3d. HMM limitations             | Critical analysis of model assumptions                                                                                                                                                                          | Discuss scenarios where HMMs fail                                                      | 1. HMM seems to describe qualitative human predictions well<br>2. Main limitation: Markov assumption fails when p(switch) depends on last n flips<br>3. Potential solution: Redefine state to include n flips, but this expands state space considerably \|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                           |


You're right. Let me help revise question 3 with all parts (a-d) according to the updated parameters and structure from the questions.

For 3(a)(i), here's the code:
```javascript
var makeModel = editor.get("makeModel")

// Test 1: Different coin weights
var obss = ['H', 'H', 'H', 'T', 'T', 'T', 'T', 'T', 'T', 'H']
// Original model (Table 1)
var model_table1 = makeModel(0.2, 0.2, 0.2, 0.8, obss)
// Modified coin weights
var model_parsing_pheads = makeModel(0.2, 0.2, 0.1, 0.9, obss)
// Modified switch probability
var model_parsing_pswitch = makeModel(0.5, 0.5, 0.2, 0.8, obss)

var dist_gen = Infer({method:"MCMC", samples:50000}, model_table1)
var dist_parsing_pheads = Infer({method:"MCMC", samples:50000}, model_parsing_pheads)
var dist_parsing_pswitch = Infer({method:"MCMC", samples:50000}, model_parsing_pswitch)

viz.casino(obss, dist_gen)
viz.casino(obss, dist_parsing_pheads)
viz.casino(obss, dist_parsing_pswitch)
```

For 3(a)(ii):
```javascript
var makeModel = editor.get("makeModel")

// Generate fair coin sequences
var fairSeq1 = repeat(10, function() {flip(0.5) ? "H" : "T"})
var fairSeq2 = repeat(10, function() {flip(0.5) ? "H" : "T"})

// Original model
var model1 = makeModel(0.2, 0.2, 0.2, 0.8, fairSeq1)
// Modified switch probabilities
var model2 = makeModel(0.5, 0.5, 0.2, 0.8, fairSeq1)

var dist1 = Infer({method:"MCMC", samples:50000}, model1)
var dist2 = Infer({method:"MCMC", samples:50000}, model2)

viz.casino(fairSeq1, dist1)
viz.casino(fairSeq1, dist2)
```

For 3(b):
```javascript
var makeModel = editor.get("makeModel")

// Sequence 1
var obss1 = ["H","T","T","T","H","T","T","T","H","H"]
var p_coin1 = [0.66, 1, 1, 1, 1, 1, 1, 1, 0.33, 0.33]
display("Human ratings")
viz.casino(obss1, p_coin1)

var model = makeModel(0.2, 0.2, 0.2, 0.8, obss1)
var dist = Infer({method:"MCMC", samples:50000}, model)
display("Model predictions")
viz.casino(obss1, dist)
```




















