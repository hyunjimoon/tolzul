


### 2. mdp formulation (using reinforcement learning and optimal stopping) 


|                              | <mark class  = "green"> ğŸ‘ï¸state</mark>                                                                                                                                                                                      | <mark class  = "red">ğŸ‘®ğŸ»policy</mark>, learning alg, bellman eq                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | <mark class  = "orange">ğŸ¤œaction</mark>                                                                                                                                 | <mark class  = "purple"> ğŸ‘€ğŸ¤œtransition</mark>                                                     | <mark class  = "yellow">ğŸ’°reward</mark>, obj (max long term expected rwd)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | ğŸ–¼ï¸diagram                                | (timestep, horizon, $\gamma$); H=1/1-$\gamma$ |
| ---------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------- | --------------------------------------------- |
| Market-Product Pivot MDP     | (m, p, o, u, e)<br><br>m: current market<br><br>p: current product<br><br>o: optimism (mean of prediction for evaluation)<br><br>u: uncertainty (sd of prediction for evaluation)<br><br>e: remaining experiment opportunity | Ï€(s) -> a<br>  - s: current state (m, p, o, u, e)<br>  - a: action to take (change_market or change_product)<br><br>- V(s) = max_a Q(s, a), where:<br>  - s: current state (m, p, o, u, e)<br>  - a: action (change_market or change_product)<br>  - Q(s, a): action-value function, representing the expected cumulative reward starting from state s and taking action a, following the optimal policy thereafter.<br><br>V(s) = max_a [r(s, a) + Î³ * sum_{s'} T(s, a, s') * V(s')], where:<br>  - s: current state (m, p, o, u, e)<br>  - a: action (change_market or change_product)<br>  - r(s, a): immediate reward for taking action a in state s<br>  - Î³: discount factor (0 â‰¤ Î³ â‰¤ 1)<br>  - s': next state (m', p', o', c')<br>  - T(s, a, s'): state transition probability from s to s' when taking action a<br> | (change_market, change_product)                                                                                                                                         | T(s, a, s') = P(s' \| s, a)<br><br><br>s: (m, p, o, u, e)<br>a: action<br>s': (m', p', o', u', e') | r(s, a) = customer review(m, p)<br>Observed customer review for current market and product<br><br>Ï€* =$\underset{\pi}{argmax} E[\sum_{t=0}^âˆ Î³^t * r(s_t, a_t) \| Ï€]$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | ![[Pasted image 20240428153517.png\|500]] |                                               |
| e2a, a2                      | ğŸ‘®ğŸ»2ğŸ‘®ğŸ»(ğŸ‘ï¸,  $\epsilon^a$)<br>ğŸ•˜<br>update ğŸ‘®ğŸ»                                                                                                                                                                           | ğŸ‘®ğŸ»2ğŸŒ(ğŸ‘ï¸) =ğŸ¤œ<br>ğŸ•¡                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | ğŸŒ2ğŸŒ(ğŸ‘ï¸,ğŸ¤œ)=ğŸ‘ï¸'<br>ğŸ•<br>update ğŸ‘ï¸                                                                                                                                  | ğŸŒ2ğŸ‘®ğŸ»(ğŸ‘ï¸,ğŸ¤œ, ğŸ‘ï¸')=ğŸ’°<br>ğŸ•’                                                                     | <mark class  = "yellow">ğŸ’°</mark>(<mark class  = "green"> ğŸ‘ï¸</mark>,<br>  <mark class  = "purple"><mark class  = "purple"> ğŸ‘€ğŸ¤œ</mark></mark>(<mark class  = "green"> ğŸ‘ï¸</mark>, <br>       <mark class  = "orange">ğŸ¤œ</mark>(=<mark class  = "red">ğŸ‘®ğŸ»</mark>(<mark class  = "green"> ğŸ‘ï¸</mark>,  $\epsilon^a$))<br>     , $\epsilon^e$)<br>)<br><mark class  = "yellow">reward</mark>(<mark class  = "green"> state</mark>,<br>  <mark class  = "purple"><mark class  = "purple"> transition</mark></mark>(<mark class  = "green"> state</mark>, <br>       <mark class  = "orange">action</mark>=<mark class  = "red">policy</mark>(<mark class  = "green"> state</mark>,  $\epsilon^a$)<br>     , $\epsilon^e$)<br>) |                                           |                                               |
| ğŸ›‘parking (optimal stopping) | $\begin{aligned} & \{(0, T),(O, A) \\ & (1, T),(1, A) ,..., (C, A) \\ & (C, T), leave, park \}\end{aligned}$                                                                                                                 | <mark class  = "blue"> Q2.reward = avg time in system?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | $\left\{\begin{array}{l}(\text { park, continue) if } s=(\cdot, A) \\ (\text { continue) if } s=(\cdot, T) \text {, } \\ \text { do nothing } o. w\}\end{array}\right.$ |                                                                                                    | ![[Pasted image 20240429083718.png\|300]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | ![[Pasted image 20240429083741.png\|300]] |                                               |
