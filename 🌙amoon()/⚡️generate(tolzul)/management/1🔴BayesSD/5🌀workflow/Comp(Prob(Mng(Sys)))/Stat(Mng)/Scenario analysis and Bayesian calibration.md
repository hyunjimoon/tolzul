## 1. What
Iterative calibrating the chosen generative model system with self-organization which passes predictive checks, SBC, and return satisfactory diagnostics . Chosen  model consists of both mathematical and programic conveyor for {prior, Likelihood, computational algorithm}.

## 2. How
### prior
- Customized verison (probability invese tranform) is possible but choosing among [[Math-Family of distribution]] is conventional.
- SDì˜ priorì„¤ì •: ë‹¤ìŒ ì˜ìƒì˜ 06:17ì— ì‹¤ì œ dataí”¼íŒ…ì „, synthetic dataë¥¼ ìƒì„±í•´ reality checkì„ í•˜ëŠ” êµ¬ì¡°ê°€ ì†Œê°œë©ë‹ˆë‹¤. Stan+SDë…¼ë¬¸ì—ì„œë„ priorë¡œ priorSimë°ì´í„°ë¥¼ ë¨¼ì € ìƒì„±í•˜ê³  ì´ì— í”¼íŒ…í•©ë‹ˆë‹¤.

<iframe title="vimeo-player" src="https://player.vimeo.com/video/206297867?h=1d77447143" width="640" height="360" frameborder="0" allowfullscreen></iframe>
 
### likelihood
The two representative likelihood pairs are: (`poisson`, `negative binomial`) for non-negative or integer type data and (`normal`, `t`) for data without any range or type contraint. Each of them form a pair as the latter is mixture version of the former meaning the latter is a more robust (a.k.a. added layer of parameter via hyperprior) version. Notice using the mixture version of likelihood, we can have the same effect as giving the prior for the parameter.

#### `poisson` $\rightarrow$ `negative binomial`
Mixture of poisson with common rate that follows a gamma distribution becomes negative binomal mixture:

$\operatorname{NegBin}\left(t \mid n, \frac{1}{\theta+1}\right)=\int_{0}^{\infty} \operatorname{Pois}(t \mid \lambda) \operatorname{Gamma}(\lambda \mid n, \theta) d \lambda$

#### `normal` $\rightarrow$  `t`
Mixture of normals with common mean and variances that follow an inverse-gamma distribution. Formally, let X | W be normal with mean 0 and variance W. Let W âˆ¼ inverse gamma(Î½/2, Î½/2). Then the marginal distribution on X is Student-t with Î½ degrees of freedom.

$\begin{aligned} f_{X}(x) &=\int_{0}^{\infty} f_{X \mid w}(x) f_{W}(w) d w \propto \int_{0}^{\infty} \frac{1}{\sqrt{w}} \exp \left(-\frac{x^{2}}{2 w}\right) w^{-\frac{v}{2}-1} \exp \left(-\frac{v}{2 w}\right) d w \\ &=\int_{0}^{\infty} w^{-\frac{v+1}{2}-1} \exp \left(-\frac{x^{2}+v}{2 w}\right) d w =\Gamma\left(\frac{v+1}{2}\right)\left(\frac{x^{2}+v}{2}\right)-\frac{v+1}{2} \end{aligned}$

Interesting extension on variance decomposition [here](https://www.johndcook.com/t_normal_mixture.pdf).

### computational method
HMC, default stan estimator, is believed to fully recover the samples that represent the typical set of posterior space. However, due to feasibility issues such as [`L-BFGS` algorithm for opitimization](https://mc-stan.org/docs/2_29/reference-manual/bfgs-and-l-bfgs-configuration.html) or [`ADVI` algorithm for variational inference.](https://mc-stan.org/docs/2_29/reference-manual/vi-algorithms.html) 

## 3. Why

The roles of prior is summarized in [[ğŸŒ€Comp(Stat)]]. Prior constructed purely from our knowledge may not be its optimal form as the computational aspect is not relfected yet.                  
- one shot vs iterative of setting prior can be found in [this](https://arxiv.org/pdf/2112.01380.pdf) paper p.10 D5. 


For more detail on history of Bayesian calibration refer to [1. Intro ](https://www.hyunjimoon.com/blog/bayesian-calibration-1-introduction/) and SBC's role to [2.SBC as navigator](https://www.hyunjimoon.com/blog/bayesian-calibration-2-sbc-as-a-navigator/).

---

Specific question as below motivated this writing.

> SDì— í¬í•¨ëœ ë³€ìˆ˜ë“¤ì˜ ëª¨ìˆ˜ë¥¼, ì‹œìŠ¤í…œ ì „ì²´ ê´€ì ì—ì„œ ìµœì ì˜ ëª¨ìˆ˜ë¥¼ ì°¾ëŠ”ê±´ê°€ìš”? 

$\lambda$ ~ `gamma(1,1)` -Vensim SD generator-> $\tilde{y}$ - Stan HMC estimator -> $\lambda$ ~ ??

If ?? is close enough to `gamma(1,1)`. ê·¸ í›„ ì‹¤ì œ ë°ì´í„°ë¥¼ ìœ„ $\tilde{y}$ì— ëŒ€ì….

> ê·¸ëŸ¬ë©´ ê²°êµ­ í†µê³„ëª¨ë¸ì— ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì •ì´ ì‹œìŠ¤í…œ ê´€ì ì—ì„œ ìµœì í™”ë˜ëŠ”ê²Œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì´ ë˜ëŠ”ê±´ê°€ìš”? 

calibrationìš©ì–´ê°€ í˜¼ìš©ë˜ê³  ìˆì–´ìš”. estimatorë¡œ ì“°ì¼ìˆ˜ë„ ìˆê³ , iterative updateë¥¼ í†µí•œ ìˆ˜ë¦¬ì´ë¡ ì +ì»´í“¨íŒ…ê³„ì‚°ì  ëª¨ë¸ì˜ í•©ì¹˜ë¡œ ì“°ì¼ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. 1. ìˆ˜ë¦¬ì ëª¨ë¸ê³¼ ì»´í“¨í„°ëª¨ë¸ì„ êµ¬ë³„í•˜ì§€ ëª»í•˜ê±°ë‚˜ 2. ì»´í“¨í„°ëª¨ë¸ì´ ì™„ë²½íˆ ìˆ˜ë¦¬ì ëª¨ë¸ì„ ì™„ë²½íˆ ì¬í˜„í•´ë‚¼ë•Œ ë‘ ì˜ë¯¸ê°€ ê°™ì•„ì§€ëŠ”ë°ìš”. pre-asymptoticí•œ ìƒí™©ì—ì„œëŠ” êµ¬ë³„í•˜ëŠ”ê±¸ ì¶”ì²œë“œë¦½ë‹ˆë‹¤. 

> ì‹œìŠ¤í…œ ì „ì²´ ê´€ì ì—ì„œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ë˜ë©´ ê¸°ì¡´ì˜ í†µê³„ëª¨ë¸ê³¼ëŠ” ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚˜ë‚˜ìš”? ì›ë˜ í†µê³„ëª¨ë¸ ìì²´ê°€ ì‹œìŠ¤í…œì˜ ì‘ë™ í˜„ìƒë“¤ì´ ë°˜ì˜ëœ ë°ì´í„°ë¡œ ì¶”ì •í•˜ëŠ” ê²ƒì¸ë° ë‘˜ì˜ ì°¨ì´ê°€ ìˆë‚˜ìš”?

"ê¸°ì¡´ì˜ í†µê³„ëª¨ë¸"ìƒì˜ ì¶”ë¡ ì´ SD calibaritonì„ í¬í•¨í•©ë‹ˆë‹¤. ODE systemì´ë¼í•˜ê³  stanì—ì„œ ì´ë¥¼ ì§€ì›í•˜ê³  ìˆì–´ìš”.