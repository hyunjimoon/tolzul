value proposition: 
- entrepreneurial freedom ensuring automation by cleverly brute forcing lens and tools
- probabilistic programming language to represent, compare, choose entrepreneurial experimentation logic
product: probabilistic program for educating pivoting strategy and operations
mvp: experiment toolbox: a scientific procedure undertaken to make a discovery, test a hypothesis, demonstrate a known fact (exploratory (behavior), move testing (do you get what you intend,  do you like what you get), hypothesis test (improve understanding) 

demand: educators (do you have any statistics that this is increasing?)
supply:  entrep choice, bayesian workflow, system dynamics modeling, probabilistic program

probabilistic program language for choice-based entrepreneurs
word definition: probabilistic program, pivoting strategy (lens) and operations (tools)

q1. based on design from aether, how can we design language for entrep strategy?

supply: 
- cleverly brute forcing lens and tool
- process: an organized group of related activities and tasks that work together to create value to the customer
- automate: modularized process with designed interface 

reality (aether), power (utility), time (), space (), mind (vision), soul (optimization)

decision geometry 
## nature
#### understanding bayes modeling for entrepreneurial decision
1. A general framework for updating belief distributions (P. Bissiri, C. Holmes, S. Walker): We propose a framework for general Bayesian inference. We argue that a valid up date of a prior belief distribution to a posterior can be made for parameters which are connected  to observations through a loss function rather than the traditional likelihood function, which is  recovered as a special case. Modern application areas make it increasingly challenging for  Bayesians to attempt to model the true data-generating mechanism. For instance, when the  object of interest is low dimensional, such as a mean or median, it is cumbersome to have to  achieve this via a complete model for the whole data distribution. More importantly, there are  settings where the parameter of interest does not directly index a family of density functions  and thus the Bayesian approach to learning about such parameters is currently regarded as  problematic. Our framework uses loss functions to connect information in the data to functional of interest. The updating of beliefs then follows from a decision theoretic approach involving  cumulative loss functions. Importantly, the procedure coincides with Bayesian updating when  a true likelihood is known yet provides coherent subjective inference in much more general  settings. Connections to other inference frameworks are highlighted. Keywords : Decision theory; General Bayesian updating; Generalized estimating equations;  Gibbs posteriors; Information; Loss function; Maximum entropy; Provably approximately  correct Bayes methods; Self-information loss function

2. Parameterization and Bayesian Modeling (Andrew Gelman): Progress in statistical computation often leads to advances in statistical modeling. For example, it is surprisingly common that an existing model is reparameterized, solely for computational purposes, but then this new con guration motivates a new family of models that is useful in applied statistics. One reason why this phenomenon may not have been noticed in statistics is that reparameterizations do not change the likelihood. In a Bayesian framework, however, a transformation of parameters typically suggests a new family of prior distributions. We discuss examples in censored and truncated data, mixture modeling, multivariate imputation, stochastic processes, and multilevel models. KEY WORDS:Censored data; Data augmentation; Gibbs sampler; Hierarchical model; Missing-data imputation; Parameter expansion; Prior distribution; Truncated data.

3. The geometry of decision theory (Alexander Philip Dawid, Steffen Lauritzen): A decision problem is deﬁned in terms of an outcome space, an action space and a loss function. Starting from these simple ingredients, we can construct: Proper Scoring Rule; Entropy Function; Divergence Function; Riemannian Metric; and Unbiased Estimating Equation. We illustrate these for the case of a Riemannian outcome space. From an abstract viewpoint, the loss function deﬁnes a duality between the outcome and action spaces, while the correspondence between a distribution and its Bayes act induces a self-duality. Together these determine a “decision geometry” for the family of distributions on outcome space. This allows generalisation of many standard statistical concepts and properties. In particular we deﬁne and study generalised exponential families.

#### solving bayes modeling for entrepreneurial decision
1. Geometric Deep Learning Grids, Groups, Graphs, Geodesics, and Gauges (M. Bronstein, J. Bruna, T. Cohen, P. Veličković)'s sec.2 Learning in High Dimensions: [[geom_dl_highdim.pdf]]
2. domain specific language 
3. `From word to world models`: How does language inform our downstream thinking? In particular, how do humans make meaning from language—and how can we leverage a theory of linguistic meaning to build machines that think in more human-like ways? In this paper, we propose rational meaning construction, a computational framework for language-informed thinking that combines neural models of language with probabilistic models for rational inference. We frame linguistic meaning as a context-sensitive mapping from natural language into a probabilistic language of thought (PLoT)—a general-purpose symbolic substrate for probabilistic, generative world modeling. Our architecture integrates two powerful computational tools that have not previously come together: we model thinking with probabilistic programs, an expressive representation for flexible commonsense reasoning; and we model meaning construction with large language models (LLMs), which support broad-coverage translation from natural language utterances to code expressions in a probabilistic programming language. We illustrate our framework in action through examples covering four core domains from cognitive science: probabilistic reasoning, logical and relational reasoning, visual and physical reasoning, and social reasoning about agents and their plans. In each, we show that LLMs can generate context-sensitive translations that capture pragmatically-appropriate linguistic meanings, while Bayesian inference with the generated programs supports coherent and robust commonsense reasoning. We extend our framework to integrate cognitively-motivated symbolic modules (physics simulators, graphics engines, and goal-directed planning algorithms) to provide a unified commonsense thinking interface from language. Finally, we explore how language can drive the construction of world models themselves. We hope this work will help to situate contemporary developments in LLMs within a broader cognitive picture of human language and intelligence, providing a roadmap towards AI systems that synthesize the insights of both modern and classical computational perspectives.


ideal assistive partners understand our actions, words, and instructions as expressions of goals, beliefs, and intentions 59–61 that are grounded in physical possibilities62, while also understanding that these can be shared across multiple minds
How should Bayesian methods for the social sciences differ from Bayesian methods for the physical sciences?
## culture
#### visioning of bayes modeling for entrepreneurial decision
1. `Operations for entrepreneurs: Can Operations Management make a difference in entrepreneurial theory and practice?` (C.Fine, ) Although entrepreneurship-related papers have had some representation in Production and Operations Management (POM) over the past 30 years, the topic still seems a bit like a poor stepchild in the research of operations management (OM) scholars. Yet, entrepreneurship is important to the economy, and many schools are growing signiﬁcantly their entrepreneurship programs and offerings but often without reference to or inclusion of operations courses. This paper is motivated by the question of the operations needs of new ventures and how they might differ from the needs of large, established ﬁrms. Toward that end, we review brieﬂy the state of entrepreneurship scholarship in POM (and beyond), present our own (ﬁeld-based) research (and cases), and propose a framework for what we call “operations for entrepreneurs,” that we hope can be a basis for further productive research and curriculum development by the OM community. KEYWORDS entrepreneurial operations, evolutionary entrepreneurship, new ventures, operations for entrepreneurs, startup dynamics

2. `Foundations of entrep.strategy` Research Summary: This paper develops an integrated framework linking the nature of the entrepreneurial choice process to the foundations of entrepreneurial strategy. Because entrepreneurs face many alternatives that cannot be pursued at once, entrepreneurs must adopt (implicitly or explicitly) a process for choosing among entrepreneurial strategies. The interplay between uncertainty and learning has the consequence that commitment-free analysis yields multiple, equally viable alternatives from which one must be chosen. This endogenous gap between optimization and choice is a central paradox confronting entrepreneurs. Resolving this allows for a reformulation of the foundations of entrepreneurial strategy, emphasizing the role of choice rather than the centrality of the strategic environment. Managerial Summary: The central strategic challenge for an entrepreneur is how to choose: entrepreneurs often face multiple potential strategies for commercializing their idea but due to the constraint of limited resources, cannot pursue them all at once. At the same time, entrepreneurs are venturing into new domains and as such, must choose under conditions of high uncertainty with only noisy learning available. This paper explores the interplay between these unique conditions that shape the entrepreneurial choice process, finding that often, the process will not yield a single best strategy but instead several equally attractive strategic alternatives. A key implication is that entrepreneurs cannot simply choose what not to do, but instead must proactively decide which equally viable alternatives to leave behind when choosing an entrepreneurial strategy. KEYWORDS: appropriability, entrepreneurial strategy, entrepreneurship, strategy, uncertainty

3. Choosing Technology (J.Gans, M. Kearney, E. Scott, S.Stern) A central premise of research in the strategic management of innovation is that start-ups are able to leverage emerging technological trajectories as a source of competitive advantage. But, if the potential for a technology is given by the fundamental character of a given technological trajectory, then why does entrepreneurial strategy matter? Or, put another way, if the evolution of technology is largely shaped by the strategic choices entrepreneurs make, then why do technological trajectories exhibit systematic patterns such as the technology S-curve? Taking a choice-based perspective, this paper illuminates the choices confronting a start-up choosing their technology by resolving the paradox of the technology S-curve through a reformulation of the foundations of the technology S-curve. Speciﬁcally, we reconceptualize the technology S-curve not as a technological given but as an envelope of potential outcomes reﬂecting differing strategic choices by the entrepreneur in exploration versus exploitation. Taking this lens, we are able to clarify the role of technological uncertainty on start-up strategy, the impact of constraints on technological evolution, and how technology choice is shaped by the possibility of imitation. Our ﬁndings suggest that staged exploration may stall innovation as a result of the replacement effect, increasing the strategic importance of commitment.

#### valuing bayes modeling for entrepreneurial decision (simulation, calibration, optimization)
1. `Simulation-Based Calibration Checking for Bayesian Computation: The Choice of Test Quantities Shapes Sensitivity` (M.Modrak, A.Moon, A.Gelman, A.Vehtari): Simulation-based calibration checking (SBC) is a practical method to validate computationally-derived posterior distributions or their approximations. In this paper, we introduce a new variant of SBC to alleviate several known problems. Our variant allows the user to in principle detect any possible issue with the posterior, while previously reported implementations could never detect large classes of problems including when the posterior is equal to the prior. This is made possible by including additional data-dependent test quantities when running SBC. We argue and demonstrate that the joint likelihood of the data is an especially useful test quantity. Some other types of test quantities and their theoretical and practical beneﬁts are also investigated. We provide theoretical analysis of SBC, thereby providing a more complete understanding of the underlying statistical mechanisms. We also bring attention to a relatively common mistake in the literature and clarify the diﬀerence between SBC and checks based on the data-averaged posterior. We support our recommendations with numerical case studies on a multivariate normal example and a case study in implementing an ordered simplex data type for use with Hamiltonian Monte Carlo. The SBC variant introduced in this paper is implemented in the SBC R package. Keywords: calibration, probabilistic programming, software testing.
   
2. `Some models are useful, but how do we know which ones? Towards a unified Bayesian model taxonomy` (P. Burkner, M. Scholz, S. Radev): Probabilistic (Bayesian) modeling has experienced a surge of applications in almost all quantitative sciences and industrial areas. This development is driven by a combination of several factors, including better probabilistic estimation algorithms, flexible software, increased computing power, and a growing awareness of the benefits of probabilistic learning. However, a principled Bayesian model building workflow is far from complete and many challenges remain. To aid future research and applications of a principled Bayesian workflow, we ask and provide answers for what we perceive as two fundamental questions of Bayesian modeling, namely (a) “What actually is a Bayesian model?” and (b) “What makes a good Bayesian model?”. As an answer to the first question, we propose the PAD model taxonomy that defines four basic kinds of Bayesian models, each representing some combination of the assumed joint distribution of all (known or unknown) variables (P), a posterior approximator (A), and training data (D). As an answer to the second question, we propose ten utility dimensions according to which we can evaluate Bayesian models holistically, namely, (1) causal consistency, (2) parameter recoverability, (3) predictive performance, (4) fairness, (5) structural faithfulness, (6) parsimony, (7) interpretability, (8) convergence, (9) estimation speed, and (10) robustness. Further, we propose two example utility decision trees that describe hierarchies and trade-offs between utilities depending on the inferential goals that drive model building and testing. Keywords: Probabilistic modeling, statistical learning, Bayesian statistics, machine learning, model comparison

three idea from system dynamics
1. forresterpuposeful model building 
2. aggregation 


#### idea agency: 

understandable for both user and maker, choosable, feedback
##### ent2bayes
-- (`user-understandable`) --> <font color  = "#C0A0C0"> bayesian visual representation</font>
-- (`maker-understadable`)-->  <font color  = "red">diagnostics (predictive/retrodictive check, simulation-based calibration)</font>
-- (`choosable`) --> decision-based calibration

--> `test quantity elicitation`

##### bayes2ent
-- (`choosable`) -> persuading through experiment and educate (bayesian entrepreneurship)
-- (`feedback`) -> growth of stan and system dynamics community 

industry's tech s-curve (backward looking) as envelope of individual's s-curve growth (forward looking)
causal for lower level agent becomes correlation in upper level agent (existence of latent variable that makes everything homogenous)

inject pink noise when generating synthetic data (uncertainty representation); design generator, estimator, test quantity ( #todo connect this with estimation error, precision error)

induction bias: approximation, statistical, optimization error (meaning of zero optimization error in entrep's behavior? hypothesis and function space correspondance?)

---
##### bayes2ent
<-- (`feedback`) -- growth of stan and system dynamics community 
<-- (`choice`) -- customer, technology, competition, organization


<-- (`user-understandable`) -- bayesian visual representation
<-- (`maker-understadable`)-- diagnostics (predictive/retrodictive check, simulation-based calibration)
<-- (`choosable`) -- decision-based calibration

##### ent4bayes
<--(`software, policy feedback`)-- system dynamics 
<--(`choice`)-- bayesian entrepreneurship
self-consistent prior 

of academia and creative destruction: startup (mover advantage)

#### individual agency