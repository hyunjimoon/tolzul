
pf. Let X be any set and P the set of all probabilities on X (for measure-theoretic details, see Diaconis and Freedman). For $F \in P$, let $F_k$ be the independent probability on sequences of length k, so $F_k(A_1, ..., A_k) = F(A_1)F(A_2)\cdots F(A_k)$ for $A_i$ subsets of X. If Î¼ is a probability on P, let

$P_{\mu k}(A) = \int F_k(A) \mu(dF)$ (11)

Let P be an exchangeable probability on sequences of length n. For $1 \leq k \leq n$, let $P_k$ be the marginal distribution on the first k coordinates. Thus

$P_k(A_1, ..., A_n) = P(A_1, ..., A_k, X, ..., X)$

**Theorem:** Let X be a set and P an exchangeable probability on sequences of length n. Then there exists a probability Î¼ on P such that for all $k \leq n$ and any set A,

$|P_k(A) - P_{\mu k}(A)| \leq k(k-1)/2n$

Thus, if n is large with respect to $k^2$, the first k coordinates of an exchangeable probability are uniformly well approximated by a mixture of independent identically distributed probabilities. De Finetti's theorem holds approximately! The result is often used in reverse: suppose P is an exchangeable probability on sequences of length k that can be extended to an exchangeable probability on sequences of length n (we can imagine getting more, similar data). Then P is almost a mixture of independent identically distributed probabilities.

The proof is easy and instinctive. Recall the multinomial and hypergeometric probabilities $M_u$ and $H_u$ generated by drawing with or without replacement from an urn u containing n balls labeled with various elements of X (repeats allowed). For $1 \leq k \leq n$, let $M_{uk}$, $H_{uk}$ be the probabilities induced on sequences of length k.

**Lemma:** For any set A and any urn u,  
$|M_{uk}(A) - H_{uk}(A)| \leq k(k-1)/2n$

**Proof:** It is without loss of generality to take $X = {1, 2, ..., n}$ and $u = {1, 2, ..., n}$. Then for any sequence $x = (x_1, ..., x_k)$ in X,


$$M_{uk}(x) = \frac{1}{n^k}$$
$$H_{uk}(x) = \begin{cases}  
\frac{1}{n(n-1)\cdots(n-k+1)} & \text{if all }x_i\text{ distinct} \  
0 & \text{otherwise}  
\end{cases}$$

It follows that the worst case for A is $A = {x: \text{all }x_i\text{ distinct}}$. Since $H_{uk}(A) = 1$ and $M_{uk}(A) = n(n-1)...(n-k+1)/n^k$. For this A, direct computation shows

$|M_{uk}(A) - H_{uk}(A)| = 1 - n(n-1)\cdots(n-k+1)/n^k$

Using $(1-x)(1-y) = 1-x-y+xy \geq 1-x-y$ (if x,y > 0), we see

$n(n-1)...(n-k+1)/n^k = (1-1/n)(1-2/n)\cdots(1-(k-1)/n) \geq 1-(1+\cdots+(k-1))/n = 1-k(k-1)/2n$

Thus, for any A,  
$|M_{uk}(A) - H_{uk}(A)| \leq k(k-1)/2n$ QED

**Remark:** Note that the preceding calculations simply involve calculating the chance that a sample of size k from {1, 2, ..., n} with replacement yields all distinct elements. This is just the birthday problem of chapter 1. It is straightforward to show that for the chosen A, $|M_{uk}(A) - H_{uk}(A)| \geq 1-e^{-k(k-1)/2n}$. Thus our analysis is sharp; $k^2/n$ must be small to have the two distributions close.

To conclude, let P be an exchangeable probability on sequences of length n. We saw that P can be exactly represented as a mixture of urn measures $H_u$. The mixing measure Î¼ is simply induced by P: pick from P, what's the chance that the induced sample gives the urn u? Now,

$|P_k(A) - M_{\mu k}(A)| = |\int H_{uk}(A) \mu(du) - \int M_{uk}(A) \mu(du)| \leq \int |H_{uk}(A) - M_{uk}(A)| \mu(du) \leq k(k-1)/2n$

This concludes the proof of the general de Finetti theorem.
 
 [[ðŸ“œMeehl67_theory-test_ðŸ”´vsðŸ’œ_ method_paradox]]``