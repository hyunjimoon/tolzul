2025-02-06
Below is a three-paragraph summary of **Chapter 13: Models With Memory** (multilevel or hierarchical Bayesian models), followed by a comparative table that addresses how **Theory-Based Entrepreneurial Search** (which benefits from hierarchical Bayesian thinking) contrasts with **Practice-Based Entrepreneurial Search** in key dimensions.

---

[[ğŸ“œchavda24_theoent]]
## **Three-Paragraph Summary of #13 Models With Memory**

**Paragraph 1.** Conventional statistical models often suffer from â€œanterograde amnesia,â€ meaning they treat each category or group in the data as if it has no relationship to any other. When modeling outcomes across multiple clustersâ€”whether they are individuals, locations, or time periodsâ€”older approaches use either a single pooled average for all clusters or assign each cluster its own completely unpooled parameter. Both extremes can yield poor inference: pooling everything together can underfit by ignoring important differences, while treating every cluster in isolation can overfit and â€œforgetâ€ that clusters often share similarities. Chapter 13 introduces **multilevel (hierarchical) Bayesian models** to address this shortcoming, showing how they â€œrememberâ€ information from all clusters and pool that information adaptively.

**Paragraph 2.** The central idea is that each group-level parameter (like a cafÃ©â€™s average waiting time, or an entrepreneurâ€™s theory-based learning rate) is drawn from a population distribution that itself is estimated from the data. This allows **partial pooling**, shrinking extreme or data-scarce groups toward the overall mean, while leaving well-supported (data-rich) groups closer to their own local estimates. A classic illustration is the â€œcoffee robotâ€: if it observes waiting times in one cafÃ©, it updates its population-level beliefs about cafÃ©s in general. When it visits a new cafÃ©, it starts from that overall beliefâ€”rather than forgetting what it learnedâ€”yet it still recognizes that each cafÃ© has unique features. This â€œmemoryâ€ boosts accuracy and guards against under- or overfitting.

**Paragraph 3.** Applied to **entrepreneurial search**, a hierarchical Bayesian model makes it possible for an entrepreneur to learn from multiple ventures, teams, or market niches simultaneously. Some ventures have scant data (small samples) and should borrow strength from the others, whereas well-tested ventures (large samples) remain closer to their own evidence. The model â€œremembersâ€ the variation among clusters through a learned variance term, so it neither forces everyone into a single mean nor splits them into unrelated points. Overall, **Models With Memory** emphasize how such multilevel approaches harness the entire data set to inform each subgroup, yielding more robust and balanced inference for complex, structured problems.


2025-02-04
## exchangeability

compilation of exchangeability from [[eg(use(exbl))]],

- The point isnâ€™t to say epistemology trumps reality, but rather that in ignorance of such correlations the most conservative distribution to use is i.i.d. 71 This issue will return in Chapter 10. Furthermore, there is a mathematical result known as de Finettiâ€™s theorem that tells us that values which are exchangeable can be approximated by mixtures of i.i.d. distributions. Colloquially, exchangeable values can be reordered. The practical impact of this is that â€œi.i.d.â€ as an assumption cannot be read too literally, as different process models again correspond to the same statistical model (as argued in Chapter 1). Even furthermore, there are many types of correlation that do little or nothing to the overall shape of a distribution, but only affect the precise sequence in which values appear. For example, pairs of sisters have highly correlated heights. But the overall distribution of female height remains almost perfectly normal. In such cases, i.i.d. remains perfectly useful, despite ignoring the correlations. Consider for example that Markov chain Monte Carlo (Chapter 9) can use highly correlated sequential samples to estimate most any iid distribution we like.
- In a time series, a previous observation becomes a predictor variable for the next observation. So itâ€™s not easy to think of each observation as independent of, or exchangeable with, the others
- The reason to use varying effects is because they provide better inferences. It doesnâ€™t matter how the clusters arise. If the individual units are exchangablethe index values could be reassigned without changing the meaning of the modelâ€”then partial pooling could help.
- essence of the general varying effects strategy: Any batch of parameters with exchangeable index values can and probably should be pooled. Exchangeable just means the index values have no true ordering, because they are arbitrary labels. Thereâ€™s nothing special about intercepts; slopes can also vary by unit in the data, and pooling information among them makes better use of the data. So our coffee robot should be programmed to model both the population of intercepts and the population of slopes. Then it can use pooling for both and squeeze more information out of the data.



----

## learning covariance


ex-aption ()
ìƒì¡´í•œê²Œ ì ì
ex-aption (ì¤„ë¬´ëŠ¬; ì¸ê³¼ (ì‹œê°„ì¶•ì„ byproduct; ë‹¤êµ­ì–´))

ë‹¤êµ­ì–´ë¥¼ í•˜ëŠ”ê²Œ ì˜í–¥ì„ ë¯¸ì¹œë‹¤. theory 
- test two choose one ()
rationality is ex-post, ì‚¬í›„ì ìœ¼ë¡œ êµ¬ì„±ëœ ì§€ì‹ì€ ì“¸ëª¨ê°€ ì—†ë‹¤ (ì¸ê³¼ì ì´ë¼ì„œê°€ ì•„ë‹ˆë¼ ë‹¤ì–‘ì„±ì„ ì„¤ëª…í•˜ê¸° ë•Œë¬¸ì—)

ì •ë³´ì¢…ê²°ìš•êµ¬

ë§ì€ ì •ë³´ë¥¼ ë°›ì•„ë“¤ì´ëŠ”ê²Œ ì•„ë‹ˆë¼ (ë°ì´í„°ê°€ ); t-1 (ë¨¸ë¦¬ê°€ ì•„íŒŒì„œ)

A, B, C, D (ambidexirty; ì¶”ì¶œ (ë°°í„°ë¦¬ ê¸°ìˆ - ì°¨ ì‹œì¥, ì§‘ ë°°í„°ë¦¬ ì‹œì¥ ))
ë³´ê²€ (ëˆ„êµ¬ë‚˜ ë‹¤ ì´ê¸¸ìˆ˜ ìˆë‹¤.) -> ë‹¤ ì˜ í• ìˆ˜ ìˆë‹¤. (10ë°°ëŠ” ì˜ í•˜ëŠ”)

material (core competence) í™•ë³´ ì‹œ (resource-based view; ì–´ë””ë‚˜ í†µí• ìˆ˜ ìˆëŠ” í•µì‹¬ì—­ëŸ‰ - ì‚´ì•„ë‚¨ì„ ìˆ˜ ìˆì„ìˆ˜ ìˆë‹¤.)

powerê°€ ë” ì¤‘ìš”í•¨ - ì„±ìˆ™ë‹¨ê³„ (ê·œëª¨ê°€ ìˆì–´ì•¼í•¨; platform)

ì‚¬í›„ì ìœ¼ë¡œëŠ” ë¹„í•©ë¦¬ì 

actorì˜ ì…ì¥ì—ì„œëŠ” ìˆœì„œê°€ ë°”ë€œ (ê³¼ê±°ì˜ ì„±ê³µìš”ì†Œê°€ í˜„ì¬ì˜ ì‹¤íŒ¨ìš”ì†Œ (ì›”ë§ˆíŠ¸ - cross dockingëŠ” amazon internet based í™•ì¥ê°€ëŠ¥ì„± ë°”ë€œ))

- optimal stopping rule: exchangeability (ìµœì í™”ë˜ëŠ” ì§€ì ; marginal underage = overage cost); exchangeability 
- ê³„ëŸ‰í™” (ë‘ê°€ì§€ ëŒ€ì•ˆ; 3ê°€ì§€ ëŒ€ì•ˆ)
- max(shipment)

[[Aggregate and Marginalize]]

[[ğŸ’ integ(process-product)]]