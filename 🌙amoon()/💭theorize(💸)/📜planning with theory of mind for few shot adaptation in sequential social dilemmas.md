using [contents cld](https://claude.ai/chat/703dddbc-3bd6-4d28-bb12-75a8dd8fdadd)- ask questions here

|Section/Subsection|ğŸ”Research Question|ğŸ§±Literature Brick|ğŸ”‘Key Message|ğŸ“ŠEmpirical Evidence/ğŸ“Mathematical Formalization|
|---|---|---|---|---|
|1. Introduction|How can AI agents rapidly adapt to unseen agents in mixed-motive environments?|â€¢ MARL algorithms for zero-sum games (Vinyals et al., 2019)<br>â€¢ MARL for cooperative environments (Barrett et al., 2011)<br>â€¢ Mixed-motive environment research (Dafoe et al., 2020)<br>â€¢ Sequential social dilemmas work (Leibo et al., 2017)|ğŸ§â€â™€ï¸Mixed-motive environments require balancing short-term interests with long-term rewards while considering trade-offs between ğŸ—ºï¸self-interest and group benefit. Few-shot adaptation is more challenging in these settings than in zero-sum or cooperative environments.|â€¢ Definition of sequential social dilemmas (SSDs)<br>â€¢ Explanation of mixed-motive environments<br>â€¢ Comparison to other MARL domains|
|2. Related Work|What approaches exist for multi-agent learning in social dilemmas and opponent modeling?|â€¢ Intrinsic reward MARL (Hughes et al., 2018; Peysakhovich & Lerer, 2018)<br>â€¢ LOLA (Foerster et al., 2018)<br>â€¢ Opponent modeling (Albrecht & Stone, 2018)<br>â€¢ Theory of Mind models (Baker et al., 2017)<br>â€¢ MCTS planning (Silver et al., 2018)|ğŸ§ Current approaches either: (1) rely on hand-crafted intrinsic rewards, (2) need access to opponent parameters, or (3) suffer from computational complexity issues. ğŸ§­A hierarchical approach combining opponent modeling with planning could address these limitations.|â€¢ Review of intrinsic rewards for social dilemmas<br>â€¢ Analysis of computational limitations in I-POMDP<br>â€¢ Comparison of Bayesian vs. neural ToM approaches|
|3. Problem Formulation|How can we formally represent multi-agent hierarchical decision-making in SSDs?|â€¢ Markov games (Liu et al., 2022)<br>â€¢ Goal-directed agent models<br>â€¢ Belief representation frameworks|ğŸŒSSDs can be formalized as Markov games with goals, where agents maintain beliefs over others' goals and must make decisions under uncertainty about these goals.|ğŸ“Formal definition of:<br>â€¢ Markov game with goals <N, S, A, T, R, Î³, Tmax, G><br>â€¢ Agent policies Ï€i: S Ã— Ai â†’ [0, 1]<br>â€¢ Goal beliefs bij: Gj â†’ [0, 1]|
|4. Methodology|How can we implement hierarchical goal reasoning and planning for few-shot adaptation?|â€¢ Cognitive psychology on hierarchical reasoning (Butz & Kutter, 2016)<br>â€¢ Goal inference (Gergely et al., 1995)<br>â€¢ Monte Carlo Tree Search (Silver et al., 2018)|ğŸ§­PToM uses a hierarchical approach with two modules: (1) an opponent modeling module that infers others' goals and goal-conditioned policies, and (2) a planning module using MCTS to determine best responses based on inferred opponent behavior.|â€¢ Fig 1: PToM architecture diagram<br>â€¢ Pseudo-code (in appendix)<br>ğŸ“Equations for:<br>â€¢ Intra-ToM and Inter-ToM belief updates<br>â€¢ Goal-conditioned policy learning<br>â€¢ MCTS planning under uncertainty|
|4.1 Opponent Modeling|How can we efficiently infer other agents' goals from their behavior?|â€¢ Bayesian theory of mind (Baker et al., 2017)<br>â€¢ Goal-conditioned policies<br>â€¢ Belief updating mechanisms|ğŸ§ Two complementary belief update mechanisms improve adaptability: (1) intra-ToM for within-episode updates and (2) inter-ToM for between-episode updates. Goal-conditioned policies predict opponent actions given inferred goals.|ğŸ“Formalization of:<br>â€¢ Intra-ToM: b^{K,t+1}_{ij}(g_j) = (1/Z_1)b^{K,t}_{ij}(g_j)Pr_i(a^{K,t}_j\|s^{K,0:t}, g_j)<br>â€¢ Inter-ToM: b^{K,0}_{ij}(g_j) = (1/Z_2)[Î±b^{K-1,0}_{ij}(g_j) + (1-Î±)1(g^{K-1}_j = g_j)]<br>â€¢ Goal-conditioned policy training|
|4.2 Planning|How can we plan effective actions given uncertain opponent models?|â€¢ MCTS (Browne et al., 2012)<br>â€¢ Planning under uncertainty<br>â€¢ AlphaZero (Silver et al., 2018)|ğŸ—ºï¸PToM samples opponent goal combinations from current beliefs and runs MCTS for each sample to estimate action values. Final actions maximize average return across sampled configurations.|ğŸ“Formalization of:<br>â€¢ Goal combination sampling<br>â€¢ Action value averaging: Q_{avg}(s^{K,t}, a) = âˆ‘_{l=1}^{N_s} Q_l(s^{K,t}, a, g^l_{-i})<br>â€¢ Boltzmann rationality model for action selection<br>â€¢ Neural network training|
|5. Experiments|How effective is PToM for self-play and few-shot adaptation across different SSDs?|â€¢ Classic social dilemma paradigms (stag hunt, snowdrift, prisoner's dilemma)<br>â€¢ Baseline algorithms (LOLA, SI, A3C, PS-A3C)|ğŸ‘“PToM was tested in three representative SSD environments: sequential stag-hunt (SSH), sequential snowdrift (SS), and sequential prisoner's dilemma (SPD), both in self-play and adapting to various opponent types.|â€¢ Fig 2: Overview of the three SSD environments<br>â€¢ Tables 1-2: Self-play and few-shot adaptation performance<br>â€¢ Baseline comparisons including rule-based strategies|
|5.1 Experimental Setup|How can we design environments that capture key aspects of different social dilemmas?|â€¢ Matrix-form social dilemmas (Rousseau, 1999; Rapoport & Chammah, 1966)<br>â€¢ Sequential extensions of classic dilemmas|ğŸŒThree environments designed to capture essential dilemmas: (1) SSH - tension between maximizing benefit and minimizing risk, (2) SS - temptation to free-ride vs. group benefit, (3) SPD - short-term self-interest vs. long-term sustainability.|â€¢ Detailed description of three environments<br>â€¢ Fig 2: Visual representation of environments<br>â€¢ Definition of agent actions, rewards, and dilemma structures|
|5.2 Performance|How well does PToM perform in self-play and adapt to unseen agents compared to baselines?|â€¢ Self-play evaluation methods<br>â€¢ Few-shot adaptation metrics<br>â€¢ Baseline MARL algorithms|ğŸ¤œPToM demonstrates superior self-play performance and few-shot adaptation ability across all three environments, particularly excelling in complex mixed-motive scenarios. Belief updating mechanisms are key to adapting to different opponent behaviors.|â€¢ Table 1: Self-play performance<br>â€¢ Table 2: Few-shot adaptation performance<br>â€¢ Analysis of how intra-ToM and inter-ToM contribute to adaptation<br>â€¢ Observation of emerging social intelligence|
|6. Conclusion|What are the implications and limitations of the proposed approach?|â€¢ Limitations of current approach<br>â€¢ Future directions for AI in mixed-motive environments|ğŸ§ PToM provides a principled approach for few-shot adaptation in SSDs by combining theory of mind with planning. Limitations include needing predefined goals and potential misalignment with human values in certain scenarios.|â€¢ Summary of key results<br>â€¢ Discussion of limitations<br>â€¢ Potential applications and future work directions|

This table summarizes the key components of the paper, highlighting how PToM combines hierarchical goal inference with Monte Carlo planning to enable effective few-shot adaptation in sequential social dilemmas. The approach demonstrates how theory of mind capabilities can be formally implemented to improve AI agents' abilities to rapidly adapt to previously unseen agents in mixed-motive environments.