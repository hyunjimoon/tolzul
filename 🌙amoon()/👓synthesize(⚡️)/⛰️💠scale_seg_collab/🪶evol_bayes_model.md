###  üö®todo
 - üö®todo4: 
	 1. explain the table below in one paragraph.
	 2. make sure to specify "This rationalizes organization's operational pivots on how many learning "event" happens before earning event and we argue ratio of earning to learning time costs fundamentally determines this behavior. When learning time costs are reduced‚Äîsuch as 1) by moving testing facilities closer to operational hubs or 2) standardizing testing processes‚Äîorganizations are recommended to learn more to maximize reward flow. Conversely, when earning time costs are increased ‚Äîsuch as when 1) implementing innovations requires major production changes or external supplier adjustments or 2) organization decision structure becomes rigid‚Äî dedicating more on learning event is beneficial to avoid costly mistakes." from ### 1. model abstract.
	 3. based on 1, fill in "**Outside Tesla:** üö®" from üî∫ Earning Time Cost ‚Üë row of table.
 - üö®todo5: 
		1.  understand two tables in ## 4. extending to two dimensions section which compares exploiting vs exploring learning choices in market and product domain. make sure you understand the mechanism behind tesla and rapidSoS.
		2. make sure you have one paragraph each for each table. make sure this table conveys how exploitive vs explorative learning in product choice and market choice are exemplified.
		3. fill in your expectation of reward in four different situations. i denoted it as üö® (customer willingness to pay * total addressable market), given the score is from 1 to 5. be sure to remember exploitive has larger mean but smaller standard deviation compared to explorative.

### 1. model abstract

V1 (prescribing operational pivot): Prior research on experimentation strategies has focused on maximizing reward per decision, overlooking the critical dimension of time. Additionally, the bandit literature assumes learning and earning occur simultaneously, contrary to computational rationality theory which suggests organizations face cognitive constraints forcing an earn-or-learn tradeoff.  By analyzing reward flow, defined as reward per decision event divided by the event's time cost, we treat learning (sampling, testing (JB, Charlie), experimenting) and earning (acting, implementing (JB, Charlie), deciding) as separate events. This research suggests how organization with certain learning to earning ratio (e.g. 2:1 for learn-learn-earn (LLE)) adapts to evolving world state, by adjusting its decision cycle (e.g. from LLE to LELE or LLLE).  We first show how optimal learning per earning is proportional to the ratio of earning to learning time costs. This rationalizes organization's operational pivots on how many learning "event" happens before earning event and we argue ratio of earning to learning time costs fundamentally determines this behavior. When learning time costs are reduced‚Äîsuch as 1) by moving testing facilities closer to operational hubs or 2) standardizing testing processes‚Äîorganizations are recommended to learn more to maximize reward flow. Conversely, when earning time costs are increased ‚Äîsuch as when 1) implementing innovations requires major production changes or external supplier adjustments or 2) organization decision structure becomes rigid‚Äî dedicating more on learning event is beneficial to avoid costly mistakes. 

V2 (explaining behaviors on heterogeneous event ratio (learning per earning), resolving contradictory findings): Prior research on experimentation strategies has focused on maximizing reward per decision, overlooking the critical dimension of time. Additionally, the bandit literature assumes learning and earning occur simultaneously, contrary to computational rationality theory which suggests organizations face cognitive constraints forcing an earn-or-learn tradeoff. By analyzing reward flow, defined as reward per decision event divided by the event's time cost, we treat learning (sampling, testing, experimenting) and earning (acting, implementing, deciding) as separate events. We show how the ratio of earning to learning time costs fundamentally shapes optimal learning strategies. When earning time costs are high relative to learning time costs‚Äîsuch as when implementing innovations requires lengthy production changes‚Äîorganizations benefit from dedicating more periods to learning upfront, increasing the number of learning event per earning event to avoid costly errors. Conversely, when earning is relatively quick and inexpensive event compared to learning, alternating between learning and earning maximizing reward flow. This temporal perspective reconciles seemingly contradictory findings: in environments with low earning time costs, frequent alternation between learning and earning is optimal, while in environments with high earning time costs, extensive upfront learning is preferred. By explicitly modeling learning and earning as distinct components and introducing their time cost, this framework introduces a computationally realistic approach to reward-driven learning and earning, addressing the limitations of bandit models that oversimplify the temporal dynamics of organizational decision-making.

### 2. definition and model

The üß†learning time-cost represents the time resource required for experimentation and learning. When Tesla's engineers evaluate battery specifications, interview potential suppliers, conduct engineering assessments, or test prototypes, they are incurring learning costs. The efficiency of these learning activities determines how quickly Tesla can gather meaningful information before making implementation decisions. 

The üìçearning time-cost captures the time resource needed for implementation and execution. When Tesla commits to a specific battery design, they must invest substantial time in building supply chain relationships, implementing engineering characteristics, adjusting form factors, integrating cooling systems, and setting up production lines. 

These execution activities represent long-term commitments that are often difficult to reverse. Reward represents the degree of success in achieving goals. We use 1 to 5 scoring system with 5 being higher reward than 1. Scores like 445 represent satisfaction ratings from multiple customers/trials - e.g., 4,4,5 means three customers gave ratings of 4,4,5 out of 5. For Tesla's battery situation, reward measures both technical performance (meeting specifications) and market performance (customer willingness to pay). When batteries meet design specs and customers validate this through purchases at target price points, the reward is high.

The ratio of earning time-cost to learning time-cost (r) serves as a critical decision metric for Tesla's battery development strategy. When this ratio is high, indicating that implementation requires substantially more time resources than testing, Tesla benefits from conducting more thorough evaluations before commitment. Conversely, when the ratio is low, suggesting that implementation can be accomplished relatively quickly compared to testing, a faster implementation approach with fewer testing iterations maximizes reward flow. The model optimizes decision-making by balancing reward per unit time instead of just reward per decision. For each decision, $\color{orange}{k}$ exchangeable learnings events happens before decision is made (action) to earn reward. The quality of our decision improves with more learnings but faces diminishing returns, captured by the decision quality function $Q(\color{orange}{k}, \color{skyblue}{p}\color{white}{)} = \color{skyblue}{p} \color{white}{\cdot (1 - B(}\color{orange}{\frac{k}{2}}, \color{orange}{k}, \color{skyblue}{p} \color{white}{))} + (1-\color{skyblue}{p}\color{white}{)} \color{white}{\cdot B(}\color{orange}{\frac{k}{2}}, \color{orange}{k}, \color{skyblue}{p} \color{white}{)}$, where $\color{skyblue}{p}$ represents the true probability of success and $B$ is the binomial CDF. The total time cost $T$ combines both earning time and learning time: $T(\color{orange}{k}, \color{green}{r}\color{white}{)} = \color{green}{r} \color{white}{+} \color{orange}{k}$, where $\color{green}{r}$ represents the ratio of earning time-cost to learning time-cost. Unit of T is event (not time), meaning r is (earning time cost/learning time cost) x (one earning event) and k is (k learning events). The optimal strategy maximizes the reward rate $R(\color{orange}{k}, \color{green}{r}\color{white}{)} = \frac{Q(\color{orange}{k}, \color{skyblue}{p}\color{white}{)}}{T(\color{orange}{k}, \color{green}{r}\color{white}{)}}$. The decision-making process balances reward per earning decision event against time costs of event. Like the sampling model in [[üìúVul14_onedone]], each learning event provides information but faces diminishing returns. The quality function $Q(\color{orange}{k}, \color{skyblue}{p}\color{white}{)}$ resembles the Bayesian posterior estimation process, where more samples (learning events) improve decision quality but with decreasing marginal benefit. The time cost $T(\color{orange}{k}, \color{green}{r}\color{white}{)}$ reflects both learning and earning event sequences, similar to how sampling and action costs are balanced in [[üìúVul14_onedone]]'s decision-making framework. 

Difference between decision quality $Q(\color{orange}{k}, \color{skyblue}{p}\color{white}{)}$ and reward rate $R(\color{orange}{k}, \color{green}{r}\color{white}{)}$ is, learning-earning-learning-earning (LELE) can have higher reward rate compared to that of learning-learning-learning-earning (LLLE), even though its decision quality is lower. Assuming earning and learning takes equal amount of time, LLLE's reward per earning event is lower (2 vs 3), but as two earning event can happen in the same duration of time, it claims victory in reward flow realm.

| Variable                                                  | Definition (unit)                                   | Tesla Example                                                                                                                                       |
| --------------------------------------------------------- | --------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |
| $\color{orange}{k}$                                       | learnings per decision                              | Number of battery designs/specifications tested before committing to production                                                                     |
| $\color{skyblue}{p}$                                      | True success probability                            | Underlying probability that a given battery design will meet performance requirements                                                               |
| $\color{green}{r}$                                        | earning/learning time cost ratio                    | Time needed to retool production line and establish supplier relationships divided by time needed to test one battery design (request for proposal) |
| $Q(\color{orange}{k}, \color{skyblue}{p}\color{white}{)}$ | Decision quality (reward / earning event)           | How well the chosen battery design performs in actual production/market                                                                             |
| $T(\color{orange}{k}, \color{green}{r}\color{white}{)}$   | Total time cost (learning and earning event / time) | Total time from start of testing to full production implementation                                                                                  |
| $R(\color{orange}{k}, \color{green}{r}\color{white}{)}$   | reward rate (reward / time)                         |                                                                                                                                                     |

## 3. tesla example

![[Pasted image 20241121142032.png|500]]

| **Phase 1**                    | **Phase 2**                        | **Phase 3**                           |                                      |
| ------------------------------ | ---------------------------------- | ------------------------------------- | ------------------------------------ |
| **üß† testing Time Costs**      | 8 weeks (distributed testing)      | 2 weeks (integrated facility testing) | 2 weeks (focused on 4680 refinement) |
| **üìç implementing Time Costs** | 4 weeks (outsourced manufacturing) | 10 weeks (early in-house production)  | 72 weeks (ramp-up for 4680 cells)    |
| **Key Transition**             | Basic outsourced production        | Integrated manufacturing              | Retooling for 4680 cell production   |

- Phase1-2: **Learning Time Costs Reduction:** The reduction in üß† learning time costs between Phase 1 (8 weeks) and Phase 2 (2 weeks) demonstrates the benefit of moving testing facilities closer to operational hubs (Fremont) and standardizing processes for rapid iterations. This aligns with the claim that organizations should increase learning when learning time costs are reduced to ensure efficient decision-making.

- Phase2-3: **Earning Time Costs Increase:** The sharp rise in üìç earning time costs from 10 weeks in Phase 2 to 72 weeks in Phase 3 (to implement the 4680 cells) reflects the increased complexity and retooling required for innovation. This supports the idea that dedicating more to learning events is necessary when earning time costs rise, as mistakes at this stage could be extremely costly.

Tesla's supply chain evolution demonstrates two distinct shifts in learning-to-earning time ratios. For the Roadster, initial learning cycles took 8 weeks due to distributed testing across three continents, while earning (implementation) time was relatively quick at 4 weeks since changes only required coordinating with outsourced suppliers. Tesla pivoted to an integrated Fremont facility, cutting learning time to 2 weeks while accepting longer 10-week implementation cycles - recognizing rapid learning was crucial for developing a novel product. Later, when developing 4680 cells, Tesla faced much higher earning costs of 18 months for production changes due to complex retooling requirements and supplier adjustments. Given these extended implementation timelines, Tesla adapted by conducting more thorough upfront testing and validation to minimize costly implementation mistakes. These examples show how Tesla optimized its development approach based on the ratio between learning and earning time costs - first by reducing learning time through integration, then by increasing learning iterations when faced with high earning costs.

[Roadster to Model S]
The Tesla Roadster was Tesla‚Äôs Ô¨Årst car sold to the public. The company designed an initial supply chain that spanned three continents and resulted in very long prototyping cycles. In that initial model, the design and engineering of the key electronics and battery modules were performed in California, along with the Ô¨Ånal vehicle test and tuning. The manufacturing and supply chain team lacked professionals from the automotive industry, and with a focus on low labor costs outsourced the manufacture of key modules to multiple sites in Asia. Further, due to capability requirements, vehicle assembly was located in Europe. The footprint of this outsourcing model yielded very long design-manufacturetest cycles‚Äîfrom California to Asia to Europe and back to California. Such long debugging cycles, especially for a Ô¨Årst-of-its-kind product, were not sustainable, and the company went through a major re-capitalization and a radical organizational change to restructure and redesign its operations toward more a more insourced and geographically compact manufacturing footprint, which enabled it to then debug and deliver the Roadster vehicles. The initial concept of minimizing costs by outsourcing manufacturing to low-cost geographies was supplanted by the insight that supply chain speed can often save more money than low-cost labor. The extreme operations pivot that Tesla was forced to undergo under duress, is often not possible for a company that does not have backers with deep pockets. The lesson that Tesla drew from the Roadster experience was how short supply chains can increase development speed, which encouraged it to invest in a large, much more integrated facility (in Fremont California) for its next product generation, the Model S.

üö®todo4: 
1. explain the table below in one paragraph.
2. make sure to specify "This rationalizes organization's operational pivots on how many learning "event" happens before earning event and we argue ratio of earning to learning time costs fundamentally determines this behavior. When learning time costs are reduced‚Äîsuch as 1) by moving testing facilities closer to operational hubs or 2) standardizing testing processes‚Äîorganizations are recommended to learn more to maximize reward flow. Conversely, when earning time costs are increased ‚Äîsuch as when 1) implementing innovations requires major production changes or external supplier adjustments or 2) organization decision structure becomes rigid‚Äî dedicating more on learning event is beneficial to avoid costly mistakes." from ### 1. model abstract.
3. based on 1, fill in "**Outside Tesla:** üö®" from üî∫ Earning Time Cost ‚Üë row of table.

| Cause                                                                        | Factor & Tesla Battery Example                                                                                                                                                                                                                                                                                                      |
| ---------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| üîª Learning Time Cost ‚Üì<br>*Making it easier to learn/test*                  | **Inside Tesla:** Engineers got really good at battery testing through experience (professionalize)<br>**Outside Tesla:** Moving testing facilities closer to main office<br>**Example:** When Tesla moved testing from Thailand to California, they could test new battery designs in days instead of waiting 8 weeks for shipping |
| üî∫ Earning Time Cost ‚Üë<br>*Making actual implementation more time-consuming* | **Inside Tesla:** New battery designs required completely changing how cars were built<br>**Outside Tesla:** üö®<br>**Example:** When Tesla introduced their new 4680 battery, it took 18 months to set up production because both Tesla and suppliers needed major changes                                                          |
[[üìúMokyr92_evoldyn_tech]]
## 4. extending to two dimensions

üö®todo5: 
	1.  understand two tables in ## 4. extending to two dimensions section which compares exploiting vs exploring learning choices in market and product domain. make sure you understand the mechanism behind tesla and rapidSoS.
	2. make sure you have one paragraph each for each table. make sure this table conveyes how exploitive vs explorative learning in product choice and market choice are exemplified.
	3. fill in your expectation of reward in four different situations. i denoted it as üö® (customer willingness to pay * total addressable market), given the score is from 1 to 5. be sure to remember exploitive has larger mean but smaller standard deviation compared to explorative.

|                                             | Exploitive learning in Market choice                              | Explorative learning  in Market choice                       |
| ------------------------------------------- | ----------------------------------------------------------------- | ------------------------------------------------------------ |
| Exploitive learning in Product choice<br>   | California customers<br>Japanese batteries<br>Proven supply chain | Texas customers<br>Japanese batteries<br>Proven supply chain |
| higher mean & lower variance reward         | 445                                                               | üö®                                                           |
| Explorative learning  in Product choice<br> | California customers<br>US-made batteries<br>New supply chain     | Texas customers<br>US-made batteries<br>New supply chain     |
| lower mean & higher variance reward         | üö®                                                                | 135                                                          |

Suppose we're selling Japanese batteries to Texas, and Texas only wants Made in America, right? Yeah, yeah. Right Track is yeah, so. So we expect this is a really tough test. It. It's going to have to be extremely high performing to give satisfaction to these Texans for them to accept. I'm going to take these foreigner batteries in my car. Is that right? Do I have the right intuition? Yes, yes. So I'm so I'm I'm creating a very difficult test now. But why is it one? Okay, so it's three customers, yeah, three Texans gave us a one, one. She gave us a three one. Gave us a phone. Yeah. And so there are two dimensions here. One is the customer expectations. Are they Texans or are they Californians, right? Yeah. What's the other and the other dimension is the product? Is it a Japanese car, American, Made in America, made in Japan or something?

- Top Left = Easiest path (known Japanese tech + accepting market)
- Bottom Right = Most challenging path (new US tech + demanding market)
- Bottom Left  = Mixed (new US battery + accepting market)
- Top Right = Mixed (known Japanese battery + demanding market)

The reward (445, 222, 135, 345) represent customer satisfaction ratings from three different customers on a 1-5 scale, where higher numbers indicate greater satisfaction.

|                                             | Exploitive learning in Market choice                                                | Explorative learning  in Market choice                                                                                   |
| ------------------------------------------- | ----------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| Exploitive learning in Product  choice<br>  | Basic consumer app<br>Regular mobile network<br>Location sharing app                | For medical/NGOs/non-profit focus<br>Basic emergency tracking (e.g. Epilepsy)                                            |
| higher mean & lower variance reward         | 445                                                                                 | üö®                                                                                                                       |
| Explorative learning  in Product choice<br> | Full emergency response integration<br>911 infrastructure<br>24/7 emergency support | National 911 integration<br>For emergency services<br>National 911 system integrating iOS + Android + emergency services |
| lower mean & higher variance reward         | üö®                                                                                  | 135                                                                                                                      |
- Top Left =  Easiest path (Simple consumer app)
- Bottom Right = Most challenging path  (Full emergency response system, what they became)
- Bottom Left  = Mixed ( Basic medical/NGO tracking (what they tested; "If they don't like it, we should quit"))
- Top Right = Mixed (Basic medical/NGO tracking (what they tested))

RapidSOS chose to test with high bar market (medical/NGO) first using relatively low bar operations, then built up to high bar operations to serve the entire emergency response market.

---
## future work
- "The concept of exaptation (the repurposing of existing resources for new uses), offers two key strategies for structured experimentation."
- explain the difference with test 2 choose 1 which is sequential learning i.e. second test builds on the first test. but we are multiverse situation where each learnings are exchangeable. increased learning increase precision of world representation of the world i.e. P(State|Data). One earning decision is made based in majority voting way by choosing action that gained more than k/2 votes among k multi-verse.
- word choice between "reward" flow VS "utility" flow
- analyze variance?

| üîª learning Cost Variance (œÉ¬≤‚Çõ‚Üì)<br>*Making learning time more predictable*       | **Inside Tesla:** Created standard testing procedures (processify)<br>**Outside Tesla:** Set up reliable testing labs<br>**Example:** At first, testing time varied between 1-4 weeks. After creating standard procedures, it consistently took about a week                                               |
| ------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| üî∫ earning Cost Variance (œÉ¬≤‚Çê‚Üë)<br>*Making implementation time less predictable* | **Inside Tesla:** Some changes needed just software updates, others needed complete car redesign<br>**Outside Tesla:** Suppliers took unpredictable time to get ready<br>**Example:** Early Roadster battery implementation could take anywhere from 6-18 months depending on how complex the changes were |
