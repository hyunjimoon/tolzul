We have embraced a **test-and-learn framework** as the engine of our product development and business evolution. Rather than planning in large, inflexible cycles, we operate in iterative loops where each iteration is a chance to test a hypothesis and learn from it. This framework is influenced by Lean Startup principles and our own Bayesian twist on things. Here’s how we execute our test-and-learn cycle:

1. **Formulate a Hypothesis:** We start each iteration by clearly stating what we want to learn. This could be _“Offering a 10% discount will increase user sign-ups by at least 20%”_ or _“Adding a route recommendation feature will improve rider retention.”_ These hypotheses often come from our prior beliefs or from patterns we think we see in the data. Everyone on the team knows what we’re trying to validate or invalidate in that cycle.
    
2. **Design the Experiment:** We then design a focused experiment to test the hypothesis. This could be a product A/B test, a small-scale pilot, a user survey, or even a paper prototype – whatever method is appropriate to get data quickly and ethically. Key here is that we define **metrics or criteria** in advance. For example, _if sign-ups increase by >15%, we consider the hypothesis supported_. This pre-definition prevents us from moving goalposts later (a common bias trap). We leverage our [[Decomposing Experiments]] approach to keep tests bite-sized and targeted.
    
3. **Execute and Monitor:** We run the experiment, often in a rapid timeframe (days or weeks). During execution, we monitor real-time where possible. If it’s a longer running test and we start seeing unexpected trends, our adaptive ethos (from [[Adaptive Bayesian Experimentation]]) allows us to tweak or even pause for a quick mid-course correction. For instance, if a test feature is broken or users are confused, we don’t just let it run to completion; we intervene to fix obvious errors so that the test remains valid.
    
4. **Analyze Results and Learn:** Once the experiment concludes or yields sufficient data, we analyze the results rigorously. Did the metric move as expected? We don’t just look at the primary metric; we also check for side effects (maybe sign-ups increased 20% but uninstalls also spiked – why?). This is where our emphasis on [[Rational Meaning Construction]] comes in. We gather the team, sometimes including mentors like [[Moshe]], and interpret the findings: What narrative do the data tell? Do they confirm our prior belief or should we update our model? We document the outcome – whether the hypothesis was supported or not – and more importantly, **what we learned**. For example: “Discounts helped sign-ups, but mostly among students; working professionals were indifferent. So the tactic may only be useful for certain segments.”
    
5. **Decide and Iterate:** Based on the analysis, we make a decision. It could be **to pivot, persevere, or tweak**. If the hypothesis was supported, we might roll out the change more broadly (or form a new hypothesis building on that success). If it wasn’t, we discuss why and either drop that approach or design a follow-up experiment to dig deeper. The critical part is feeding the learnings back into our strategic roadmap. Every test result updates our understanding (often fed into our probabilistic models, see [[Probabilistic Mental Modeling]]). Then the cycle repeats with a new or refined hypothesis.
    
We often maintain a **backlog of experiments** – a list of ideas to test, which we prioritize by potential impact and uncertainty. This test-and-learn mentality ensures that at any given time, we’re actively reducing uncertainty on the most pressing questions. It also creates a culture where failure is okay as long as it yields insight. By framing efforts as experiments, the team and stakeholders see even “failed” tests as progress (we now know what _doesn’t_ work, which is valuable).

Our test-and-learn framework is inherently collaborative too. We link experiments to owners and stakeholders: for example, a customer-facing test will involve our marketing lead and perhaps a design partner, whereas a backend algorithm test might involve our engineering advisor. Everyone knows the learnings will be shared and used, fostering trust that time spent testing is time well spent.

Finally, this approach resonates with our investors and accelerators. When we update [[MIT delta v Accelerator]] mentors, we can show a timeline of hypotheses tested and insights gained, demonstrating momentum and a learning velocity. It’s far more compelling than a static plan. In summary, **test-and-learn experimentation** is how we systematically navigate from uncertainty to knowledge. It keeps us agile, evidence-driven, and continuously improving, which is essential in the fast-moving realm of mobility tech.