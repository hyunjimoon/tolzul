_(Core Technology 2 of 5 – Learned from API-111 Microeconomic Theory)_

A key principle we’ve adopted is **dynamically adapting our experimentation pace based on what we learn**. In other words, we don’t set a fixed plan of “we will run exactly 10 trials no matter what”; instead, we constantly update our plan as new data comes in – very much like a Bayesian updating process. From microeconomic theory, we learned that when you replace the notion of known probabilities (objective risk) with **subjective beliefs**, the optimal timing and frequency of actions can change. This introduced us to the concept of a _“clock-speed ratio of action to state.”_ In plain terms, if we’re very uncertain about the market (state), we might act more slowly or gather more info before a big decision; if we become more confident (through learning), we can speed up our actions.

Practically, this means our testing is **not a rigid checklist but a responsive journey**. For example, if an early experiment gives a very strong positive signal (say, 80% of participants loved our service), we might accelerate plans – do a larger rollout sooner than scheduled, because our belief in success jumped. Conversely, if we encounter ambiguous or concerning results, we pause and design new micro-experiments to investigate the issue before proceeding. We moved away from any fixed-length pilot mentality to a more fluid approach: experiment until your _belief threshold_ for decision X is met. This concept was a shift from thinking “we’ll test for 3 months then decide” to “we’ll decide as soon as the evidence is compelling (or pivot if it consistently isn’t).”

Our more **optimistic advisors, like [[Charlie]] and [[Scott]], have been strong champions of this adaptive experimentation**. They encourage us to grab opportunities – if initial data is great, double down quickly (why waste time?). This reflects their optimism: they assume positive results are meaningful and want to capitalize on them. Because of their support, we felt confident to, for instance, extend our pilot to a second city earlier than planned when the first city’s metrics exceeded targets. At the same time, being adaptive also satisfies the spirit of our skeptics because it means we **don’t overcommit without evidence** – we are always ready to change course if needed. In fact, even Moshe, who can be skeptical, liked this approach because it’s essentially continuous improvement.

  

From a methodology standpoint, this is Bayesian in nature: we assign a prior belief (e.g., “there’s a 50% chance commuters will adopt our app daily”). We then run an experiment (small trial on campus), update our belief (say it goes to 70% after seeing usage stats), and that updated belief informs the next action (maybe now we’re confident enough to invest in better app features). We treat _every piece of feedback or data as an update_, not just at the end of an experiment but during it as well. In practice, we’ve implemented **sequential experimentation** techniques: A/B tests that can stop early if one variant is clearly better, rolling market releases where we expand week by week if metrics hold, etc. We no longer think in terms of fixed sample sizes or fixed timelines by default. This **adaptive experimentation framework** makes us faster and more efficient in the long run, focusing resources where they matter most and responding to reality in real time. It’s the embodiment of “Bayesian” thinking in action for our startup – always be updating.