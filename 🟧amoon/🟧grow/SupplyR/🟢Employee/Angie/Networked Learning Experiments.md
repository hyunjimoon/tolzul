_(Core Technology 4 of 5 â€“ Learned from 1.200 Transportation Systems)_

In transportation, we learned about **network effects** â€“ how elements in a network (like roads, or transit lines) increase each otherâ€™s value. We took this idea and applied it to our experimentation process, conceiving our tests not as isolated trials but as a **network of learning opportunities**. Concretely, this means every experiment we run is connected to others; the outcome of one can influence or shape the setup of another. We map our experiments like a graph: nodes are individual tests (say, a pricing A/B test, a new feature rollout, a partnership trial) and edges represent informational or contextual links between them (one testâ€™s result informs anotherâ€™s hypothesis, or two tests run in parallel to cover different facets of one question).
 
For example, suppose we test a ride-pooling algorithm in one city and simultaneously test a driver incentive scheme in another. These might seem separate, but theyâ€™re part of the larger question: â€œHow do we improve ride-pooling efficiency and supply?â€ The **networked approach** ensures we consider how the findings connect: maybe driver incentives only work if the pooling algorithm produces enough rides per driver. So we iterate the combination as a system, not just individually optimize each part in a vacuum. The transportation course highlighted that **the value of each route in a transit network isnâ€™t standalone; it depends on how it connects and feeds other routes**. Similarly, the value of each experiment is compounded when viewed in context of others.
 
One payoff of this approach is that we **maximize what we learn from every piece of work**. Even if one experiment â€œfailsâ€ in isolation, it might provide critical insight that makes another experiment succeed. Itâ€™s like portfolio theory for experiments â€“ some may have low direct return, but high informational return for the network. Also, by recognizing interdependence, we avoid redundant tests and can design complementary tests. For instance, rather than run two marketing experiments that teach us the same thing about user acquisition, we ensure each is covering unique ground or one is validating the conditions of the other.

Mentors such as [[ğŸŸ§amoon/ğŸŸ§grow/supplyR 1/Candidate/Jinhua]] and [[ğŸŸ§amoon/ğŸŸ§grow/supplyR 1/ğŸ”´Operational Resource Partner/Charlie]] have been enthusiastic about this network perspective. Their optimism aligns with seeing the big picture â€“ Jinhua often reminds us that a cityâ€™s transport system works as a whole, so he was glad to see us treating our venture tests holistically. [[ğŸŸ§amoon/ğŸŸ§grow/supplyR 1/ğŸ”´Operational Resource Partner/Moshe]] too, despite his cautious nature, supports the idea that understanding system-level effects is important; he just ensures we donâ€™t draw false conclusions from linked experiments without proper analysis.

In practice, implementing networked learning means we do a lot of **mapping and modeling**. We maintain an experiment map: a living document that shows active and planned experiments and how they relate to our key research questions. If we notice too many experiments clustered in one area of the map, we adjust to cover blind spots elsewhere. Conversely, if one critical hypothesis is under-tested, we might run several linked experiments attacking it from different angles. Itâ€™s an iterative web: the failure of one node might redirect the path to a different node.

A concrete example: We ran a small-scale test of a carpool service at a college (node A) and a parallel survey of employers about offering commute benefits (node B). The college test flopped in ridership, which could mean low demand â€“ but the employer survey (node B) revealed interest in sponsoring rides. That insight looped back, and we realized the service might work if marketed as an employer perk. We then tested a modified service with that backing (node C) which succeeded. Traditional siloed testing might have just written off carpooling from the college test failure. Our networked approach salvaged and repurposed the knowledge.

In short, **we treat experiments like interconnected pieces of a puzzle**. This not only accelerates learning (as pieces inform each other), but also acknowledges the reality that elements of our business (product features, user behavior, partner support) are interlinked. By learning in a networked way, we ultimately aim to assemble a cohesive picture of a successful mobility solution that accounts for all these moving parts.