_(Core Technology 2 of 5 â€“ Learned from API-111 Microeconomic Theory)_

A key principle weâ€™ve adopted is **dynamically adapting our experimentation pace based on what we learn**. In other words, we donâ€™t set a fixed plan of â€œwe will run exactly 10 trials no matter whatâ€; instead, we constantly update our plan as new data comes in â€“ very much like a Bayesian updating process. From microeconomic theory, we learned that when you replace the notion of known probabilities (objective risk) with **subjective beliefs**, the optimal timing and frequency of actions can change. This introduced us to the concept of a _â€œclock-speed ratio of action to state.â€_ In plain terms, if weâ€™re very uncertain about the market (state), we might act more slowly or gather more info before a big decision; if we become more confident (through learning), we can speed up our actions.

Practically, this means our testing is **not a rigid checklist but a responsive journey**. For example, if an early experiment gives a very strong positive signal (say, 80% of participants loved our service), we might accelerate plans â€“ do a larger rollout sooner than scheduled, because our belief in success jumped. Conversely, if we encounter ambiguous or concerning results, we pause and design new micro-experiments to investigate the issue before proceeding. We moved away from any fixed-length pilot mentality to a more fluid approach: experiment until your _belief threshold_ for decision X is met. This concept was a shift from thinking â€œweâ€™ll test for 3 months then decideâ€ to â€œweâ€™ll decide as soon as the evidence is compelling (or pivot if it consistently isnâ€™t).â€

Our more **optimistic advisors, like [[ğŸŸ§amoon/ğŸŸ§grow/supplyR 1/ğŸ”´Operational Resource Partner/Charlie]] and [[ğŸŸ§amoon/ğŸŸ§grow/supplyR 1/ğŸ”´Operational Resource Partner/Scott]], have been strong champions of this adaptive experimentation**. They encourage us to grab opportunities â€“ if initial data is great, double down quickly (why waste time?). This reflects their optimism: they assume positive results are meaningful and want to capitalize on them. Because of their support, we felt confident to, for instance, extend our pilot to a second city earlier than planned when the first cityâ€™s metrics exceeded targets. At the same time, being adaptive also satisfies the spirit of our skeptics because it means we **donâ€™t overcommit without evidence** â€“ we are always ready to change course if needed. In fact, even Moshe, who can be skeptical, liked this approach because itâ€™s essentially continuous improvement.

  

From a methodology standpoint, this is Bayesian in nature: we assign a prior belief (e.g., â€œthereâ€™s a 50% chance commuters will adopt our app dailyâ€). We then run an experiment (small trial on campus), update our belief (say it goes to 70% after seeing usage stats), and that updated belief informs the next action (maybe now weâ€™re confident enough to invest in better app features). We treat _every piece of feedback or data as an update_, not just at the end of an experiment but during it as well. In practice, weâ€™ve implemented **sequential experimentation** techniques: A/B tests that can stop early if one variant is clearly better, rolling market releases where we expand week by week if metrics hold, etc. We no longer think in terms of fixed sample sizes or fixed timelines by default. This **adaptive experimentation framework** makes us faster and more efficient in the long run, focusing resources where they matter most and responding to reality in real time. Itâ€™s the embodiment of â€œBayesianâ€ thinking in action for our startup â€“ always be updating.