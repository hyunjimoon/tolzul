Below is a Markdown translation of your LaTeX document. Because there are no LaTeX tables, no conversion to a Markdown table was needed. Everything else has been translated as closely as possible, including the math, figure code (in a fenced code block), references, and footnotes.

---

# Introduction

Simulation-based calibration checking (SBC; [@talts_sbc]) is a method to validate Bayesian computation, extending ideas from [@cook_gelman_rubin].[1](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#user-content-fn-1)  
While SBC is primarily intended for validating sampling algorithms such as MCMC, it can be used for validating any method implementing or approximating Bayesian inference. Published applications include variational inference [@yao_yes_2018] and neural posterior approximations [@radev2023jana].

Throughout this paper we assume an implicit and fixed Bayesian statistical model π\pi with data space YY and parameter space Θ\Theta. For y∈Y,θ∈Θy \in Y, \theta \in \Theta the model implies the following joint, marginal, and posterior distributions:

```latex
\begin{gather*}
    \pi_\text{joint}(y, \theta) = \pi_\text{obs}(y | \theta) \pi_\text{prior}(\theta)\\
    \pi_\text{marg}\left(y \right) = \int_\Theta \mathrm{d} \theta \: \pi_{\text{obs}}(y | \theta) \pi_\text{prior}(\theta)\\
    \pi_\text{post}(\theta | y) = \frac{\pi_\text{obs}(y | \theta) \pi_\text{prior}(\theta)}{\pi_\text{marg}\left(y \right)}.
\end{gather*}
```

Typically, the posterior distribution πpost\pi_\text{post} is the target of inference but is impossible to evaluate directly. While many computational approaches exist for sampling from the posterior or its approximations, they may fail to provide a correct answer. Problems can arise from errors in how the algorithm or the statistical model are encoded or from inherent inability of the computational method to correctly handle a given model with a given dataset.

## Self-consistency of Bayesian models

To discover problems with computation, several classes of checks can be derived from self-consistency properties of statistical models. One such property concerns the data-averaged posterior [@geweke_getting_2004]:

```latex
\begin{equation}
\pi_\text{prior}(\theta) = \int_Y \mathrm{d} y \int_\Theta \mathrm{d}\tilde\theta  \: \pi_\text{post}(\theta |y) \pi_\text{obs}(y | \tilde\theta) \pi_\text{prior}(\tilde \theta).  
\label{eq:data_averaged_posterior_introduction} 
\end{equation}
```

SBC relies on a different property that involves the joint distribution of prior and posterior samples from the same model [@cook_gelman_rubin]:

```latex
\begin{equation}
\pi_\text{SBC}(y, \theta, \tilde\theta) = \pi_\text{prior}(\tilde\theta) \pi_\text{obs}(y | \tilde\theta) \pi_\text{post}(\theta | y).
\label{eq:sbc_joint_distribution}
\end{equation}
```

Since πobs(y∣θ~)πprior(θ~)=πmarg(y)πpost(θ~∣y)\pi_\text{obs}(y | \tilde\theta) \pi_\text{prior}(\tilde\theta) = \pi_\text{marg}\left(y \right)\pi_\text{post}(\tilde\theta | y), this implies,

```latex
\begin{equation}
\pi_\text{SBC}(y, \theta, \tilde\theta) = \pi_\text{marg}(y) \pi_\text{post}(\theta | y) \pi_\text{post}(\tilde\theta | y).      
\label{eq:sbc_joint_2}
\end{equation}
```

Equation ([eq:sbcjoint2eq:sbc_joint_2](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#eq:sbc_joint_2)) immediately shows that conditional on a specific data y∈Yy \in Y, the distributions of θ\theta and θ~\tilde\theta in Equations ([eq:sbcjointdistributioneq:sbc_joint_distribution](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#eq:sbc_joint_distribution)) and ([eq:sbcjoint2eq:sbc_joint_2](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#eq:sbc_joint_2)) are identical. In general, SBC-like checks are sensitive to different deviations from the correct posterior than checks based on the data-averaged posterior (see [Section sec:examplessummarysec:examples_summary](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#sec:examples_summary) for more details). The two families of checks coincide when YY has just a single element as in this case both reduce to directly comparing two distributions.

SBC and related methods employ two different implementations of the same statistical model and check if the results have the same distribution conditional on data. The first step is to define a _generator_ capable of directly simulating draws from πprior(θ~)\pi_\text{prior}(\tilde\theta) and πobs(y∣θ~)\pi_\text{obs}(y | \tilde\theta), and the second step is to define a _probabilistic program_ that, in combination with a given _posterior approximation algorithm_, samples from the posterior distribution πpost(θ∣y)\pi_\text{post}(\theta | y). Each simulation from the generator yields,

```latex
\begin{align}
  \randvar{\tilde\theta} &\sim \pi_\text{prior}(\tilde\theta)    \notag\\
  \randvar{y} &\sim \pi_\text{obs}(y | \randvar{\tilde\theta})    \notag\\
  \theta_1, \dots \theta_M &\sim \pi_\text{post}(\theta | \randvar{y}),
  \label{eq:sbc_setup}
\end{align}
```

where MM is the number of posterior draws sampled. Where confusion is possible we use \randvar\randvar{} to mark a random variable. We run many such simulations and then inspect the realized distributions of θ1,…,θM\theta_1, \ldots, \theta_M and \randvarθ~\randvar{\tilde\theta} conditional on \randvary\randvar{y}. Specific calibration checking methods differ in how exactly they test the conditional equality of the two distributions.

### Proposed SBC variant

SBC has been believed to be insensitive to some classes of mismatches, and as described in [@talts_sbc] would not work for discrete variables. To remove those limitations, we argue for the following variant of the SBC check: First, project the potentially high-dimensional parameter and data space into a scalar _test quantity_ f:Θ×Y→Rf: \Theta \times Y \rightarrow \mathbb{R}. Second, compute the rank of the prior draw in the posterior conditional on yy. Specifically, we take the number of posterior sample draws where the test quantity is lower than in the prior draw, and, if there are any ties, choosing the rank randomly among the tied positions:

```latex
\begin{align*}
   N_{\mathtt{less}} &:= \sum_{m=1}^M \mathbb{I} \left[f(\theta_m, y) < f(\tilde \theta, y) \right] \\
   N_{\mathtt{equals}} &:= \sum_{m=1}^M \mathbb{I} \left[f(\theta_m, y) = f(\tilde \theta, y) \right] \\
   K &\sim \mathrm{uniform}(0,  N_{\mathtt{equals}})\\
   N_\mathtt{total} &:= N_{\mathtt{less}} + K,
\end{align*}  
```

where I[P]\mathbb{I}[P] denotes the indicator function for predicate PP. The procedure simplifies if there are no ties, which will be true for most practical test quantities over models with continuous parameter space. When no ties occur, we have Ntotal=NlessN_\mathtt{total} = N_{\mathtt{less}}. Then, if the probabilistic program and the generator implement the same probabilistic model, we have

```latex
\begin{equation}
    N_{\mathtt{total}} \sim \mathrm{uniform}(0, M).    
    \label{eq:sbc_sample}
\end{equation}
```

See Theorems 3 and 4 for a formal statement and proof. As a result, once we obtain a set of draws from empirical distribution of NtotalN_{\mathtt{total}} via multiple simulations, we can perform a test for uniformity. The process is then repeated for all test quantities we want to consider. If we are using MCMC to sample from πpost\pi_\text{post}, the posterior sample typically needs to be thinned to ensure that θ1,…,θM\theta_1, \dots, \theta_M are approximately independent [@talts_sbc; @sailynoja_graphical_2021]. The overall SBC process is illustrated in Figure [fig:sbcschemafig:sbc_schema](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#fig:sbc_schema).

```latex
\begin{figure}
    \centering
    \usetikzlibrary{positioning}


    \definecolor{priorcolor2}{HTML}{DBEEF3}
    \definecolor{priorcolor1}{HTML}{31859B}
    \definecolor{programcolor2}{HTML}{F2DCDB}
    \definecolor{programcolor1}{HTML}{953734}
    \definecolor{rankcolor2}{HTML}{FDEADA}
    \definecolor{rankcolor1}{HTML}{E36C09}
    \definecolor{unifcolor2}{HTML}{EBF1DD}
    \definecolor{unifcolor1}{HTML}{76923C}
    \resizebox{\textwidth}{!}{%
    \begin{tikzpicture}
    
    \tikzstyle{invisible} = [outer sep=0,inner sep=0,minimum size=0]
    
    % Groupings
    \filldraw [color=priorcolor1,fill=priorcolor2] (0.55,3.1) rectangle (2.45,-2.9);
    \filldraw [color=programcolor1, fill=programcolor2] (3,3.1) rectangle (5.2,-2.9);
    \filldraw [color=rankcolor1, fill=rankcolor2] (5.7,3.1) rectangle (11.1,-2.9);
    \node at (1.45,3.6) {Generator};
    \node at (4,3.6) {\begin{tabular}{c} Probabilistic \\ program \end{tabular}};
    \node at (8.2,3.6) {Test quantity};
    
    
    
    %Nodes
    \node[invisible] (prioranchor) {};
    \node (priordraw2) [right = 5mm of prioranchor] {$\tilde\theta^{(2)}$};
    \node (priordraw1) [above  = 12mm of priordraw2] {$\tilde\theta^{(1)}$};
    \node (priordrawdots) [below = 4mm of priordraw2.center] {$\vdots$};
    \node (priordrawS) [below = 10mm of priordrawdots] {$\tilde\theta^{(S)}$};
    
    \foreach \i in {1,2,S} {
      \node (datadraw\i) [right = 12mm of priordraw\i.west] {$y^{(\i)}$};
      \node (postdraw\i) [right = 12mm of datadraw\i.west] {$\theta^{(\i)}_1, \dots, \theta^{(\i)}_M$};
      \node[invisible] (priordatajoinhelper\i) [above = 9mm of datadraw\i.west] {};
      \node[invisible] (priordatajoin\i) [right = 10mm of priordatajoinhelper\i] {};
      \node (fpost\i) [right = 27mm of postdraw\i.west] {$f\left(\theta^{(\i)}_1, y^{(\i)}\right), \dots, f\left(\theta^{(\i)}_M, y^{(\i)}\right)$};
      \node (fprior\i) [right = 43mm of priordatajoin\i] {$f\left(\tilde\theta^{(\i)}, y^{(\i)}\right)$};
      \node (rank\i) [right = 60mm of fpost\i.west] {$N^{(\i)}_\text{total}$};
    }
    
    \foreach \g in {datadraw,postdraw,fpost,rank} {
        \node ({\g}dots) [below= 4mm of \g2.center] {$\vdots$};
    }

    \node[rectangle,  draw=unifcolor1, fill=unifcolor2] (uniformity) [right = 5mm of rank2] {\begin{tabular}{c} Uniformity \\ test \end{tabular}};
    
    
    \foreach \i in {1,2,S} {
      \draw[->] (prioranchor) -- (priordraw\i);
      \draw[->] (priordraw\i) -- (datadraw\i);
  
      \draw (priordraw\i) -- (priordatajoin\i);
      \draw (datadraw\i) -- (priordatajoin\i);
      \draw[->] (priordatajoin\i) -- (fprior\i);

      \draw[->] (datadraw\i) -- (postdraw\i);
      \draw[->] (postdraw\i) -- (fpost\i);
      \draw[->] (fprior\i.east) .. controls ++(15mm, 0mm) .. (rank\i);

      \draw[->] (datadraw\i) .. controls ++(20mm, 8mm) .. (fpost\i.north west);
      \draw[->] (fpost\i) -- (rank\i);
      %\draw[->] (priordraw\i).. controls ++(35mm, 8mm) .. (rank\i);
    }
    
     \draw[->] (rank1) -- (uniformity.north west);
     \draw[->] (rank2) -- (uniformity.west);
     \draw[->] (rankS) -- (uniformity.south west);

    \end{tikzpicture}
    }%
    \caption{\em Schematic representation of SBC with $S$ simulations. The generator is responsible for sampling from the prior distribution $\tilde\theta \sim \pi_\text{prior}(\tilde\theta)$ and from the observation model $y\sim \pi_\text{obs}(y \mid \tilde\theta)$. The draws from the observation model are then treated as input for the probabilistic program and the associated algorithm which takes $M$ posterior draws $\theta_1, \dots \theta_M$. Each test quantity projects the prior draw and the posterior draws (potentially using data) onto the real line, letting us compute a single rank ($N_\text{total}$). Finally, deviations from discrete uniform distribution are assessed numerically or visually.}
    \label{fig:sbc_schema}
\end{figure}
```

While it is possible to use numerical tests for uniformity with SBC, we generally prefer to use visualisations of the rank distribution as they are more informative than numerical summaries and discourage dichotomous thinking. Most prominent are rank histograms and plots of empirical cumulative distribution functions [@sailynoja_graphical_2021].

Our proposed SBC variant improves upon the way SBC has been previously reported and used in two major ways:

- We let test quantities depend on both data and parameters, while previous work only considered quantities that depend on the parameters. In practice, these test quantities were almost exclusively just the individual parameters themselves.
    
- Previous formulations of SBC required uniformity of NlessN_\mathtt{less}. However, even if the probabilistic program is exactly correct, NlessN_\mathtt{less} will not be uniform if \mboxPr(Nequals>0)>0\mbox{Pr}(N_\mathtt{equals} > 0) > 0, that is, if ties can occur. With our improved SBC procedure, we can handle test quantities that have distributions with point masses and thus ties between f(θ~,y)f(\tilde\theta, y) and (f(θ1,y),…,f(θM,y))(f(\theta_1, y), \ldots, f(\theta_M, y)). Resolving ties lets us use SBC for models with discrete parameters as well as in some other special cases, such as when a theoretically strictly positive test quantity suffers underflow and some prior/posterior sample draws are numerically zero. Random tie-braking has previously been used for checking that data-averaged posterior equals prior [eq:dataaveragedposteriorintroductioneq:data_averaged_posterior_introduction](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#eq:data_averaged_posterior_introduction) over discrete parameter spaces [@saad_family_2019].
    

## Practical considerations

SBC will be satisfied if the generator, probabilistic program, and posterior approximation algorithm are in harmony: The generator and the probabilistic program should correspond to the same data-generating process. At the same time the posterior approximation algorithm (including the associated tuning parameters) provides samples that have at most a negligible difference from the correct posterior for the probabilistic program, given the data simulated from the prior. Failure indicates that at least one of the components is mismatched to the others. However, by itself, SBC cannot determine where exactly the problem lies. As a result, two broad uses of SBC arise:

- We have code to simulate data and a probabilistic program we trust, and the goal is to check that an algorithm correctly samples from the posterior, or
- We have an algorithm that we trust is correct and trustworthy code to simulate data, and the goal is to check that we correctly implemented our probabilistic program.

In practice, those classes overlap and mix: we are rarely completely certain of the correctness of any algorithm, generator, or probabilistic program. Additionally, SBC as a simulation method has no way to inform us about a discrepancy between the process that generated real data and the assumptions of our statistical model. For reliable inference, SBC thus needs to be combined with other elements of Bayesian workflow that can detect model misspecification, such as posterior predictive checks or analysis of residuals [@gabry_visualization_2019; @workflow_preprint; @kay_residuals].

## Importance of test quantities

It has been generally believed that methods based on Equation ([eq:sbcjointdistributioneq:sbc_joint_distribution](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#eq:sbc_joint_distribution)), including SBC, are never sensitive to some classes of mismatches between the generator and the probabilistic program—most notably that it is impossible to detect if the probabilistic program samples from the prior distribution and ignores the information in the data (e.g., Equation (1.3) of [@lee_calibration_2019]; Appendix M.2 of [@pmlr-v130-lueckmann21a]; [@schad_sbc_bf; @pmlr-v161-zhao21b; @ramesh_gatsbi_2022; @cockayne_testing_2022]).

In this paper we show that the choice of test quantities greatly influences the usefulness and sensitivity of SBC. We show that using test quantities that depend on data makes it possible to detect _any_ conceivable mismatch between the generator and the probabilistic program. Thus, we demonstrate that the belief in inherent limitations of SBC has relied on overly restrictive and sometimes plainly incorrect assumptions. We discuss useful classes of test quantities that have not been used so far and provide characterization of possible remaining undetected failures. We provide simulation studies as well as theoretical analysis of SBC to support our findings. We hope that our theoretical framework can serve as a basis for a better understanding of the properties of SBC and related methods. All of the techniques discussed are implemented in the `SBC` R package [@SBC_package].

The rest of the paper is structured as follows: Section [2](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#sec:related) discusses related work, Section [3](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#sec:summary_theory) summarizes the theoretical results we derived, Sections [4](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#sec:numerical) and [5](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#sec:real_world) show results of simulation and real-world case studies, and Section [6](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#sec:conclusions) discusses the results and our recommendations for practical use of SBC.

# Related work

\label{sec:related}

Prior contributions to validation of Bayesian computation can be roughly split into works that focus on the data-averaged posterior, those that focus on the SBC property, and other relevant works that do not directly invoke any self-consistency property.

## Data-averaged posterior

The idea of using simulations via a generator to verify Bayesian computation can be traced back to [@geweke_getting_2004] who compared the moments of the prior and the data-averaged posterior distributions for multiple test quantities. That paper proposed to integrate a transition kernel for an MCMC sampler targeting πpost\pi_\text{post} into a scheme that samples πjoint\pi_\text{joint} directly. This lets one obtain the data-averaged posterior from a single run of this sampler, potentially reducing the computational cost but increasing implementation burden. Geweke’s formalism allows the test quantities to depend on data, although all the examples actually shown only depend on parameters. Comparing the mean vector and covariance matrix of the prior distribution and the data-averaged posterior distribution is also discussed by [@yu_assessment_2021], who use repeated fits to build the data-averaged posterior.

[@saad_family_2019] proposed a check for identity of two potentially high-dimensional discrete distributions by inspecting ranks generated by different total orderings over the parameter space. Their work is relevant in four ways: (1) it can be used to assess the data-averaged posterior criterion ([eq:dataaveragedposteriorintroductioneq:data_averaged_posterior_introduction](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#eq:data_averaged_posterior_introduction)) for discrete domains, (2) in close analogy to the use of test quantities in this paper, they focus on different orderings of the underlying domain and their different power to detect discrepancies, (3) they propose breaking ties in ordering uniformly at random in the same way we do, and (4) they prove some results that are analogous to or special cases of some of our theoretical results.

## SBC-like checks

The identity of prior and posterior distributions conditional on a specific dataset as a tool to check computation was proposed by [@cook_gelman_rubin] and further refined by [@talts_sbc] who introduced SBC as it is currently used. Specific variants of SBC have been proposed for variational inference [@yao_yes_2018], Bayes factors [@schad_sbc_bf], and Gaussian processes [@mcleod_validating_2021]. SBC has also been used to validate likelihood-free inference methods including neural posterior approximators with normalizing flows [@radev2020bayesflow; @radev2021bayesflow] and an SBC variant for checking joint calibration of such methods has been proposed and used in [@radev2023jana].

[@gandy_unit_2021] proposed a procedure similar to SBC that can work with shorter sequences of Markov transitions than a full fit, reducing computational cost. This is, however, less relevant for algorithms that need a nontrivial warmup phase to adapt to the specific posterior (e.g., the adaptive Hamiltonian Monte Carlo sampler implemented in Stan [@stan_jss]). This is because warmup is a fixed cost that occurs during every model fit even if fewer post-warmup draws are needed.

[@prangle_abc] proposed an SBC-like procedure for approximate Bayesian computation (ABC). They note that the possibility that the probabilistic program simply samples from the prior distribution cannot be ignored in this context and resolve this issue by separately inspecting ranks for some subsets of the simulated datasets. SBC is closely related to the _coverage property_ discussed by [@prangle_abc]: when using MM posterior sample draws, SBC can be understood as checking for all posterior intervals of width α∈{1M,…,M−1M}\alpha \in \left\{\frac{1}{M}, \dots, \frac{M - 1}{M}\right\} that the probability the interval contains the original simulated value of the test quantity is α\alpha.

A broader framework for calibration of learning procedures has been proposed by [@cockayne_testing_2022]. There, Bayesian inference is just one example of procedures where calibration can be empirically verified with an SBC-like check. They distinguish between _strong calibration_ which corresponds to passing SBC (specifically continuous SBC as defined in Appendix A [@cockayne_testing_2022]) for all measurable test quantities and _weak calibration_ which corresponds to having a correct data-averaged posterior ([eq:dataaveragedposteriorintroductioneq:data_averaged_posterior_introduction](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#eq:data_averaged_posterior_introduction)). They however only consider test quantities that depend only on parameters.

## Miscellaneous

The problem of diagnosing and understanding computational issues is transformed by [@pmlr-v151-rendsburg22a]. Their approach tries to find a prior distribution that would make the probabilistic program and algorithm exactly match the generator.

Both [@grosse2016] and [@domke_easy_2021] proposed to use fits to multiple generated datasets to estimate the symmetrized KL-divergence between a distributional approximation to the correct posterior (e.g., Laplace or variational inference) and the true posterior. [@CusumanoTowner2017] described a method to compute the symmetrized KL-divergence between a gold standard posterior and an approximate posterior.

# Theoretical results

\label{sec:summary_theory}

The formalism required for SBC is relatively heavy on definitions and syntax, so for all results in this section we also provide plain English-language summaries. Proofs, expanded definitions (as required for proofs) and some additional discussion can be found in Appendix A [@SBCappendices]. Some of the results for stochastic rank statistics (SRS) by [@saad_family_2019] can be understood as special cases of some of our theorems. Specifically, SRS assumes that the parameter space Θ\Theta is finite or countable and the goal is to directly compare two distributions, which is the same as assuming the data space YY has just a single element. We will refer to this special case as the _SRS assumption_.

```latex
\begin{definition}[Posterior family, test quantity]
A \emph{posterior family} $\phi$ assigns a normalized posterior density to each possible $y \in Y$. That is, a posterior family is a function $\phi: \Theta \times Y  \rightarrow  \mathbb{R^{+}}$ such that $\forall y: \int \mathrm{d}\theta \:\phi(\theta | y) = 1.$
For each $y$, we will denote the implied distribution over $\Theta$ as $\phi_y$. 

A \emph{test quantity} is any measurable function $f: \Theta \times Y  \to  \mathbb{R}$ 
\end{definition}
```

```latex
\begin{definition}[Sample rank CDF, sample Q, sample SBC]
Given a test quantity $f$, $M \in \mathbf{N}$ and a posterior family $\phi$. If $\theta_1, \dots, \theta_M \sim \phi_y$ we can define the following random variables:
%
\begin{align*}
   N_{\phi,f,\tilde\theta,y}^{\mathtt{less}} &:= \sum_{m=1}^M \mathbb{I} \left[f(\theta_m, y) < f(\tilde\theta, y) \right] \\
   N_{\phi,f,\tilde\theta,y}^{\mathtt{equals}} &:= \sum_{m=1}^M \mathbb{I} \left[f(\theta_m, y) = f(\tilde\theta, y) \right] \\
    K_{\phi,f,\tilde\theta,y} &\sim \mathrm{uniform}\left(0,  N_{\phi,f,\tilde\theta,y}^{\mathtt{equals}}\right) \\
    N_{\phi,f,\tilde\theta,y}^\mathtt{total} &:= N_{\phi,f,\tilde\theta,y}^{\mathtt{less}} + K_{\phi,f,\tilde\theta,y}^{\mathtt{equals}}.
\end{align*}    

The \emph{$M$-sample $Q$} is:
%
\begin{equation*}
  Q_{\phi,f}(i | y) := \int_\Theta \mathrm{d}\tilde\theta \: \pi_\text{post}(\tilde\theta|y)  \mbox{Pr} \left( N_{\phi,f,\tilde\theta,y}^\mathtt{total} \leq i \right)
\end{equation*}
%
We then say that \emph{$\phi$ passes $M$-sample SBC w.r.t.\ $f$} if, $\forall i \in 0, \dots, M - 1$,
%
\begin{equation*}
\int_Y \: \mathrm{d}y \: Q_{\phi,f}(i |y) \pi_\text{marg}(y) = \frac{i+1}{M + 1}.    
\end{equation*}
\end{definition}
```

This definition does not match immediately with the procedure we actually use to run SBC in practice but is more convenient for further analysis and is equivalent:

```latex
\begin{theorem}[Procedural definition of sample SBC]
\label{th:procedural_sbc}
A posterior family $\phi$ passes $M$-sample SBC w.r.t.\ $f$ if and only if given $\randvar{\tilde\theta} \sim \pi(\tilde\theta), \randvar{y} \sim \pi_\text{obs}(y | \randvar{\tilde\theta}), N^\mathtt{total} = N_{\phi,f,\randvar{\tilde\theta},\randvar{y}}^\mathtt{total}$ we have $N^\mathtt{total} \sim \mathrm{uniform}(0, M)$.    
\end{theorem}
```

Next, we define an idealized, continuous version of SBC that will be more amenable to theoretical analysis:

```latex
\begin{definition}[Continuous rank CDF, continuous $q$, continuous SBC]
We first define fitted CDF: $C^\pi_{\phi,f}: \bar{\mathbb{R}}\times Y\to [0,1]$, $C^\pi_{\phi,f}(s | y) := \int_{\Theta}\mathrm{d}\theta\,\mathbb{I}\left[f\left(\theta, y \right) \leq s\right]\phi\left(\theta| y\right)$ and fitted tie probability: $D^\pi_{\phi,f}: \bar{\mathbb{R}}\times Y\to [0,1]$, 
    $D^\pi_{\phi,f}(s | y) := \int_\Theta \mathrm{d}\theta \: \phi(\theta | y) \mathbb{I}\left[ f(\theta, y) = s \right]$

We then define the \emph{continuous } $q: [0,1] \times Y \to [0,1]$ as
\begin{equation*}
    q_{\phi,f}(x|y) := 
\int_\Theta \mathrm{d} \tilde\theta \: \pi_\text{post}(\tilde\theta | y) \mbox{Pr}\left(C_{\phi,f} \left(f \left(\left.\tilde\theta, y \right) \right| y \right) - U D_{\phi,f} \left(f \left(\left.\tilde\theta, y \right) \right| y \right) \leq x\right),
\end{equation*}
%
assuming $U$ is a random variable distributed uniformly over the $[0, 1]$ interval. 

Finally, \emph{$\phi$ passes continuous SBC w.r.t.\ $f$} if $
\forall x \in [0, 1]: \int_Y \mathrm{d}y \: q_{\phi,f}(x|y) \pi_\text{marg}(y)  = x$.
\end{definition}
```

## Correctness

With the definitions ready, we first establish that if a probabilistic program achieves uniform distribution of ranks in sample SBC for a given test quantity as M→∞M \to \infty, then it will satisfy continuous SBC as well.

```latex
\begin{theorem}[Sample SBC implies continuous SBC] 
\label{th:sample_implies_continous}
\ 
\begin{enumerate}
    \item For any fixed $y \in Y$ if as the number of sample draws $M \to \infty$ we have $\forall i \in \{0, \dots, M \}: Q_{\phi,f}(i | y) \to \frac{i + 1}{M + 1}$ then $\forall x \in [0, 1]: q_{\phi,f}(x | y) = x$.
    \item If as $M \to \infty$ we have $\forall i \in \{0, \dots, M \}: \int_Y \mathrm{d}y \, Q_{\phi,f}(i | y) \pi_\text{marg}(y) \to \frac{i + 1}{M + 1}$ then $\phi$ passes continuous SBC for $f$.
\end{enumerate}
\end{theorem}
```

Theorem 3 then shows that if a probabilistic program passes continuous SBC for a given test quantity, it will pass sample SBC for all MM. We then show that passing continuous SBC (and thus our SBC variant) is a necessary condition for the correctness of posterior estimation (Theorem 4). That is, the correct posterior will always produce uniformly distributed ranks, including for test quantities that may have ties (see also Examples 5 and 6 in Appendix B [@SBCappendices]). A special case of Theorem 4 under the SRS assumption was proven as Theorem 3.1 of [@saad_family_2019].

```latex
\begin{theorem}[Continuous SBC implies sample SBC] 
\label{th:continuous_implies_sample}
For all $M \in \mathbf{N}$:
\begin{enumerate}
    \item For any $y \in Y$, if $\forall x \in [0,1]: q_{\phi,f}(x|y) = x$ then $\forall i \in \{0, \dots, M - 1 \}: Q_{\phi,f}(i | y) = \frac{i + 1}{M + 1}$.
    \item If $\phi$ passes continuous SBC w.r.t.\ $f$, then $\phi$ passes $M$-sample SBC w.r.t.\ $f$. 
\end{enumerate}
\end{theorem}
```

```latex
\begin{theorem}[Correct posterior and $q$] 
\label{th:SBC_correct}
For any $y \in Y$, if $\forall \theta \in \Theta: \phi(\theta | y) = \pi_\text{post}(\theta | y)$ then for any test quantity $f$ we have $\forall x \in [0, 1]: q_{\phi, f}(x | y) = x$.
\end{theorem}
```

## Characterization of SBC failures

Still, many incorrect posteriors will also pass SBC for any given test quantity, so in Theorem 5 we characterize those situations.

```latex
\begin{theorem}[Characterization of SBC failures]
\label{th:characterization_failures}
For all $y \in Y$ and $s \in \mathbb{R}: \\ \int_{\Theta}\mathrm{d}\theta\,\mathbb{I}\left[f\left(\theta, y \right) \leq s\right]\phi\left(\theta| y\right) = \int_{\Theta}\mathrm{d}\theta\,\mathbb{I}\left[f\left(\theta, y \right) \leq s\right]\pi_{\text{post}}(\theta | y)$ if and only if \\ $\forall x \in [0, 1]: q_{\phi,f}(x | y) = x$.
\end{theorem}
```

Not only does the correct posterior yield a uniform distribution of ranks when averaging over the whole data space YY, but the ranks are uniformly distributed even when we only consider simulations that yielded data in some Yˉ⊂Y\bar{Y} \subset Y. The reverse implication also holds: when the ranks are uniformly distributed for all subsets of the data space Yˉ⊂Y\bar{Y} \subset Y, then the implied posterior distribution of the test quantity under investigation has to be exactly correct. In other words, whenever SBC "fails" and the implied posterior distribution of a given test quantity is incorrect although the rank distribution is uniform, we can find a subset of the data space, where the ranks are non-uniform. It just so happens that all the deviations in various subsets cancel each other out perfectly.

An obvious application of Theorem 5 is that we could partition our simulations based on some features of the data space and investigate uniformity separately for each part, similarly to the procedure suggested by [@prangle_abc]. This however quickly runs into issues of multiple testing due to the lower number of simulations in each part. It is thus in our experience not practical except for the special case that interest lies only in some subset of the data space, so that the SBC checks can focus only on that data space of interest. This is a form of rejection sampling and can be practically useful if it is easy to formulate a criterion that constrains plausible real data sets but hard to construct a defensible prior distribution that would enforce this criterion implicitly. For example, prior information can be available on the plausible variance of an outcome across the whole population, which may be hard to express as a prior on coefficients associated with predictors (but see the approaches for linear models discussed in [@zhang_bayesian_2022] and [@aguilar2022intuitive]).

## Data-dependent test quantities

\label{sec:data_data_dependent_quantities}

The characterization of SBC failures discussed above provides intuition why test quantities that depend on data are useful: If SBC passes for a test quantity ff, but the posterior is in fact incorrect, we can always pick a test quantity gg that combines ff with some aspect of the data and ensures that the discrepancies in various parts of data space add up instead of canceling out. For example, we could have over-abundance of low ranks and under-abundance of high ranks in Y1⊂YY_1 \subset Y and a matching under-abundance of low ranks and over-abundance of high ranks in Y2⊂YY_2 \subset Y. Setting

g(θ,y)={−f(θ,y)y∈Y1f(θ,y) otherwiseg(\theta, y) = \begin{cases} -f(\theta, y) & y \in Y_1 \\ f(\theta, y) & \text{ otherwise} \end{cases}

will ensure over-abundance of high ranks in both Y1Y_1 and Y2Y_2. Since such a test quantity uses all the simulations, we do not lose power from reduced number of simulations.

An even stronger reason to use data-dependent test quantities is that they make SBC in some sense complete: If there is any difference between the correct posterior and the posterior implemented by the probabilistic program, there will exist a data-dependent test quantity that fails SBC. In fact, we can construct a specific test quantity that detects the failures, which is the ratio of the correct posterior density to the posterior density actually implemented by the probabilistic program.

```latex
\begin{theorem}[Density ratio]
\label{th:density_ratio}
For any posterior family $\phi$, take $g\left(\theta,y\right)=\frac{\pi_{\mathrm{post}}\left(\theta\mid y\right)}{\phi\left(\theta|y\right)}$. Then $\phi$ passes continuous SBC w.r.t.\ $g$ if and only if $\pi_{\mathrm{post}}$ and $\phi$ are equal except for a set of measure $0$:

\begin{equation*}
\int_Y \mathrm{d}y \int_\Theta \mathrm{d}\theta \: \pi_\text{joint}(y, \theta) \mathbb{I}\left[ \pi_\text{post}(\theta | y) \neq \phi(\theta | y)\right] = 0. 
\end{equation*}
\end{theorem}
```

Here gg is not a practical test quantity, as it (a) depends on the specific probabilistic program we implemented and (b) requires that we already have the correct posterior density. However our empirical results in this paper, and our experience with using SBC in model development more generally, shows that the model likelihood πobs(y∣θ)\pi_\text{obs}(y | \theta) is frequently useful as a general-purpose test quantity. This makes sense intuitively, as the likelihood is an important contributor to the density ratio. In their Theorem 3.1, [@saad_family_2019] proved an analogous result under the SRS assumption, although relying on a different test quantity. Under the SRS assumption, they also show that the _difference_ of the two densities will fail MM-sample SBC for all M>1M > 1 (their Theorem 3.6) and has maximum power against discrepancies (their Theorem 3.7).

## Ignoring data

We generalize the result that probabilistic programs sampling from the prior distribution will pass SBC against all test quantities that do not depend on data.

```latex
\begin{theorem}[Incomplete use of data] 
\label{th:incomplete_use_data}
Assume a model $\pi$ with observation space $Y$ and parameter space $\Theta$, a space $Y^\prime$, and a measurable function $t: Y \rightarrow Y^\prime$. Denote the set $t^{-1}(y^\prime) = \{y \in Y: t(y) = y^\prime\}$. Consider the model $\pi^\prime$ with parameter space $\Theta$ and observation space $Y^\prime$ such that for all $\theta \in \Theta, y^\prime \in Y^\prime$:
%
\begin{align*}
\pi^\prime_\text{prior}(\theta) &= \pi_\text{prior}(\theta) \\
\pi^\prime_\text{obs}(y^\prime |\theta) &= \int_{t^{-1}(y^\prime)} \mathrm{d}y \: \pi_\text{obs}(y | \theta).
\end{align*}

Assume a test quantity $f^\prime: Y^\prime \times \Theta \rightarrow \mathbb{R}$. 
If we have a posterior family $\phi^\prime$ on $Y^\prime, \Theta$ such that $\phi^\prime$ passes continuous SBC w.r.t.\ $f^\prime$ and set test quantity $f: Y \times \Theta \rightarrow \mathbb{R}, f(\theta, y) = f^\prime(\theta, t(y))$ and posterior family $\phi$ on $\Theta, Y$ such that $\phi(\theta | y) = \phi^\prime(\theta | t(y))$ then $\phi$ passes continuous SBC w.r.t.\ $f$.
\end{theorem}
```

Here, the choice of tt lets us choose which aspects of the data are ignored, if ∀y∈Y:t(y)=1\forall y \in Y: t(y) = 1, we recover the case where all data are ignored: πpost′(θ∣y)=πprior(θ)\pi^\prime_\text{post}(\theta | y) = \pi_\text{prior}(\theta) and thus ϕ(θ∣y)=πprior(θ)\phi(\theta |y) = \pi_\text{prior}(\theta) will pass SBC w.r.t.\ ff. If tt is a bijection, no information is lost. Other choices of tt then let us interpolate between those two extremes, for example ignoring just a subset of the data points, treating some data points as censored, rounding all data to integers.

## Detailed analysis of simple models and test quantities

\label{sec:examples_summary}

Appendix B [@SBCappendices] provides full theoretical analysis of SBC for simple models and test quantities where we can actually characterize all possible posterior distributions that will satisfy SBC. This is aimed at providing intuition on what SBC actually does and also serves as counterexamples to some claims.  
In some literature (e.g., [@lee_calibration_2019; @pmlr-v130-lueckmann21a; @schad_sbc_bf; @grinsztajn_bayesian_2021; @ramesh_gatsbi_2022; @saad_family_2019]), it is assumed that SBC is based on the data-averaged posterior ([eq:dataaveragedposteriorintroductioneq:data_averaged_posterior_introduction](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#eq:data_averaged_posterior_introduction)).  
We show that this is incorrect: Example 2 not only explicitly constructs posterior distributions that will satisfy ([eq:dataaveragedposteriorintroductioneq:data_averaged_posterior_introduction](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#eq:data_averaged_posterior_introduction)) for some test quantity while not passing SBC, but also posterior distributions that pass SBC while not satisfying ([eq:dataaveragedposteriorintroductioneq:data_averaged_posterior_introduction](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#eq:data_averaged_posterior_introduction)). One possibly more general lesson is that SBC is most naturally understood as enforcing constraints on the _quantile_ function of the test quantity while having a correct data-averaged posterior is most naturally seen as constraint on the _density_ of the test quantity.

This implies there might be some gains from using both the data-averaged posterior and SBC when verifying the correctness of Bayesian computation. We however suspect that the additional practical benefit of using the data-averaged posterior is small in the sense that the incorrect posteriors that pass SBC but are ruled out by Equation ([eq:dataaveragedposteriorintroductioneq:data_averaged_posterior_introduction](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#eq:data_averaged_posterior_introduction)) are mostly contrived and unlikely to be a result of a computational problem or an inadvertent mistake. Lemma 2.19 of [@cockayne_testing_2022] proves that if a posterior passes SBC for _all possible_ test quantities that do not depend on data, it will have the correct data-averaged posterior for all test quantities that do not depend on data, so SBC is stronger at least in the limit of using infinitely many test quantities. We leave a more thorough examination of the relationship between data-averaged posterior and SBC as future work.

Additionally, we show the behavior of SBC when ties are present, whether induced by a test quantity (Example 5) or by discrete parameter space (Example 6). Discrete parameter spaces may induce additional structure on the space of posterior families passing SBC.

## Monotonic transformations of test quantities

\label{sec:monotonic}

Finally, transforming a test quantity by a strictly monotonic function produces equivalent SBC results:

```latex
\begin{theorem}[Monotonic transformations]
\label{th:monotonous_transformations}
Assume test quantities $f,g$ and a set of measurable functions $h_y: \mathbb{R} \to \mathbb{R}$ such that $\forall y \in Y ,\theta \in \Theta: f(\theta, y) = h_y(g(\theta_1, y))$ and a posterior family $\phi$. If either for all $y \in Y: h_y$ is strictly increasing or  for all $y \in Y: h_y$ is strictly decreasing then 1) $\phi$ passes continuous SBC w.r.t.\ $f$ if and only if $\phi$ passes continuous SBC w.r.t.\ $g$ and 2) $\phi$ passes $M$-sample SBC w.r.t.\ $f$ if and only if $\phi$ passes $M$-sample SBC w.r.t.\ $g$.    
\end{theorem}
```

The result cannot be easily strengthened as many non-monotonic transformations lead to different, non-equivalent SBC checks. Example 3 shows that flipping the ordering of values only for some subset of the data space yields a different SBC check. Example 4 shows that we can also obtain a different check if we combine a test quantity with a non-monotonic bijection, and Example 5 shows the same for the case when a whole range of values is projected onto a single point. In all those examples, the transformed test quantities rule out some sets of posteriors that pass SBC for the original quantity, but there are also sets of posteriors not passing SBC for the original quantity but passing SBC for the transformed quantity.

# Numerical case studies

\label{sec:numerical}

The theoretical analysis in previous section primarily deals with the behavior of SBC in the limit of both infinitely many posterior draws per fit and infinitely many simulations. Here, we further support the results by numerical experiments which let us understand not only whether a certain problem is detectable at all but also how much computational effort is required for SBC to detect the problem.

## Setup

To illustrate some of the properties of various types of test quantities, we use a simple multivariate normal model,

μ∼\mboxMVN(0,Σ)y1,…,yn∼\mboxMVN(μ,Σ)Σ=(10.80.81),\begin{aligned} \mathbf{\mu} &\sim \mbox{MVN}(0, \mathbf{\Sigma}) \\ \mathbf{y}_1, \ldots, \mathbf{y}_n &\sim \mbox{MVN}(\mathbf{\mu}, \mathbf{\Sigma}) \\ \mathbf{\Sigma} &= \begin{pmatrix} 1 & 0.8 \\ 0.8 & 1 \end{pmatrix}, \end{aligned}

where the two-element vector μ\mu is the target of inference and y1,…,yn\mathbf{y}_1, \ldots, \mathbf{y}_n are observed.  
Introducing yˉ=1n∑i=1nyi\bar{\mathbf{y}} = \frac{1}{n}\sum_{i = 1}^{n} \mathbf{y}_i, the correct analytic posterior is \mboxMVN(Nyˉn+1,1n+1Σ)\mbox{MVN}\left(\frac{N\bar{\mathbf{y}}}{n + 1}, \frac{1}{n + 1}\mathbf{\Sigma}\right). Unless mentioned otherwise we will use n=3n = 3.

In most previous use cases of SBC, the only test quantities used would have been the parameters themselves, that is, the elements of μ\mathbf{\mu} in the above example. Below, we also check a host of derived quantities: the sum, difference, and product of the μ\mathbf{\mu} elements, the joint likelihood of all the data, and pointwise likelihoods for the first two data points.

To quantify the discrepancy between an observed distribution of posterior ranks and the uniform distribution, we take the likelihood of observing the most extreme point on the empirical CDF if the rank distribution was indeed uniform:

γ=2min⁡i∈{1,…,M+1}(min⁡{Bin(Ri∣S,zi),1−Bin(Ri−1∣S,zi)}).\gamma = 2 \min_{i\in \{1, \dots, M+1\}}\left(\min\{\text{Bin}(R_i | S, z_i), 1 - \text{Bin}(R_i - 1 | S, z_i)\}\right).

Here, MM is the number of draws in the sample obtained from the posterior, SS is the number of simulations (and thus the number of observed ranks), zi=iM+1z_i = \frac{i}{M + 1} is the expected proportion of observed ranks smaller than ii, RiR_i is the observed count of ranks smaller than ii, and Bin(R∣S,p)\text{Bin}(R | S, p) is the CDF of the binomial distribution with SS trials and probability of success pp evaluated at RR. This metric was introduced in a paper by [@sailynoja_graphical_2021], where we can also find computational methods to evaluate the distribution of γ\gamma under uniform distribution of ranks for given MM and SS. Our primary metric of interest would then be log⁡γγˉ\log\frac{\gamma}{\bar{\gamma}}, where γˉ\bar{\gamma} is the 5th percentile of the null distribution. That is, if you adopt a hypothesis-testing framework, then log⁡γγˉ<0\log\frac{\gamma}{\bar{\gamma}} < 0 implies a rejection of the hypothesis of uniform distribution at the 5% level. Having log⁡γγˉ<0\log\frac{\gamma}{\bar{\gamma}} < 0 also corresponds to situations where visual checks of the ECDF plots would show problems (for a single test quantity). This diagnostic is typically more sensitive than the Kolmogorov-Smirnoff or χ2\chi^2 test.

## Correct posterior — Case study 1

![Case study 1: Evolution of the difference between the gamma statistic and threshold (\log \bar{\gamma}) for rejecting uniformity at 5% for the correct posterior. \texttt{mvn_log_lik[1]} and \texttt{mvn_log_lik[2]} are the pointwise likelihoods \pi(\mathbf{y}_1|\mu) and \pi(\mathbf{y}_2|\mu) respectively, while \texttt{mvn_log_lik} is the joint likelihood. As expected when using a 5% level for rejection, false positives (values below the threshold) do happen, but they tend to correspond to only small discrepancies.](https://chatgpt.com/c/hist_correct.pdf)

Figure [fig:histcorrectfig:hist_correct](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#fig:hist_correct){reference-type="ref" reference="fig:hist_correct"} shows how the γ\gamma statistic evolves in a fairly typical SBC run as we add more simulations using a probabilistic program that samples from the correct posterior. There is some variability, but most of the time all quantities would indicate uniformity and if they indicate some non-uniformity, the discrepancies tend to be small so we are unlikely to reject this model as incorrect.

## Ignoring data — Case studies 2--4

![Case study 2: Evolution of the difference between the gamma statistic and threshold for rejecting uniformity at 5% for an incorrect posterior that equals the prior. Note how quickly large discrepancies accumulate for the likelihood-based quantities, despite the horizontal axis being zoomed to show only first 50 simulations.](https://chatgpt.com/c/hist_prior_only.pdf)

Figure [fig:histprioronlyfig:hist_prior_only](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#fig:hist_prior_only) shows the evolution of the same quantities for a typical run with an incorrect posterior that is completely equal to the prior. All quantities that do not depend on data pass SBC, barring small short-term deviations as seen for the correct posterior. But all the likelihood-based quantities start showing big discrepancies after just a handful of simulations. While the overall distribution of ranks for the parameters themselves is uniform, when we look separately at data with large average yy and low average yy, the ranks are strongly non-uniform in both regions (Figure [fig:rankhistprioronlysplitfig:rank_hist_prior_only_split](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#fig:rank_hist_prior_only_split)).

![Case study 2: Rank distribution for the elements of \mu split by the average value of the corresponding y elements for the incorrect posterior that is completely equal to the prior. The distributions for the two cases exactly compensate to make the overall distribution uniform. The gray horizontal line represents exact uniform distribution and the blue areas represent an approximate 95% prediction interval for the observed ranks, assuming uniform rank distribution.](https://chatgpt.com/c/rank_hist_prior_only_split.pdf)

In case study 3, we observe similar behaviour for the posterior that ignores only the first data point; see Figure [fig:histonemissingfig:hist_one_missing](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#fig:hist_one_missing). The biggest difference is that that now the pointwise likelihood for the second data point—which was _not_ ignored—passes SBC, while the joint likelihood as well as the pointwise likelihood for the first ignored data point show problems. Additionally, the pointwise likelihood for the ignored data point now shows bigger discrepancy than the joint likelihood. For both quantities, the discrepancy is smaller and requires about S=20S = 20 simulations to reliably uncover, because ignoring a single data point produces a posterior that is closer to the correct one than when ignoring all the data.

![Case study 3: Evolution of the difference between the gamma statistic and threshold for rejecting uniformity at 5% for an incorrect posterior that ignores the first datapoint among a small data set (n = 3).](https://chatgpt.com/c/hist_one_missing.pdf)

For case study 4 we increase the number of data points to n=20n = 20 (Figure [fig:histonemissing20fig:hist_one_missing_20](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#fig:hist_one_missing_20)), ignoring just a single data point produces a posterior that is close to correct and even after 1000 simulations, the discrepancy for the joint likelihood is small. The pointwise likelihood for the first (ignored) data point still detects the problem relatively quickly.

![Case study 4: Evolution of the difference between the gamma statistic and threshold for rejecting uniformity at 5% for an incorrect posterior that ignores the first datapoint among a larger dataset (n = 20).](https://chatgpt.com/c/hist_one_missing_20.pdf)

More generally, if the model (partially) ignores data, then adding a test quantity that involves both data and parameters can detect this failure. Specifically adding the joint log-likelihood of the data as a derived quantity seems to be a useful default. If only a small part of the data is missing, using the joint likelihood in SBC will turn it into a problem of precision. Missing just a single datapoint in a large dataset (e.g., an off-by-one error in the probabilistic program) may change the posterior only slightly and be undetectable with realistic computational effort.

## Incorrect correlations — Case study 5

Suppose we have an incorrect posterior that has the correct marginal distributions for both parameters, i.e., sampling is done from independent univariate normal distributions, μi∣y1,…,yn∼N(nyˉin+1,1n+1Σi,i)\mu_i \mid \mathbf{y}_1, \ldots, \mathbf{y}_n \sim N\left(\frac{n\bar{\mathbf{y}}_i}{n + 1}, \frac{1}{n + 1}\mathbf{\Sigma}_{i,i}\right). The evolution of the discrepancy as simulations are added is shown in Figure [fig:histcorrfig:hist_corr](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#fig:hist_corr). If the test quantities are the univariate parameters, SBC passes without any indication of problems, while the likelihood-based quantities as well as the difference, product, and sum of the variables show problems relatively quickly. The joint likelihood is the first to show serious issues.

![Case study 5: Evolution of the difference between the gamma statistic and threshold for rejecting uniformity at 5% for incorrect posterior that has wrong correlation structure.](https://chatgpt.com/c/hist_corr.pdf)

If the inference does not represent correlations in the posterior correctly, this should as well manifest in an SBC failure for some function of the parameters. This can be directly targeted by using products (“interactions”) of model parameters, but the log-likelihood once again seems to be generally useful as a highly nonlinear function of all model parameters.

## Less plausible problems — Case study 6

In this subsection our results get less practical and more theoretical. The (partially) unused data case may easily arise in practice due to a bug in the probabilistic program such as an indexing bug or a deficient overall approach. For example, an approximate Bayesian computation algorithm may not learn from the data at all and just stick to the prior [@prangle_abc]. Incorrect correlations or more general higher-order structure of the posterior may also easily arise due to a problem with an approximate inference algorithm. For example, mean-field variational inference will never recover any correlations by design. Beyond those examples, we have found it hard to find incorrect probabilistic programs that would satisfy the SBC identity and could plausibly arise from unintentional mistakes in program code or problems with an algorithm. We see this as anecdotal evidence that SBC augmented with a few well-chosen test quantities that probe usage of data and higher order posterior structure such as the likelihood can robustly detect these kinds of mistakes. That said, for specific models, wide sets of artificial counterexamples that incorrectly pass SBC can be constructed.

In case study 6, we show a specific case of a more general class of setups where we can create an incorrect posterior approximation that produces overabundance of low ranks for datasets with average of y\mathbf{y} positive and compensates by producing overabundance of high ranks for other datasets. If this is done right, the test quantity will pass SBC. The distribution of the ranks conditional on the average of y\mathbf{y} for one such setup is shown in Figure [fig:rankhistnonmonsplitfig:rank_hist_non_mon_split](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#fig:rank_hist_non_mon_split)—here we transform draws from the correct posterior distribution by first applying the correct CDF, manipulating the results to achieve the desired shape of ranks and then transform back via the quantile function. See the associated code for more details. As seen in Figure [fig:histnonmonfig:hist_non_mon](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#fig:hist_non_mon), when averaging over all datasets, SBC indeed passes for the univariate parameter test quantities, but if we instead look at, say, the absolute value of μ\mu (as well as some other non-monotonic transformations of μ\mu), we immediately see problems as now some of the previously low ranks flip to high ranks and the discrepancies accumulate instead of canceling each other. In this particular case, the problem is also eventually picked up by the product of the μ\mu values and with enough simulations even by the joint likelihood, but there is no guarantee this will always happen. In general, non-monotonic transformations can discover incorrect posteriors that would be otherwise hidden when looking at the original variables. Still, the practical relevance of non-monotonic transforms in SBC is, in our view, likely limited, as it required careful work to construct posteriors that manifested this behaviour. We were unable to find even remotely plausible scenarios where an issue with Bayesian computation was best discovered by using a non-monotonic transformation of another test quantity.

![Case study 6: Rank distribution for the elements of \mu split by the average value of the corresponding y elements for the incorrect posterior that satisfies SBC for individual parameters. The distributions for the two cases exactly compensate to make the overall distribution uniform. The gray horizontal line represents exact uniform distribution and the blue areas represent an approximate 95% prediction interval for the observed ranks, assuming uniform rank distribution.](https://chatgpt.com/c/rank_hist_non_mon_split.pdf)

![Case study 6: Evolution of the difference between the gamma statistic and threshold for rejecting uniformity at 5% for incorrect posterior that satisfies SBC for individual parameters. Note the different horizontal axis between top row (quantities that detect the problem slowly or not at all) and bottom row (quantities that detect the problem quickly). The vertical red dashed line marks 500 simulations. We only show quantities derived from the first element of \mu; the situation is analogous for the second element. The \texttt{drop(mu[1])} quantity is defined as  \mu_1 \text{ if } \mu_1 < 1 and as \mu_1 - 5 \text{ otherwise}.](https://chatgpt.com/c/hist_non_mon.pdf)

## Small discrepancies — Case study 7

A final case study considers small discrepancies in the posterior. To be specific, we introduce a small bias in the posterior drawn from \mboxnormal(0,0.3)\mbox{normal}(0, 0.3) independently for each simulation and element of μ\mu. The resulting SBC history is shown in Figure [fig:histsmallchangefig:hist_small_change](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#fig:hist_small_change). While all of the monitored quantities will eventually show the problem, the likelihood-based quantities and the difference of μ\mu do that noticeably sooner than others. This demonstrates that derived quantities can somewhat improve precision of SBC: small changes in the univariate marginals can result in big (and thus easy to detect) changes for some test quantities combining the univariate marginals with data and other parameters.

![Case study 7: Evolution of the difference between the gamma statistic and threshold for rejecting uniformity at 5% for incorrect posterior that introduces a small bias for each parameter.](https://chatgpt.com/c/hist_small_change.pdf)

# Real-world case study

\label{sec:real_world}

We present a case study adapted from an actual user discussion on forums of the Stan probabilistic programming language. Our goal is to use Stan and its Hamiltonian Monte Carlo implementation to sample from a distribution over an ordered KK-dimensional simplex which is then to be used as a component in a larger model:[2](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#user-content-fn-2)

OrdSimplexK={x∈RK∣0<x1<…<xK<1,∑i=1Kxi=1}. \text{OrdSimplex}_K = \{\mathbf{x} \in \mathbb{R}^K | 0 < x_1 < \ldots < x_K < 1, \sum_{i=1}^K x_i = 1 \}.

To do that, we need to construct an ordered simplex from primitive data types available in Stan and compute the logarithm of the Jacobian determinant of the transformation (up to a constant).

## Proposed implementations

We consider three variants. Mimicking the fallibility of methods proposed by real statisticians, not all of the following derivations will be correct. A reader interested in little mathematical puzzles may try to pin down any errors. In the next subsection, we will then show how to use SBC to discover the error(s) without the painstaking attention to detail required for checking the math. We then also remedy the error(s).

The first variant will be called `min`.  
Here, we start with an unordered bounded vector u∈[0,1]K−1\mathbf{u} \in [0,1]^{K - 1} (which is a primitive in Stan). The minimal element of the simplex needs to satisfy x1<1Kx_1 < \frac{1}{K}, so we set x1=u1Kx_1 = \frac{u_1}{K}. Given x1x_1, if we set x′∈RK−1,xi′=xi+1−x11−Kx1=xi+1−x11−u1\mathbf{x}^\prime \in \mathbb{R}^{K-1}, x^\prime_i = \frac{x_{i + 1} - x_1}{1 - Kx_1} = \frac{x_{i + 1} - x_1}{1 - u_1}, then x′∈OrdSimplexK−1\mathbf{x}^\prime \in \text{OrdSimplex}_{K - 1}, giving us a recursive formula for the transformation, which we can unroll as:

b1=0,r1=1for 1≤i<K:xi=bi+riuiK+1−i,bi+1=xi,ri+1=ri(1−ui)xk=bk+rk=1−∑i=1K−1xi.\begin{aligned} b_1 &= 0, \quad r_1 = 1 \\ \text{for } 1 \leq i < K &: \quad x_i = b_i + r_i \frac{u_i}{K + 1 - i}, \quad b_{i + 1} = x_i, \quad r_{i + 1} = r_{i}( 1 - u_i) \\ x_k &= b_k + r_k = 1 - \sum_{i=1}^{K - 1}x_i. \end{aligned}

Here rir_i can be understood as tracking the remaining amount to be distributed to ensure xix_i sum to 11 if all the following elements will be at least bib_i. For 1≤i<K1 \leq i < K, we have ∂xi∂ui=riK+1−i\frac{\partial x_i}{\partial u_i} = \frac{r_i}{K + 1 - i}, and when also i≤j<Ki \leq j < K then ∂xi∂uj=0\frac{\partial x_i}{\partial u_j} = 0, so the Jacobian matrix is triangular and the Jacobian determinant is thus

det⁡J=∏i=1K−1riK+1−i.\det \mathbf{J} = \prod_{i=1}^{K-1} \frac{r_i}{K + 1 - i}.

The second variant, called `softmax` starts with a positive ordered vector v∈(0,+∞)K−1\mathbf{v} \in (0, +\infty)^{K-1}, v1<…<vK−1v_1 < \ldots < v_{K-1} (also a primitive in Stan). We then prepend 00 to the vector and normalize it with the softmax function:

s=1+∑i=1K−1exp⁡(vi),x1=1s,xk=exp⁡(vk−1)s.s = 1 + \sum_{i=1}^{K-1} \exp (v_i), \quad x_1 = \frac{1}{s}, \quad x_k = \frac{\exp (v_{k - 1})}{s}.

For k>1,1≤j≤K−1,j≠k−1k > 1, 1 \leq j \leq K - 1, j \neq k - 1 the partial derivatives are:

∂xk∂vk−1=exp⁡(vk−1)s−exp⁡(2vk−1)s2=exp⁡(vk−1)(s−exp⁡vk−1)s2\frac{\partial x_k}{\partial v_{k - 1}} = \frac{\exp (v_{k-1})}{s} - \frac{\exp(2v_{k-1})}{s^2} = \frac{\exp (v_{k-1})(s - \exp v_{k-1})}{s^2} ∂xk∂vj=−exp⁡(vk−1+vj)s2.\frac{\partial x_k}{\partial v_{j}} = -\frac{\exp (v_{k -1 } + v_j)}{s^2}.

We notice the repeated elements and define a (K−1)(K-1)-dimensional diagonal matrix D\mathbf{D}, where Di,i=exp⁡(yi)s2\mathbf{D}_{i, i} = \frac{\exp (y_i)}{s^2}. We can now express the Jacobian matrix as

J=(D(−exp⁡(v1)…−exp⁡(vK−1)⋱−exp⁡(v1)…−exp⁡(vK−1))+sIK−1).\mathbf{J} = \left( \mathbf D \begin{pmatrix} - \exp (v_1) & \dots & - \exp (v_{K-1}) \\ \vdots & \ddots & \vdots \\ - \exp (v_1) & \dots & - \exp (v_{K-1}) \\ \end{pmatrix} + s\mathbf{I}_{K-1} \right).

We now define a (K−1)(K-1)-dimensional column vector c,ck=−exp⁡(vk)\mathbf{c}, c_k = -\exp (v_k) and a row vector r,rk=1\mathbf{r}, r_k = 1 and obtain J=D(cr+sIK−1)\mathbf{J} = \mathbf{D}\left(\mathbf{cr} + s\mathbf{I}_{K-1}\right).  
By the matrix determinant lemma, det⁡(cr+X)=det⁡(X)(1+rX−1c)\det(\mathbf{cr} + \mathbf{X}) =\det(\mathbf{X})( 1 + \mathbf{r}\mathbf{X}^{-1}\mathbf{c}), for any invertible matrix X\mathbf{X}. Since rc=∑i=1K−1(−exp⁡vi)=1−s\mathbf{rc} = \sum_{i=1}^{K - 1}(-\exp v_i) = 1 - s, we have:

det⁡(cr+sIK−1)=(1+1src)sK−1=(1+1−ss)sK−1=sK−2.\det( \mathbf{cr} + s\mathbf{I}_{K-1}) = \left(1 + \frac{1}{s}\mathbf{rc} \right) s^{K-1} = \left(1 + \frac{1 - s}{s} \right) s^{K-1} = s^{K - 2}.

Since det⁡(D)=exp⁡(∑i=1K−1yi)s2(K−1)\det (\mathbf{D}) = \frac{\exp (\sum_{i=1}^{K - 1} y_i)}{s^{2(K - 1)}}, we finally have

```latex
\begin{equation}
\det (\mathbf{J}) = \det (\mathbf{D}) \det (\mathbf{cr} + s\mathbf{I}_{K-1}) 
= \frac{\exp (\sum_{i=1}^{K - 1} v_i)}{s^{K - 1}}.
\label{eq:jacobian_bad}
\end{equation}
```

As a different approach, if we are willing to restrict our priors over the ordered simplex to Dirichlet distributions, we may employ the fact that if w∈(0,+∞)K,wi∼Γ(αi,1)\mathbf{w} \in (0, +\infty)^K, w_i \sim \Gamma(\alpha_i, 1) then w∑i=1Kwi∼Dirichlet(α)\frac{\mathbf{w}}{\sum_{i=1}^K w_i }\sim \text{Dirichlet}(\mathbf{\alpha}). So if we start with w\mathbf{w} positive ordered (a primitive in Stan), then x=w∑i=1Kwi\mathbf{x} = \frac{\mathbf{w}}{\sum_{i=1}^K w_i } will be Dirichlet distributed over OrdSimplexK\text{OrdSimplex}_K and no Jacobian adjustment is required. A downside of this approach is that the mapping is many-to-one and in models where x\mathbf{x} is tightly constrained by data, the implied geometry on w\mathbf{w} will likely pose difficulty for most samplers. This variant will be referred to as `gamma`.

At this point the interested reader is welcome to try to find issues with any of the above approaches.

## Testing with SBC

Whether the reader managed to find errors or not, we can use SBC to test all approaches. To run SBC we embed the ordered simplex into a simple model:

x∈OrdSimplex4,π(x)∝Dirichlet(2, 2, 2, 2)y∼Multinomial(10, x).\begin{aligned} \mathbf{x} &\in \text{OrdSimplex}_4, \quad \pi(\mathbf{x}) \propto \text{Dirichlet(2, 2, 2, 2)} \\ \mathbf{y} &\sim \text{Multinomial(10, x)}. \end{aligned}

Implementing the simulator code is straightforward: due to symmetry, we can sample x\mathbf{x} simply by ordering a sample from the unordered Dirichlet distribution. Both `min` and `gamma` variant show no problems in SBC and are indeed correct, but `softmax` exhibits issues. Figure [fig:histrealworldfig:hist_real_world](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#fig:hist_real_world) shows the evolution of the discrepancies. The problems are most quickly picked up by the first element of x\mathbf{x} and the log Dirichlet prior density. Although the problem is found relatively quickly with SBC, the bias in the inferences would likely not be noticed in an informal assessment of the model: the results are not completely wrong, just somewhat biased. The source of the issue is an off-by-one error in the exponent for ss in equation ([eq:jacobianbadeq:jacobian_bad](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#eq:jacobian_bad)); the correct Jacobian determinant is

det⁡(J)=det⁡(D)det⁡(cr+sIK−1)=exp⁡∑i=1K−1visK.\det (\mathbf{J}) = \det (\mathbf{D}) \det (\mathbf{cr} + s\mathbf{I}_{K-1}) = \frac{\exp \sum_{i=1}^{K - 1} v_i}{s^{K}}.

Indeed, if we correct the Jacobian, SBC passes.

![Evolution of the difference between the gamma statistic and threshold for rejecting uniformity at 5% for the incorrectly implemented \texttt{softmax} variant of an ordered simplex model. \texttt{log_lik} is the multinomial log likelihood of the data and \texttt{log_prior} is the log density of the prior Dirichlet distribution. Note the different horizontal axis between top row (quantities that detect the problem quickly) and bottom row (quantities that detect the problem slowly). The vertical red dashed line marks 400 simulations.](https://chatgpt.com/c/hist_softmax_bad.pdf)

## Remarks

The previous section showed the type of modeling problem where SBC is in our view the most useful: deriving and implementing the probabilistic program is relatively involved and offers plenty of opportunities for error, but building a simulator is straightforward. Jacobian adjustments for changes of variables are also in our experience one of the most confusing concepts to Stan users and SBC offers a good way to check if one’s reasoning is correct.

The examples in this section introduce several non-obvious conceptual questions: For `min` and `softmax` we compute the Jacobian only considering K−1K - 1 elements of the ordered simplex, even when the Dirichlet prior then acts on all elements. Is that correct? For `gamma`, will ordering w\mathbf{w} imply the correct ordered simplex distribution? Running SBC is then a useful (although not completely definitive) check that our reasoning is correct.

In this example, the log likelihood did not reveal the error quickly, showing that it is not a panacea, especially in cases where the problem lies with the prior. The log prior density seems potentially useful in this case as it shows the problem as quickly as the most problematic individual parameter.

Another lesson is that SBC is useful not only for testing a full model but also for testing components of a model in isolation, akin to unit tests in software engineering. Additionally, by running SBC, we get a simulation study for free: in the specific setup described by Equation (ordsimplexmodelord_simplex_model) (above), `min` is the most efficient in terms of effective sample size per second, followed by `gamma`. The correct version of `softmax` performs worst. The `softmax` variant also fails to converge in 6 of the 1000 simulations, while the other two are slightly more stable (convergence problems in 3 and 1 of the simulations respectively). Finally, our posterior uncertainty is large and the data do not really provide a lot of information about the parameter values. See the rendered output of the supplementary code for details.

# Conclusions

\label{sec:conclusions}

## Choosing test quantities for SBC

We have found that enriching the repertoire of test quantities used in SBC provides both qualitative and quantitative improvements to the ability of SBC to detect problems in Bayesian computation. For practical use of SBC in everyday model and algorithm development, we recommend to use by default the individual model parameters as test quantities as well as the joint likelihood of the data and potentially a small number of other quantities.

Individual parameters are recommended as they are always immediately available and are able to diagnose a large number of problems with a posterior approximation. Also, the parameters are themselves often of primary interest for inference, so it is desirable to check that their uncertainty is correctly calibrated.

The joint likelihood is a highly useful quantity to detect the types of problems discussed in [Section sec:numericalsec:numerical](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#sec:numerical) (especially ignoring data and incorrect correlations). In all of the cases presented in our simulations, the joint likelihood was able to detect the discrepancies and in many cases it was even able to detect them with the fewest simulations among all considered quantities. While, for some specific problems, we could find quantities that are more sensitive than the joint likelihood, none other was useful in _all_ cases. [Section sec:datadatadependentquantitiessec:data_data_dependent_quantities](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#sec:data_data_dependent_quantities) provides theoretical justification for why we could expect this to hold frequently and not only in the examples we discussed. We think this generality makes the joint likelihood a good default quantity to monitor in SBC. If not using all the data correctly is a potential issue (e.g., because the code handling the data is particularly complex), then adding selected likelihoods for subsets of the data might also be sensible.

As shown in [Section sec:realworldsec:real_world](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#sec:real_world), knowing where a potential problem lies can let us design more sensitive problem-specific checks (e.g., when we are not sure our prior density is correct, the log prior can be highly useful).  
It also makes sense to add test quantities tailored to the specific inferential goals we have built the model for (e.g., some specific model predictions). These quantities often let us implicitly check the correctness of parameter correlations or other dependency structures and safeguard the user against problems that they care about the most.  
If correlations or other dependencies in the posterior are directly of interest, then pairwise products or differences of the model parameters can also be sensible test quantities.

## Limitations

Although we have shown that SBC can in principle diagnose any problem, limitations for practical use remain. For nontrivial models, adding a finite number of test quantities cannot guard against all possible ways the SBC identity may be satisfied by an incorrect posterior. However, as we check more quantities, the potential counterexamples become contrived, hard to construct, and unlikely to be the result of an inadvertent bug in model or algorithm code. At the same time, adding more test quantities increases the risk of false SBC failures simply due to the number of tests performed (if no corrections for multiple comparisons are made for the SBC checks) or it may reduce the overall power of the check (if corrections for multiple comparisons are made), so choosing test quantities carefully remains important.

This problem could potentially be alleviated by improving our understanding of the expected dependency structure of different test quantities’ uniformity checks, letting us correct for multiple comparisons without loosing that much power. However, even similar test quantities can lead to in principle different SBC checks ([Section sec:monotonicsec:monotonic](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#sec:monotonic)). So any practical measure of dependency or orthogonality between test quantities would need to reflect not only existence of a difference, but also its magnitude. We leave that as future work. In practice, we have seen similarity in the degree of uniformity violation between different test quantities using the same inputs, making the need for multiple comparison correction less urgent.

Moreover, there are practical limitations imposed by the fact that we always have only limited computational resources for SBC: We can produce only a limited number of simulated datasets to fit the model on and only a limited number of posterior draws per fitted model. Both contribute to the stringency and precision of the uniformity test we can perform. The difference between continuous SBC and any practical implementation of sample SBC arises due to (a) approximating qϕ,f(x∣y)q_{\phi,f}(x | y) by Qϕ,f(⌊xM⌋ ∣ y)Q_{\phi,f}(\lfloor xM \rfloor \,|\,y), and (b) using finite number of simulations to assess uniformity of NtotalN_{\mathtt{total}}. In both cases, the underlying difference can be understood as estimating a CDF by an empirical CDF and should therefore have similar rate of decrease with more draws. This suggests that for a given computational budget a user is likely to obtain the highest sensitivity using the same order of magnitude of simulated datasets as posterior draws per dataset. However, in practice most algorithms incur a substantial cost in a warmup phase, before any samples can be extracted. We also want to assess that our fitting algorithm has converged for each dataset, which typically requires the equivalent of at least 100 independent posterior samples (as measured by effective sample size) to do that (e.g., to get a low R^\hat R statistic, as discussed by [@improvedrhat]). It is thus hard to get a speedup by reducing the number of posterior draws. Unless we can afford to run many thousands of simulations, we are also unlikely to benefit substantially from getting more than this minimal number of draws.

Additional test quantities do not help much with precision problems—if the posterior is close to correct, the test quantities will also be close to correct. Although in some cases, some test quantities can slightly increase the sensitivity of the check by combining multiple parameters, so small imprecisions in each of the parameters can get compounded (once again the nonlinearity of the likelihood seems to be at least sometimes useful in this regard).

## Implications for non-SBC checks

As a contribution to the broader discussion about validation of Bayesian computation, we show that SBC and the data-averaged posterior provide different checks, despite being repeatedly conflated in the literature (see [Section sec:examplessummarysec:examples_summary](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#sec:examples_summary)). We leave a more detailed comparison of SBC and data-averaged posterior as future work, although there are some tentative arguments to believe that SBC provides stricter checks.

SBC is not the only approach to validating Bayesian computation that relies on choosing specific test quantities—test quantities are fundamental to the methods of [@geweke_getting_2004], [@prangle_abc], [@gandy_unit_2021], and [@cockayne_testing_2022]. We suspect that many of the considerations regarding their choice for SBC are applicable also in these other approaches.

---

## Footnotes

1. The term in the literature is “simulation-based calibration”; here we have added the word “checking” to emphasize that these methods do not themselves produce calibration; rather, they measure departure from calibration. [↩](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#user-content-fnref-1)
    
2. The discussion can be found at [https://discourse.mc-stan.org/t/ordered-simplex-constraint-transform/24102](https://discourse.mc-stan.org/t/ordered-simplex-constraint-transform/24102). We thank Sean Pinkney, Bob Carpenter and Ben Goodrich for contributing to the discussion and suggesting solutions. [↩](https://chatgpt.com/c/6790f8b8-e7fc-8002-8eb4-aaab997a15b1#user-content-fnref-2)