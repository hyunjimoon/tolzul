https://arxiv.org/abs/2402.17930
using [cooperative language guided planning cld](https://claude.ai/chat/fd424eab-3f00-4823-8347-1ffa890e5208)

|Section/Subsection|🔐Research Question|🧱Literature Brick|🔑Key Message|📊Empirical Evidence|
|---|---|---|---|---|
|1. Introduction|How can assistive agents follow ambiguous instructions in a context-sensitive manner?|• Bayesian inverse planning [8, 57, 80]<br>• Rational speech act theory [19, 23, 67]<br>• Joint intentionality [66, 70, 78]<br>• Assistance games [18, 25]<br>• Reward learning [34, 44, 56]|🧍‍♀️Humans often give ambiguous instructions expecting that their actions or goals will disambiguate their intentions through pragmatic context. For effective 🌏human-AI coordination, machines must similarly interpret language in light of observed actions and goals.|None (conceptual introduction)|
|2. Cooperative Language-Guided Inverse Plan Search<br>2.1 Modeling cooperative action and communication|How can we model humans as cooperative planners who communicate joint plans to assistants?|• Joint intentionality [73]<br>• Real-time heuristic search [9, 38, 40]<br>• Cooperative planning [70, 78]<br>• Assistance games [18, 25]|🧍‍♀️→🧭 Humans can be modeled as computing a joint policy π for both agents, taking actions from this policy while communicating salient action subsets as instructions. Language generation is modeled as a structured process using LLMs to translate commands into natural language utterances.|Fig 1: CLIPS overview<br>Fig 2: Model architecture with generative process<br>Fig 2d: Command-utterance examples|
|2.2 Goal inference via inverse planning|How can an assistant infer goals from both actions and language?|• Inverse planning [63, 81]<br>• Bayesian inference<br>• Intervention calculus [53]|🧠An assistant should perform Bayesian inference over the human's goal and policy by conditioning on observed actions and instructions, while accounting for its own interventions in the environment through Pearl's do-calculus.|Algorithm 1: CLIPS belief initialization and update<br>Fig 2a-c: Graphical model and probabilistic programs|
|2.3 Pragmatic instruction following as goal assistance|How should an assistant act when uncertain about the human's goal?|• Statistical decision theory<br>• POMDP approximation [27, 45, 47]<br>• Expected utility minimization [61]|👓Given uncertainty over the human's goal, the assistant should select actions that minimize expected goal achievement cost across the distribution of likely goals, enabling pragmatic instruction following.|Algorithm 2: CLIPS QMDP assistance policy|
|3. Experiments<br>3.1-3.2 Configuration and Baselines|How does CLIPS compare to baselines on goal assistance tasks?|• Literal instruction following<br>• Unimodal inverse planning<br>• Multimodal LLMs|🧍‍♀️Comparing to: (1) unimodal inverse planning from only actions or only language, (2) literal instruction following without pragmatic context, and (3) GPT-4V as a multimodal LLM baseline.|Fig 3: VirtualHome example with ambiguous instruction<br>Table A1-A2: Dataset descriptions|
|3.3-3.5 Human judgments and Results|How well does CLIPS match human goal inferences and assistance judgments?|• Bayesian theory of mind<br>• Human evaluation methodology|🌏CLIPS significantly outperforms all baselines in goal inference accuracy, assistance precision/recall, plan efficiency, and correlation with human judgments (r=0.93 for goals, r=0.96 for assistance).|Table 1: Performance metrics<br>Fig 4: Qualitative examples showing CLIPS vs baselines<br>Fig C2: Human-model correlations|
|4. Related Work|How does CLIPS relate to existing approaches?|• BToM [6, 7]<br>• RSA theory [19, 23, 67]<br>• Multimodal goal inference<br>• Joint intentionality [73]<br>• Instruction following with LLMs|🗺️CLIPS integrates multiple research threads: Bayesian inverse planning, pragmatic language understanding, joint intentionality in cooperation, and LLM-based instruction following.|None (literature review)|
|5. Discussion and Future Work|What are the limitations and potential extensions of CLIPS?|• Sequential Monte Carlo [42, 86]<br>• Constrained LLM decoding [43, 76]<br>• Belief-space planning [47, 68]|🤜Future work could address computational challenges through more sophisticated sampling strategies, constrained LLM decoding for command inference, and belief-space planning for information gathering actions.|None (discussion)|

This framework demonstrates how CLIPS integrates Bayesian reasoning with language models to create assistive agents that can interpret ambiguous instructions in context. The key innovation is modeling humans as cooperative planners who communicate parts of their joint plans, then performing multimodal inference over goals from both actions and language. Unlike literal instruction followers or unimodal approaches, CLIPS can resolve ambiguities pragmatically by considering what would be most helpful given the inferred goal distribution, closely matching human assistance judgments.