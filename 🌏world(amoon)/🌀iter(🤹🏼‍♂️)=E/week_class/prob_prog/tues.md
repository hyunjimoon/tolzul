brain parallism, comsci, cognitive aspect
![[Pasted image 20230822130907.png]]
![[Pasted image 20230822130934.png]]
![[Pasted image 20230822130952.png]]
commercial drone market is growing fast (few settings where robotics meet real world; much faster than AV;  open-ended than warehouse ; real system than home robotics); thermal sensors (rescue, conservation, but have some worries) - weather like fog is uncertainty make 

SLAM: simulat.locailziation 
![[Pasted image 20230822131420.png]]![[Pasted image 20230822131426.png]]
what prob.interest can do (take drone - is actually in top right, but  ![[Pasted image 20230822131447.png]]![[Pasted image 20230822131537.png]]
rejuvanation - widening the (trigger in larger hypo space) than my motion prior would suggest ![[Pasted image 20230822131643.png]]
![[Pasted image 20230822132027.png]]
can trace (record of every stoch choice made)
hd is head direcation
![[Pasted image 20230822132114.png]]

fly the robot in the direction of control; start.dp (velocity controls), 
![[Pasted image 20230822132239.png]]
![[Pasted image 20230822132343.png]]
![[Pasted image 20230822132333.png]]
![[Pasted image 20230822132442.png]]

![[Pasted image 20230822132612.png]]
lager range scanner (construct one by computation, classical motion sensor - vision)
noisy sensors are roatating and uniforming sample from interval (by taking actual disctace from wall and adding randomness)
![[Pasted image 20230822132749.png]]


where robots were trying to go, not where it actually went 
![[Pasted image 20230822133009.png]]

gaussian sensors 
![[Pasted image 20230822133314.png]]
gen trace are (index of angle - key for distance
![[Pasted image 20230822133416.png]])
glue the sensor and (curly mode is defining initial) - sequence of pose in sensor readings
![[Pasted image 20230822133431.png]]

motion model ()
![[Pasted image 20230822133602.png]]
each choice is latent variable in probability model
because of taking out of concrete object in code (stochastic choices) - include deep learning ecology (because of modularity - infernce meta program)
able to scale logic that mathematics, robotics couldn't do (10 billion codes); mathematical objects
trace data structure 
![[Pasted image 20230822133532.png]]
only give idea (Q. Doesn't tell how to find it?)
![[Pasted image 20230822134045.png]]

Another big idea in gen (inf meta prog)

made it possible to use all that are seen as alternatives (inf lib for diff methods and its combination - even transformers and diffusion model's engine)![[Pasted image 20230822134228.png]]

simulating generative execution (update trace with small change and calculate = post-updating, evaluating gradients of score); incremental computation (efficiency updating output)
![[Pasted image 20230822134338.png]]

most scalable; evolving set of traces (updating them and how they fit locally, and get credit for those via weights (how close it to the actual posterior))
![[Pasted image 20230822134501.png]]
![[Pasted image 20230822134710.png]]

![[Pasted image 20230822134809.png]]
generate bunch of traces along with the trace and likelihood of any contstraints from the data (data via observation pass into `generate` ; partial trace which stores - can exact (rejection sample - much closer)
![[Pasted image 20230822135007.png]]
for shorter path, SIRI and rejection sampling workes well but
![[Pasted image 20230822135044.png]]
iterate over steps (reset weights at each time, for each particles, improve the particle by using stoch choice as if to perturb via gaussian centered on current trace - wiggle around) - design space of 

![[Pasted image 20230822135115.png]]
![[Pasted image 20230822135233.png]]
genjax (likelihood and hierarchical extension - don't give good output for clutter)
in graduate level, we learn how each SIRI, MCMC rej are instances of much larger algorithm
![[Pasted image 20230822135421.png]]

gen program that corrupts: forw gen + post (missing from fig)![[Pasted image 20230822135533.png]]

![[Pasted image 20230822135602.png]]
repetitions - not normalized and inconsistent repitition![[Pasted image 20230822135635.png]]
![[Pasted image 20230822135706.png]]
generate traces acc to ontology
![[Pasted image 20230822135744.png]]
90% of imputation were correct (97% were real fixes)

![[Pasted image 20230822135813.png]]structure of domain (link str)+ skeleton 
![[Pasted image 20230822135945.png]]
![[Pasted image 20230822140000.png]]
![[Pasted image 20230822140009.png]]
addd in inference hings (data driven ) - let users specific hints (subproblem being ) - parameterize meta inference ; reconfigurable by end users (cusomized template)
![[Pasted image 20230822140014.png]]
![[Pasted image 20230822140135.png]]
- as worse as RL lol
data driven works much better 
![[Pasted image 20230822140201.png]]
compiles substructure (use exact inf algo tosolve problems)  - coupling ?? and ?? which is most 
![[Pasted image 20230822140225.png]]

two paths (forward) = working on the highltithted (blue rows - link - reconsidering in light of that context - to improve to fix mistakes)
![[Pasted image 20230822140326.png]]
![[Pasted image 20230822140439.png]]
- mutually help you (choose what variables to block over - mini batch (but have analogy))

![[Pasted image 20230822140446.png]]![[Pasted image 20230822140532.png]]
translate to discrete subproblem exactly
- top down (high accuracy that take account of current state)
- amortized (gen sim data from model and train nn treating all the latent var as labels; actual posterior of label - no gap)
- fast and modular (exact - for the parts of the problem that's affected); expoit ; wouldn't explore whole palce but explot gen model is symbolic and proceed or invalidate given new data 
- instead generate a label data than do (principles are superset of ; can use amortized in submodule)

- pclean is not only for data cleaning (massive spread of disimforable - confabulation from ; humand and machine generated confabulation)

![[Pasted image 20230822140924.png]]

ground truth, combined with smc (score unstructure text in terms of  - filter llm - check inclaims againt pclean)- making a factual claim if it a factural than what is the fact set that supports

possible to build, test design for immune system (claims and facuality and language model - audit trail - ppl make prob listic judgement)

- type of prob. (symbolic representation) - neural generative models

immune system (operate probablistically - no answer in ) - fundamental escape from immune ; risk award tradeoff - 
societal scale (additional what rights should be; shallow parts  consumption of speech - virally )

alternative to social media unregulative (genAI) - enormaous partnership btw government (painful for industry (facutality)- )
work towards step ; parse text how poetnetial factual it is (ind) then you can see what's the evidence of facts (entity that googles use ; structure boxes (that have lots of facts in it) pcleasns to )

very hard smart entrep (legal and ) - catalyze political will 

---
colab notebook to genjax (tutorials); two cells (google machine) - lab runs that (present with everything with no setup) 

![[Pasted image 20230822142229.png]]

explores hier.sensor model (data generated from the model) gpu accelarated model
one scale parallism (play with this notebook)
![[Pasted image 20230822142419.png]]
1. make a copy and run first two celss (configure gpu)

genjl tutorials (problem sets) - gen probgraph (including amortized data driven progposals )

![[Pasted image 20230822142530.png]]

tutorial https://github.com/probcomp/gen-quickstart
cloud will be coming 

access to now - down

> how does rejuvanation works?

clasic resample (after resampling when the weights are identically 1) - smc p3(del moraal ) - mc psuedo marginal (general way to improve the sample when decided to this )
https://proceedings.mlr.press/v206/lew23a/lew23a.pdf

----
![[Pasted image 20230824090611.png]]
depth 바닥, 벽 상자
![[Pasted image 20230824085618.png]]
relative location of box and wall is given (ray given)
slam (given map, use sensor (ray casting)) - gps 
localization의 관측값 (위치를 옮겨가면서, 전 단계 관측을 바탕으로 지도를 그려)
wall is blue, box is orange
inliner은 벽으로부터
![[Pasted image 20230824085757.png]]
there are two models constrained and baseline

![[Pasted image 20230824090505.png]]

Y는 deterministic ray operation,
X: sampled from operation / 
with P(X|Y), update sigma, omega 
- hd: heading (각도)
@ is for alias: python 의 x변수를 gen의 x 변수로 alias - to classify well in the trace 
![[Pasted image 20230824091648.png]]


tweaking with X @ sensor

![[Pasted image 20230824092028.png]]![[Pasted image 20230824092132.png]]
![[Pasted image 20230824092307.png]]
![[Pasted image 20230824092714.png]]

outlier을 배제하는게 목적, 벽상자서 반사된게 들어오는게

센서모델에 대해 얼마나 가능한 데이터가 나오는지 (outlier면 비율이 낮게 나옴 - importance sampling했을때 - 가능도가 희박)

- is it better to have more outlier (or not)
![[Pasted image 20230824094106.png]]
![[Pasted image 20230824094117.png]]
u is "key:PRNGkey"
model.importance(keys[0], ch0, ) 
second parameter is value to be fixed (chm: choice map as constraing)
![[Pasted image 20230824094646.png]]


![[Pasted image 20230824094849.png]]
mixture가 없는 ch1 outlier = 0 has a higher likelihood
![[Pasted image 20230824094956.png]]
vmap의 인자는 어떤 input기준으로 벡터화시킬지 를 결정하는 인자가 있다 (sig, out 기준 벡터화)

![[Pasted image 20230824095718.png]]

x,p, sigma, omega, Y

1. Y-> omega, sigma
2. X-> omega, sigma
![[Pasted image 20230824100259.png]]
lru_cache (simulation)
함수를 function_cache (함수를 불렀을때 값을 캐시화)

instruction, 함수값도 캐시화 (cpu구조적으로 불가 , byte code interpreter - cpu는 파이썬의 실행파일을 캐시)
lru_cache는 vm 

cpu에서 실행되는 어셈블리어 (cpu)


![[Pasted image 20230824101457.png]]![[Pasted image 20230824101515.png]]

Y를 가지고 와서, 센서모수를 확인해야 (추론), X뽑을수 잇음 (보정된 센서값; Y가 주어졌을때 X, sigma추론) - pose추론 (보정된 센서로 pose추론)
![[Pasted image 20230824095237.png]]