Abstract In many learning or inference tasks human behavior approximates that of a Bayesian ideal observer, suggesting that, at some level, cognition can be described as Bayesian inference. However, a number of Ô¨Åndings have highlighted an intriguing mismatch between human behavior and standard assumptions about optimality: People often appear to make decisions based on just one or a few samples from the appropriate posterior probability distribution, rather than using the full distribution. Although sampling-based approximations are a common way to implement Bayesian inference, the very limited numbers of samples often used by humans seem insufÔ¨Åcient to approximate the required probability distributions very accurately. Here, we consider this discrepancy in the broader framework of statistical decision theory, and ask: If people are making decisions based on samples‚Äîbut as samples are costly‚Äîhow many samples should people use to optimize their total expected or worst-case reward over a large number of decisions? We Ô¨Ånd that under reasonable assumptions about the time costs of sampling, making many quick but locally suboptimal decisions based on very few samples may be the globally optimal strategy over long periods. These results help to reconcile a large body of work showing sampling-based or probability matching behavior with the hypothesis that human cognition can be understood in Bayesian terms, and they suggest promising future directions for studies of resource-constrained cognition.

Keywords: Bayesian; Computational; Sampling; Inference; Bounded rationality

[[üß†9.66 comp.cog.sci_one_done.txt]] is how josh introduce this paper.

| Section/Subsection                                                                                                                                                              | üîêResearch Question                                                                               | üß±Literature Brick                                                                                                                                                                                                                                                                           | üîëKey Message                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Figure                                                                                                                                                                               |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 1. Introduction                                                                                                                                                                 | When and why do humans make decisions based on very few samples despite statistical inefficiency? | ‚Ä¢ Bayesian cognitive models showing humans approximate optimal inference on average<br>‚Ä¢ Individual trials show probability matching suggesting few samples used                                                                                                                             | Framework needed to explain why few samples might be optimal when considering time costs                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Fig 1: Human categorization vs Bayesian aggregates<br>![[Pasted image 20241106170516.png\|100]]                                                                                      |
| 2. Ideal aggregates from sampling behavior<br>2.1 Category learning example<br>2.2 World knowledge example                                                                      | How can individual probability matching produce ideal Bayesian aggregates?                        | ‚Ä¢ Goodman et al (2008): Category learning data<br>‚Ä¢ Griffiths & Tenenbaum (2006): Everyday predictions<br>‚Ä¢ Research on probability matching                                                                                                                                                 | Individuals sampling from posterior can produce optimal aggregate results despite using few samples                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Fig 2: QQ plots of predictions vs posterior<br>![[Pasted image 20241106170812.png\|100]]                                                                                             |
| 3. Approximating Bayesian inference<br>3.1 Sampling algorithms<br>3.2 What is a sample?                                                                                         | How can few samples implement approximate Bayesian inference?                                     | ‚Ä¢ Monte Carlo methods literature<br>‚Ä¢ Work on particle filtering and MCMC<br>‚Ä¢ Research on approximate inference                                                                                                                                                                             | Simple sampling strategies can work well for decisions even if not for accurate distribution estimation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | None                                                                                                                                                                                 |
| 4. Two-alternative decisions<br>4.1 Bayesian vs sample-based agents<br>4.2 Good decisions from few samples<br>4.3 How many samples for a decision?<br>4.4 Optimal SPRT policies | What's optimal number of samples for binary choices given time costs?                             | ‚Ä¢ Statistical decision theory<br>‚Ä¢ Sequential sampling models<br>‚Ä¢ Speed-accuracy tradeoff literature<br>‚Ä¢ sequential probability ratio test (SPRT) policy (Wald, 1947)<br>‚Ä¢ Accumulator policy (Vickers, 1979) draws samples until a threshold number of them favor one of the two options. | **Decision Structure:**<br>-  two alternative forced-choice (2AFC) tasks reduce complex situations to binary choices (e.g., bridge vs tunnel)<br>- One action is "correct" for each world state (i.e. correct in the sense that it has higher utility; there are two possible values for U(A; S) and only one action for each state receives the higher value)<br>- Posterior collapses to Bernoulli distribution with parameter p<br><br>**Agent Behavior:**<br>- Sample-based agent chooses majority action from samples<br>- Correct with probability qp + (1-q)(1-p)<br>- Can use fixed samples, SPRT policy (threshold-based), or accumulator policy (favored-option threshold)<br><br>**Key Finding:**<br>One or few samples can be globally optimal when considering time costs vs accuracy | Fig 3-4: Error rates and gains from sampling<br><br>![[Pasted image 20241106171631.png\|100]]<br>Fig 5-7: Expected utility analysis<br><br>![[Pasted image 20241106172943.png\|100]] |
| 5. N-alternative decisions                                                                                                                                                      | How does optimal sampling change with more alternatives?                                          | ‚Ä¢ Hick-Hyman law research<br>‚Ä¢ Multi-alternative decision models                                                                                                                                                                                                                             | Optimal samples increase logarithmically with alternatives while accuracy drops                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Fig 8-9: N-AFC analysis<br>Fig 10: Hick's law emergence                                                                                                                              |
| 6. Continuous decisions<br>6.1 Making continuous decisions<br>6.2 How many samples needed?                                                                                      | How do sampling strategies extend to continuous choice spaces?                                    | ‚Ä¢ Continuous optimization literature<br>‚Ä¢ Research on maximum local mass utility                                                                                                                                                                                                             | Sampling approach generalizes to continuous cases with similar few-sample benefits                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Fig 11-13: Continuous decision analysis                                                                                                                                              |
| 7. Strategic adjustment<br>7.1 Empirical evidence<br>7.2 Probability matching                                                                                                   | Do people adaptively adjust their sampling based on stakes?                                       | ‚Ä¢ Research on probability matching<br>‚Ä¢ Studies of strategic resource allocation                                                                                                                                                                                                             | People use more samples when stakes are higher, matching theoretical predictions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Fig 14: Stakes vs samples correlation                                                                                                                                                |
| 8. Discussion<br>8.1 Related arguments<br>8.2 Sample cost<br>8.3 Reusing samples<br>8.4 Black swans<br>8.5 Limitations                                                          | What are broader implications and limitations of the sampling framework?                          | ‚Ä¢ Bounded rationality literature<br>‚Ä¢ Work on cognitive constraints<br>‚Ä¢ Research on rare events                                                                                                                                                                                             | Framework explains many empirical patterns but has important limitations and boundary conditions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | None                                                                                                                                                                                 |

