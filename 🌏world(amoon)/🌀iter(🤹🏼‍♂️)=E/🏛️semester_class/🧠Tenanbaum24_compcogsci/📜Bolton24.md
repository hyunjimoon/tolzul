2025-05-18
1. Builds a theoretical foundation
2. Signals contributions to specific literature
3. Motivates the modeling choices

#### Structural Analysis of Bolton’s Ten-Paragraph Introduction
using [gpt](https://chatgpt.com/c/68288db3-bf58-8002-b25c-2dcf3a79db56)
1. **Paragraph 1 – Broad Context and Research Objective:** The introduction opens by framing the general problem and theoretical context. It introduces _strategic experimentation_ as an extension of the classic two-armed bandit problem to a multi-agent setting, where each player can learn from others. This paragraph establishes the **theoretical foundation** by highlighting the key phenomena under study – notably the **free-rider problem** (information as a public good leading players to wait for others) and the countervailing **encouragement effect** (future experimentation by others incentivizing early experimentation). It signals the paper’s **contribution** by posing the central research question (how these effects impact equilibrium behavior) and states the _objective of the paper_, thereby motivating the specific modeling approach to come.
    
2. **Paragraph 2 – Baseline Model and Literature Basis:** This paragraph describes the fundamental model setup, grounding the study in known theory. It explains the game’s structure as a many-player, common-value variant of the continuous-time two-armed bandit model, referencing classical sources (e.g. work by Karatzas, Berry & Fristedt, etc.) to **connect to existing literature**. By outlining how each period players choose between a safe and a risky action (with known vs. unknown payoffs), it provides **theoretical foundation** details necessary to understand the problem. This linkage to a well-known single-agent bandit framework both situates the work in the literature and **motivates the modeling choices** (e.g. adopting a continuous-time bandit model) by showing it builds directly on established models.
    
3. **Paragraph 3 – Real-World Motivation and Simplification Rationale:** Here the authors broaden the perspective, discussing real-world scenarios of strategic experimentation (e.g. R&D races, new technology adoption, resource exploration). By listing such examples of **social learning situations**, the paragraph **motivates the importance of the model** – showing that the simplified game captures core features present in many complex settings. It justifies focusing on the _simplest form_ of the problem (fully observable actions and outcomes) as a stepping stone to understanding more complex cases. This rationale strengthens the **theoretical foundation** by arguing that, just as the single-agent bandit became a cornerstone of active learning theory, the multi-agent bandit will serve as a natural benchmark for multi-agent learning. In doing so, it implicitly **motivates the modeling choice** of analyzing the basic multi-player bandit: to gain insight before layering on additional complications.
    
4. **Paragraph 4 – Approach and Equilibrium Structure:** This paragraph transitions to the solution approach and main theoretical insight. It explains that the analysis will focus on symmetric Markov perfect equilibrium strategies (a clear **modeling choice** made for tractability and relevance). The authors announce that they find an equilibrium characterized by certain cutoff strategies (thresholds for the belief _p_ at which players switch experimentation behavior). By doing so, the paragraph **signals a key contribution**: the existence and characterization of a non-trivial equilibrium in the multi-agent experimentation game. It builds directly on the prior setup – given the context and motivation, now we learn how the authors tackle the problem analytically. This provides a **theoretical foundation** for the results by outlining the equilibrium concept and criteria that will be used.
    
5. **Paragraph 5 – Equilibrium Outcomes and Key Effects:** Expanding on the equilibrium, this section details the nature of the symmetric cutoff strategies and their implications. The authors describe how players experiment with the risky option above one belief threshold and stop below another, with experimentation increasing gradually as beliefs improve. This highlights how the **free-rider effect** leads to _less experimentation than socially optimal_ (players hold back, hoping others experiment) and how the equilibrium payoff for a representative player stays below the full-information benchmark. In other words, it connects the equilibrium outcome back to the central thesis about inefficiency due to free-riding. By quantifying these effects (e.g. showing equilibrium experimentation is lower than first-best, even as the number of players grows large), the paragraph provides strong support for the **central theoretical foundation**. It also **motivates the modeling approach** by demonstrating that the chosen framework can capture the tension between individual incentives and collective optimum – a primary contribution of the paper.
    
6. **Paragraph 6 – Methodological Choices and Comparative Statics:** The sixth paragraph justifies specific modeling decisions and showcases an important finding. The authors explain _why_ they adopted a continuous-time formulation – because it makes the mathematics tractable (yielding a simple Bellman equation) and enables rich comparative statics. They then signal a notable **contribution to literature**: a counter-intuitive comparative static result. For example, they mention that equilibrium payoffs **increase with the number of players**, illustrating the **encouragement effect** at work (more players can spur more experimentation by raising the value of information) – a non-obvious result since one might expect more free-riding with more players. By highlighting this finding, the introduction emphasizes how the model provides new insights, thereby reinforcing the **cohesiveness** of the argument (everything ties back to free-riding vs. encouragement). This paragraph thus connects a modeling choice (continuous time for analytic clarity) to the **theoretical contribution** (deriving new implications like the positive effect of additional players), further motivating the robustness of the approach.
    
7. **Paragraph 7 – Robustness and Alternative Modeling Considerations:** Anticipating potential questions, the authors discuss variations in the modeling approach here. They note that if players were constrained to _pure strategies_ in each period (i.e. no mixing or time-sharing between actions), a symmetric equilibrium would not exist – thus justifying their choice to allow mixed or “divisible” allocation of effort. They explain that introducing a public or private randomization device (players taking turns experimenting) yields outcomes equivalent to their continuous-time mixed strategy model. This paragraph is squarely about **motivating modeling choices**: it reassures the reader that the chosen model is general and flexible, and that alternative formulations align with it. By addressing these technical considerations, the authors strengthen the introduction’s rigor, ensuring the **theoretical foundation** is sound and that the contribution is not an artifact of an arbitrary assumption. It shows that the paper’s insights hold under various strategy implementations, thereby **validating the modeling strategy**.
    
8. **Paragraph 8 – Positioning Relative to Prior Literature:** Having described their approach and findings, the authors explicitly situate their work in the context of existing research. They point out that only one prior study (Smith 1991) considered a similar multi-agent experimentation framework, and even that work was limited (focused only on boundary cases of beliefs, without analyzing equilibrium strategies or efficiency). They also allude to other related papers dealing with learning or experimentation in different settings, noting that those address tangential issues. By doing this, the introduction **signals the contribution to the literature** clearly: it fills a gap by analyzing equilibrium behavior in a multi-agent bandit problem which had not been solved before. This paragraph thus cements the paper’s novelty and importance. It connects back to the **theoretical foundation** by acknowledging prior building blocks, and it motivates why the present model is needed – no existing work fully addresses the questions posed in Paragraph 1. The tone here is one of differentiation: it establishes how this paper advances the state of knowledge.
    
9. **Paragraph 9 – Outline of the Paper’s Structure:** In this brief segment, the authors provide a roadmap of the remainder of the paper. They enumerate each section and its purpose (e.g. Section 2 introduces the model, Section 3 derives the belief dynamics, Section 4 sets up the Bellman equation and best responses, Sections 5–7 characterize the equilibrium and comparative statics, Section 8 examines the pure-strategy discrete-time variant, Section 9 discusses an intuitive “taking turns” strategy, etc.). This **organizational overview** ensures the introduction is complete and transparent about the paper’s structure. While it doesn’t add new theory, it contributes to **clarity and cohesion** by showing how each upcoming part will build on the introduction’s claims. Including such a roadmap is a standard practice signaling rigor and helping readers navigate the paper’s content.
    
10. **Paragraph 10 – Emphasis on Intuition and Rigor:** The introduction concludes with a commentary on the paper’s expository style and rigor. The authors note that the paper is written in a relatively informal way, emphasizing the intuition behind results rather than heavy theorem-proof presentation. Lemmas and proofs are used mainly to organize the discussion, and the authors reassure readers that despite the informal style, the results are backed by formal mathematics (they even mention that a full rigorous treatment can be constructed using advanced stochastic calculus texts, and an earlier draft contains a mathematical appendix). This closing paragraph serves to **underscore the cohesiveness** and credibility of the work: it aligns the introduction’s approachable narrative with the underlying mathematical **rigor**. For the reader, this builds confidence that the theoretical foundation laid out is solid, and it clarifies the authors’ approach to theory development (focusing on insights while ensuring correctness in the background). In essence, it motivates the presentation style as a conscious choice to enhance understanding, without sacrificing the paper’s standards of proof.
    

## Dependency Graph of Introduction Flow

Each paragraph of the introduction logically leads to the next, forming a dependency chain that builds the argument step by step. The flow can be thought of as a directed graph where each node (paragraph) adds new layers while relying on the prior ones:

- **Paragraph 1 → Paragraph 2:** The broad problem posed in P1 (multi-agent experimentation and key effects) necessitates detailing the specific setup. Thus P2 follows by describing the base model and theoretical context, which directly addresses the scenario introduced in P1. (P2 depends on P1’s introduction of the problem and provides the groundwork to explore it further.)
    
- **Paragraph 2 → Paragraph 3:** After P2 establishes the model’s basic elements, P3 expands the context by giving real-world meaning and justification. P3 builds on the model from P2, arguing why analyzing that model is important. In the graph, P3 depends on P2’s setup (it references the same game and its features) and adds external motivation, which in turn reinforces the relevance of P1’s broad problem in practice.
    
- **Paragraph 3 → Paragraph 4:** With the problem contextualized and justified, P4 moves to how the authors will solve/analyze it. P4’s discussion of equilibrium strategies and approach is only sensible after P3 (and earlier) established what the game is and why it matters. Thus, P4 depends on the foundation laid by P2 and the importance highlighted by P3. The logical flow is that once we accept the model and its relevance, the next step is to find its solution; P4 initiates that.
    
- **Paragraph 4 → Paragraph 5:** Paragraph 4 announces an equilibrium exists and its general nature (cutoff strategies). Paragraph 5 then delves into the details and implications of that equilibrium. So P5 directly relies on P4 – it is effectively an expansion of the result introduced in P4. In a dependency graph, P5 is a child of P4, further explaining and drawing consequences (like inefficiency) from the equilibrium concept P4 presented.
    
- **Paragraph 5 → Paragraph 6:** After describing equilibrium outcomes in P5, P6 justifies the modeling method and highlights a particular result (comparative static) made possible by that method. P6 depends on P5’s results to some extent: it uses the fact that an equilibrium was characterized to discuss how one can compute things like payoffs as functions of number of players. Also, P6’s key insight (encouragement effect with more players) is a deeper analysis building on the equilibrium described in P5. Thus, the flow is P5’s results prompting P6’s methodological note and additional finding. P6 is also linked back to P4 (the approach) because it justifies the use of continuous time in deriving those results.
    
- **Paragraph 6 → Paragraph 7:** Paragraph 7 addresses an alternative scenario (pure strategies vs. mixing) and reaffirms the chosen approach. This comes logically after P6 (which touted the benefits of the chosen continuous-time approach). P7 depends on the equilibrium framework introduced earlier (P4/P5) and on the specific modeling choice context (P6), because it is essentially saying “given we chose this approach, here’s why that was necessary; if we tried a different approach, here’s what happens.” So P7 branches from the methodological node: it is prompted by the consideration of modeling choices in P6 and ties back to the equilibrium concept of P4/P5, ensuring no loose ends in model assumptions.
    
- **Paragraph 7 → Paragraph 8:** With the model and solution approach fully defended and characterized, P8 zooms out to compare with existing literature. This follows naturally: P7 concluded the discussion of internal consistency, so now P8 can place the contribution in a broader scholarly context. P8 depends on the understanding of what the paper accomplished (from P4–P7) in order to assert how it is novel relative to prior work. In the flow, P8 doesn’t introduce new theory; it reflects on how the chain up to P7 stands apart from or builds on earlier nodes (other works). So P8 is connected to all prior content, but especially needs P4–P7’s summary of contributions to make the comparison meaningful.
    
- **Paragraph 8 → Paragraph 9:** Once P8 has clarified the paper’s niche and novelty, P9 outlines the structure of the paper, which is a straightforward next step. The dependency here is more about rhetorical flow than content: having established the “what and why” of the research, the authors now explain “how the rest of the paper is organized.” P9 assumes the reader is convinced of the value of the study (by P1–P8) and now simply needs a guide to navigate the detailed sections. In a graph, P9 would depend on the completion of the argument in P1–P8 (since an outline is only useful after one knows what the paper is about).
    
- **Paragraph 9 → Paragraph 10:** The final paragraph (P10) is somewhat tangential to new content but still logically placed. After giving the roadmap in P9, the authors add a note about the writing style and rigor in P10. This can be seen as a parenthetical dependent on P9 in the sense that it’s the last piece of introductory material before diving into Section 2. P10 depends on the existence of a full paper (implied by P9’s outline) to assure the reader about how to read it. It also harkens back to earlier technical content (P4–P7) by promising that the intuitive exposition there is backed by formal proofs elsewhere. In the logical flow, P10 wraps up the introduction by ensuring the reader trusts the preceding chain. It does not lead to another paragraph (since it’s last), but it solidifies the entire introduction node as a self-contained module.
    

Overall, the dependency graph is essentially a linear progression **P1 → P2 → … → P10**, with each paragraph building directly on the previous ones. There are slight feedback links (for instance, P7 refers back to assumptions from P4, or P10 references results from P5–P6 in discussing rigor), but no section is isolated or out of order. Each node in this introduction graph adds a necessary layer: context → model → justification → approach → results → implications → literature → roadmap → caveats/clarity. This logical sequencing ensures that by the end of the introduction, the reader has been led through a coherent argument from a broad motivation to specific contributions and how they will be detailed in the paper.

## Cohesiveness of the Introduction

The introduction’s ten paragraphs form a tightly integrated whole, each reinforcing the central thesis of the paper. The **central thesis** can be summarized as: _analyzing the strategic experimentation problem in a multi-agent setting reveals a tension between free-riding (discouraging individual experimentation) and an encouragement effect (whereby the prospect of others’ future experiments incentivizes current experimentation), and this tension shapes the equilibrium outcomes_. Every paragraph of the introduction serves to support and elaborate this thesis in some way, contributing to a high level of cohesiveness:

- **Unified Focus:** From the very first paragraph, the reader is made aware that the paper’s focus is on the interplay of those two effects (free-rider vs. encouragement) in a multi-player learning game. Subsequent paragraphs **consistently keep this focus in view**. For example, when the model is introduced (P2–P3), the relevance of that model is directly tied to scenarios of learning and information sharing – implicitly pointing back to the notion of informational externalities (the root of free-riding) and the value of information (underpinning encouragement). When results are summarized (P4–P6), they explicitly mention the free-rider effect causing under-experimentation and the encouragement effect manifesting in equilibrium when more players are present. This repetition and development of the same key ideas ensure that each part of the introduction is aligned with the thesis. No paragraph ventures into an unrelated topic; even the literature review (P8) and methodological note (P7) are framed in terms of how this work addresses the core problem differently or more effectively than others.
    
- **Logical Transitions:** The cohesiveness is further strengthened by clear and logical transitions between paragraphs, as detailed in the dependency flow above. Each paragraph begins in a way that **connects to the previous content**, which prevents any jarring shifts that could undermine the unity of the introduction. For instance, Paragraph 3 starts by essentially saying, “this phenomenon we model appears in many situations,” linking back to the model description in Paragraph 2 and the importance from Paragraph 1. Paragraph 4 starts analyzing equilibrium strategies, clearly following the setup of P2–P3. These smooth transitions mean the introduction reads as a single narrative developing one argument rather than a patchwork of separate points. This narrative coherence makes the support for the central thesis cumulative – each paragraph builds on prior reasoning, so by the end, the reader sees a well-substantiated case for why the research is needed and what its insights are.
    
- **Support of the Thesis:** Each paragraph contributes a distinct type of support to the thesis:
    
    - Some provide **conceptual grounding** (P1, P2, P3) – they ensure the reader understands the problem and why it matters. This is crucial for the thesis because if the reader didn’t grasp the nature of the free-rider/encouragement tension (P1) or the baseline model that exhibits it (P2–P3), the rest of the argument would not hold together.
        
    - Others provide **evidence and claims** (P4, P5, P6) – by stating what the model finds, these paragraphs supply the “proof” of concept that the thesis is addressing. P5 and P6, in particular, show that the two effects indeed emerge in equilibrium (free-riding causes inefficiency; encouragement can offset it when more players join), directly supporting the thesis’s claims about those effects.
        
    - Another set provides **context and differentiation** (P7, P8) – ensuring that the thesis stands up to scrutiny. P7 removes doubts that the findings might be artifacts of an arbitrary modeling choice, and P8 shows that no one else has already solved this thesis in the literature. This means the thesis is not only demonstrated but is also _necessary and novel_, adding to the overall persuasiveness.
        
    - Finally, P9 and P10 provide **clarity and credibility** – while they don’t add new facts about the research question, they ensure the reader can follow the upcoming argument and trust its integrity. An outline (P9) helps maintain cohesion by telling readers how the thesis will be explored in the paper’s body, and the note on rigor (P10) reassures that the seemingly narrative approach has a solid backbone. These elements indirectly support the thesis by keeping the reader engaged and confident that the argument is well-founded.
        
- **Consistent Emphasis:** Throughout the introduction, key terms and themes are repeated in a purposeful way. The phrases “experimentation”, “information”, “free-rider”, “encouragement”, and “equilibrium” appear multiple times. This deliberate reiteration acts as a glue holding the paragraphs together on the same topic. For example, the free-rider problem is mentioned early on, and later paragraphs circle back to it when discussing results and related work. This consistency in terminology ensures that the central thesis is never out of sight; the reader is continuously reminded of the core focus.
    

In summary, the introduction is highly cohesive because **every paragraph is aligned with the central thesis and contributes a necessary piece of the overall argument**. The theoretical motivation, literature positioning, model description, and summary of findings all revolve around the same core issue. There are no extraneous digressions – each part either sets up a facet of the problem or provides a facet of the solution. The result is that by the end of the ten paragraphs, the reader has a clear understanding of what question the paper addresses, why it’s important, what approach will be used to answer it, and how it advances knowledge. This unity of purpose and clarity of progression exemplify a well-crafted introduction in an academic paper.

## Lessons for a Bayesian Entrepreneurship Introduction

The structured approach seen in Bolton’s introduction provides a blueprint that a scholar in _Bayesian entrepreneurship_ (or any rigorous academic domain) can emulate. Bayesian entrepreneurship research often deals with how entrepreneurs and investors update their beliefs (Bayesian learning) through experiments or staged investments, which is analogous in many ways to the strategic learning in Bolton’s paper. To achieve similar standards of rigor and clarity, an introduction in this field could adopt the following structural elements:

- **Start with a Broad Motivation and Key Question:** Begin the introduction by clearly identifying the big-picture problem or puzzle in entrepreneurship that your paper addresses. For example, you might open with the observation that _entrepreneurial finance involves a dynamic learning process_ – entrepreneurs must persuade investors by experimenting or generating evidence over time. Pose the critical questions that arise (e.g. “How do iterative learning and belief-updating impact an investor’s willingness to fund a startup?” or “Why might entrepreneurs in certain settings face greater difficulty in securing funding despite optimistic initial beliefs?”). This is analogous to how Bolton opened with the free-rider vs. encouragement dilemma in multi-agent experiments. The goal is to **hook the reader with a clear, important problem** and signal the theoretical lens (here, Bayesian learning) from the outset.
    
- **Build on Theoretical Foundations and Prior Literature:** After setting the stage, provide the necessary background that grounds your question in existing knowledge. In a Bayesian entrepreneurship paper, this could involve explaining the baseline framework you’re building on – for instance, a well-known model of sequential investment or a reference to the _learning perspective on multi-stage financing_. Cite key literature that has addressed related aspects (perhaps prior studies of entrepreneurial learning, experiments in startup contexts, or classical models of investment under uncertainty). This part should play a role similar to Paragraph 2 (and parts of P3 and P8 in Bolton’s intro): **establish what is already known and where the gaps are**. By doing so, you demonstrate both your command of the literature and the theoretical foundation your work will rest upon. For instance, you might reference a recent overview like Agrawal et al. (2024) on Bayesian entrepreneurship to show that scholars are calling for such an approach.
    
- **Identify the Gap and Introduce Your Contribution:** Clearly articulate how your paper will add to the literature or address a gap. Once the context is laid out, dedicate a paragraph to pinpointing what existing studies have not yet explained or what is novel about your angle. In Bolton’s case, this was the multi-agent aspect and equilibrium behavior that prior single-agent models or partial analyses (like Smith 1991) didn’t cover. In Bayesian entrepreneurship, your gap could be something like: “While previous work has recognized that entrepreneurs learn from market feedback, no model to date has examined how this Bayesian updating process affects the likelihood of securing _future_ funding rounds under varying conditions of uncertainty.” State your **central thesis or main proposition** here – essentially the answer your introduction is working towards. By doing so, you signal the **contribution to existing literature** early and prepare the reader to appreciate why your modeling choices matter. This is the “what we do and find” moment of the introduction.
    
- **Motivate Your Modeling Choices and Approach:** Once the reader knows what you’re trying to accomplish, explain _how_ you plan to tackle the problem, and why that method is appropriate. If you are introducing a theoretical model (perhaps a Bayesian updating model or a simulation of investor-entrepreneur interactions), justify its structure in intuitive terms. For example, “In our model, an entrepreneur faces multiple potential funding rounds, updating their venture’s success probability based on sequential experiments (signals) – a setup that mirrors real startup investment stages.” If you make particular assumptions (e.g. risk-neutral investors, or a certain prior distribution for beliefs), briefly explain why these assumptions make sense or are needed (much like Bolton & Harris justified using continuous time and symmetric Markov strategies for tractability). This part of the introduction ensures the reader understands the **logical reasoning behind the model design**. It’s also wise to mention any noteworthy methodological aspect upfront – for instance, “we adopt a Bayesian reinforcement learning framework to solve for the equilibrium investment strategy – something that allows analytical insights where a purely empirical approach would fall short.” By framing your approach in this way, you mirror Bolton’s strategy of **justifying how the model is solved or analyzed** and setting expectations for the nature of results.
    
- **Preview the Main Findings or Theoretical Insights:** Just as Bolton’s introduction summarizes the equilibrium results (free-riding causes underinvestment in experimentation, encouragement effect can counteract it, etc.), you should give the reader a taste of your key findings. In a Bayesian entrepreneurship context, this might read like: “Our analysis reveals that early-stage investors optimally adopt a ‘skeptical’ stance – requiring significantly positive signals before increasing their investment – which can lead to an under-investment in high-potential ideas (an analog to a free-rider problem). However, we also find an _encouragement effect_: the prospect of follow-on investors in later rounds can induce current investors to finance experiments they otherwise wouldn’t, thereby accelerating learning in thriving ventures.” Such a summary (tailored to whatever your actual results are) does two things: (1) it directly **supports your central thesis** by stating the answers or insights your thesis promised, and (2) it creates curiosity and understanding in the reader. They now have a framework to interpret the detailed results when they encounter them in the paper. The key is to keep this preview at a high level, focusing on intuition and implications rather than technical detail – Bolton’s intro, for instance, used plain language for results (increasing players raises payoffs, equilibrium effort is below social optimum, etc.) without delving into equations.
    
- **Relate Findings Back to the Literature and Significance:** After presenting your core insights, connect back to the broader picture. Explain how these findings contribute to or challenge the existing literature you mentioned earlier. For example, if prior empirical studies observed that certain regions or industries have funding gaps, you might say how your model provides a theoretical explanation for that (perhaps high uncertainty industries suffer more from the investor’s need for strong signals, etc.). This is akin to Bolton’s paragraph that situates their results relative to Smith (1991) and others – it tells the reader _why it matters_ in context. In doing so, make sure to underscore the rigor and uniqueness of your contribution: perhaps your model could be the first to unify two previously separate perspectives (say, combining an experimentation view with a market selection view in entrepreneurship). By the end of this part, the reader should be convinced that your work meaningfully advances understanding in the field.
    
- **Conclude with a Roadmap and Any Caveats:** Finally, ensure your introduction ends with clear guidance on what comes next (just as Bolton did with an organizational paragraph). A typical closing paragraph would outline the structure: e.g., “The remainder of this paper is organized as follows: Section 2 introduces the formal model of staged investment and belief updating; Section 3 derives the investors’ optimal strategies and belief dynamics; Section 4 discusses the resulting implications for venture success and failure; Section 5 relates the model predictions to existing empirical regularities; Section 6 concludes with implications for entrepreneurs and investors.” This roadmap helps readers navigate your paper and reinforces the **logical flow** of your argument. If there are any special notes (perhaps you, too, choose to emphasize intuition and defer technical proofs to appendices, or you rely on simulations for parts of the analysis), you can briefly mention that as well, to manage expectations. The tone should be confident and clear, highlighting that you have a handle on both the content and presentation of the paper.
    

By following a structure that mirrors Bolton’s introduction – **context → literature foundation → gap → approach → findings → implications → outline** – a scholar in Bayesian entrepreneurship will ensure their introduction is comprehensive and rigorous. Each paragraph has a job to do, and together they provide the reader with a self-contained narrative of why the paper exists and what it contributes. Adopting this approach leads to an introduction that not only meets high standards of clarity (making it easy for others to grasp the importance and logic of your work) but also demonstrates scholarly rigor (by situating the work in literature and explaining methodological choices).

Ultimately, the lesson is that a well-structured introduction is like a **roadmap to a research project**: it should guide the reader through the terrain of the problem, show where the new path diverges from existing ones, and point out the destination that the paper will reach. In Bayesian entrepreneurship research – where combining Bayesian logic with entrepreneurial phenomena can be complex – this structured clarity is especially valuable. It ensures that your innovative modeling of how entrepreneurs and investors learn will be understood, appreciated, and seen as credible by your audience, much as Bolton’s introduction set up his theoretical contribution for maximum impact.

---

2025-05-14
- using [reformatting data to answer prompt cld](https://claude.ai/chat/3f600e2a-40a0-4e98-bcd3-28489a424a51)

| Bolton et al. Concept           | Bolton Variable | Moon Concept                | Moon Variable                                                                                     |
| ------------------------------- | --------------- | --------------------------- | ------------------------------------------------------------------------------------------------- |
| Venture value if successful     | $V$             | Venture value               | <span style="color:orange">$f_{js}$</span> (stakeholder state values)                             |
| Prior probability of success    | $p_0$           | Prior probability           | <span style="color:blue">$\vec{p}_j = (p_{j1}(x),\ldots,p_{jS}(x))$</span> (choice probabilities) |
| Cost of experiment              | $C$             | Cost coefficient            | <span style="color:cyan">$c_j$</span>                                                             |
| Cost of full development        | $K$             | Budget constraint           | <span style="color:cyan">$R$</span> (total budget)                                                |
| Experiment specificity          | $s_1$           | Venture attributes          | <span style="color:gray">$x$</span> (affects choice probabilities)                                |
| Experiment sensitivity          | $s_2$           | Stakeholder preferences     | <span style="color:gray">$\beta_{js}$</span> (affects choice probabilities)                       |
| Entrepreneur's private benefit  | $Z$             | Not explicitly modeled      | N/A                                                                                               |
| Investor ownership stake        | $\alpha$        | Not explicitly modeled      | N/A                                                                                               |
| Expected payoff from experiment | $\pi_{s_1,s_2}$ | Uncertainty                 | <span style="color:blue">$H(\vec{p}_j)$</span>                                                    |
| Proof-of-failure payment        | $X$             | Not explicitly modeled      | N/A                                                                                               |
| Not explicitly modeled          | N/A             | Actions                     | <span style="color:red">$a_j$</span> (weeks)                                                      |
| Not explicitly modeled          | N/A             | Optimal decision            | <span style="color:red">$a^*_j$</span> (weeks)                                                    |
| Not explicitly modeled          | N/A             | Threshold targets           | <span style="color:orange">$\mu_j$</span>                                                         |
| Not explicitly modeled          | N/A             | Dual variable for threshold | <span style="color:orange">$\lambda_j$</span>                                                     |
| Not explicitly modeled          | N/A             | Dual variable for resource  | <span style="color:cyan">$\gamma$</span>                                                          |
[[📜Bolton24]]



- using [Poster for Moral Hazard in Innovation Financing cld](https://claude.ai/chat/60d3dee3-55a1-4231-96c8-e777a80a674b) 


2025-03-10

U_I = − [p₀s₁ + (1−p₀)(1−s₂)]K − C

U_S = p₀s₁(1−α)V + [p₀s₁ + (1−p₀)(1−s₂)]Z + Z (elezabeth holmes style)

scientist should be paid for disproving the technology

| Section/Subsection                                 | 🔐Research Question                                                                        | 🧱Literature Brick                                                                                                                                                                                                                           | 🔑Key Message                                                                                                                                                                             | 📊Empirical Evidence/📐Mathematical Formalization                                                                                                                                                              |
| -------------------------------------------------- | ------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1. Introduction                                    | Why are early experiments poor at identifying whether technologies will work at scale?     | • High failure rates in pharmaceutical development (Paul et al. 2010)<br>• Similar poor predictability across industries (Siegmund et al. 2021; Greenwood et al. 2022)                                                                       | 🧍‍♀️When entrepreneurs control experiment design but investors can't verify design quality, a novel form of moral hazard emerges that impedes learning                                   | • <10% of molecules identified in preclinical stage progress to launch<br>• Billions invested annually in projects that ultimately fail                                                                        |
| 2. Relation to Literature                          | How does this moral hazard differ from other agency problems in innovation financing?      | • Principal-agent framework in innovation (Aghion & Bolton 1992; Hellmann 1998)<br>• Discretion to influence learning (Bergemann & Hege 1998; Cornelli & Yosha 2003)<br>• Entrepreneurial experimentation literature (Gans, Stern & Wu 2019) | 🧭Unlike standard agency problems resolved through incentive alignment, experiment design moral hazard cannot be mitigated through typical "skin-in-the-game" contracts                   | • Standard models focus on diverting funds/effort or window-dressing signals<br>• This model focuses on manipulation of learning technology itself                                                             |
| 3. Model: Basic Setup                              | How can we model experiment design choices in a principal-agent framework?                 | • Medical diagnostic models (sensitivity & specificity)<br>• Multitasking principal-agent problems (Holmstrom & Milgrom 1991)                                                                                                                | 🌏Experiments characterized by sensitivity (true negative rate) and specificity (true positive rate), which entrepreneurs can manipulate                                                  | • Two-state venture outcome (success/failure)<br>• Experiments produce binary signal (pass/fail)<br>• P(s=P\|v=V) = s₁ (specificity)<br>• P(s=F\|v=0) = s₂ (sensitivity)                                       |
| 3.1-3.2 Experiment Structure                       | What is the value of experiments with varying levels of informativeness?                   | • Research on experiment design<br>• Statistical decision theory                                                                                                                                                                             | 🗺️With higher specificity and sensitivity, experiments generate more value from information discovery and reduce need for costly development of unviable ventures                        | • Expected payoff: πs₁,s₂ = p₀s₁V − [p₀s₁ + (1−p₀)(1−s₂)]K − C<br>• Value increases with both s₁ and s₂                                                                                                        |
| 3.3 Solving for Independent Tasks                  | Why do entrepreneurs and investors prefer different experiment designs?                    | • Agency costs in learning<br>• Decision theory with imperfect information                                                                                                                                                                   | 🧠Entrepreneurs prefer experiments with higher false positives (maximizing continuation probability) while investors prefer "killer experiments" that accurately identify failures        | • Entrepreneur maximizes: p₀s₁(1−α)V + [p₀s₁ + (1−p₀)(1−s₂)]Z + Z<br>• Investor maximizes: p₀s₁αV − [p₀s₁ + (1−p₀)(1−s₂)]K − C<br>• Entrepreneur always chooses s₁=s̄₁, s₂=s_₂ regardless of incentives        |
| 3.4-3.5 Solving for Substitute/Complementary Tasks | How does relationship between sensitivity and specificity affect the moral hazard problem? | • Multitasking literature<br>• Research on experimental design tradeoffs                                                                                                                                                                     | 👓Depending on experiment technology, entrepreneurs might face tradeoffs between sensitivity and specificity, making problem worse when they are substitutes                              | • With perfect substitutes (s₁+s₂=κ), conflict is maximized<br>• With perfect complements (s₂=λs₁), higher incentives can align objectives if p₀>λ(1−p₀)                                                       |
| 4.1-4.2 Market Failure and Inefficiency            | What market inefficiencies arise from this moral hazard?                                   | • Market failure literature<br>• Theory of incomplete contracting                                                                                                                                                                            | 🌏Market completely fails when range of possible sensitivity is too large; otherwise ventures get funded but with inefficiently designed experiments                                      | • Market failure if p₀s̄₁V − [p₀s̄₁ + (1−p₀)(1−s_₂)]K − C < 0<br>• Inefficiency cost proportional to sensitivity range Δs₂<br>• Exists threshold Δs₂* above which market fails                                 |
| 5. Paying for Failed Tests                         | Can contracts resolve the moral hazard by incentivizing failure identification?            | • Research on incentive design<br>• Contracts under asymmetric information                                                                                                                                                                   | 🤜Rewarding "proof of failure" can align incentives, but solution is fragile and requires precise calibration to entrepreneur's private benefits                                          | • Optimal payment X ≥ Z to incentivize maximum informativeness<br>• If X too high, creates reverse incentive for false negatives<br>• Viability depends on relationship between sensitivity/specificity        |
| 6. Policy Responses                                | What institutional arrangements might overcome this friction?                              | • Institutional solutions to market failures<br>• Third-party validation literature                                                                                                                                                          | 🧭Universities can play crucial role as intermediaries validating experiment design quality, enabling financing for deep tech ventures                                                    | • Universities have expertise, equipment, and reputation incentives to maintain credible validation processes<br>• Value of validated general purpose technologies increases with number of dependent projects |
| 7. Conclusion                                      | What are the implications for financing science-based innovation?                          | • Innovation financing literature<br>• Work on deep tech venture challenges                                                                                                                                                                  | 🔑Novel moral hazard in experiment design explains both poor predictability of early experiments and lack of funding for deep tech ventures, requiring new approaches to incentive design | • Standard "skin in the game" incentives cannot resolve this moral hazard<br>• Institutional solutions like university validation may be necessary                                                             |


---

# Moral Hazard in Experiment Design – Analytical Responses (Bolton et al. 2024)

## 1. Nature of the Moral Hazard and Incentive Misalignment

Bolton et al. (2024) model a **hidden-action moral hazard** in experiment design. The entrepreneur (agent) privately chooses the design of an experiment – characterized by its **specificity** and **sensitivity** – which determines how informative the experimental outcome will be. The investor (principal) observes only whether the experiment yields a “pass” or “fail” result, not the experiment’s internal parameters. This information asymmetry means the entrepreneur can **manipulate the experiment’s informativeness**, affecting the likelihood of false positives/negatives without the investor’s knowledge. In essence, the entrepreneur controls the test’s probability of correctly identifying a viable project versus weeding out an unviable one, and cannot commit to a more informative “killer experiment” ex ante. This is the crux of the moral hazard: the entrepreneur’s hidden action (experiment design) directly impacts the informativeness of the outcome and thus the course of the venture.

The **incentives diverge fundamentally** between the investor and the entrepreneur. The investor cares only about the venture’s **net present value (NPV)** and wants to avoid “throwing good money after bad”. Formally, if the prior probability of success is $p_0$ and $V$ is the payoff if successful (with additional scale-up cost $K$ after a pass), the investor’s expected profit from funding an experiment with design $(s_1,s_2)$ is:

π(s1,s2)=p0 s1 V  −  [ p0 s1+(1−p0)(1−s2) ] K  −  C,\pi(s_1,s_2) = p_0\,s_1\,V \;-\; [\,p_0\,s_1 + (1-p_0)(1-s_2)\,]\,K \;-\; C,

where $s_1$ is the probability of a _pass_ if the project is truly viable (true-positive rate) and $s_2$ is the probability of a _fail_ if the project is truly worthless (true-negative rate), and $C$ is the experiment’s cost. The term in brackets $[p_0 s_1 + (1-p_0)(1-s_2)]$ is the overall probability of a **“pass” outcome**, which would trigger the $K$ investment. The investor’s payoff $\pi$ increases with _higher $s_1$ and $s_2$_ – fewer false negatives and fewer false positives – so the investor optimally prefers a **“killer experiment”** that maximizes both specificity and sensitivity (ideally $s_1=1$, $s_2=1$). Such a test gives the most reliable signal, ensuring funding only goes to truly viable projects. By contrast, the entrepreneur values **continuation** of the venture itself. He derives private benefits $Z$ from working on the project (advancing science, retaining control, etc.) and **bears none of the downside cost** of a failure since the funding is provided entirely by the investor. Thus, the entrepreneur’s objective is to **avoid early project termination**. He “does not want the investor to shut down the project prematurely” and prefers an experiment design that **maximizes the chance of a positive result**, even if that result is less informative. In other words, the entrepreneur favors a test that is **“less likely to fail”** (i.e. more likely to produce a pass signal), which means tolerating more false positives and fewer false negatives. Formally, Bolton et al. show the entrepreneur’s utility is **increasing in $s_1$** (he wants to capture any true success) and **decreasing in $s_2$** (he doesn’t want to correctly identify a failure). His optimal design therefore skews toward a high $s_1$ (to not miss a potentially viable project) and a low $s_2$ (to keep even a non-viable project alive longer). This conflict in objectives – informative experiment vs. permissive experiment – **is the essence of the moral hazard** identified in the model.

Traditional **“skin-in-the-game” incentives (equity ownership or pay-for-performance)** prove ineffective at resolving this moral hazard. In many principal–agent settings, giving the agent an ownership stake aligns incentives by making them bear part of the downside or share the upside. However, here the entrepreneur’s **upside is already sufficiently large** (the option value of continuing the project), and he **shares no downside** cost if the venture fails. Granting him more equity or performance pay **only increases his bias for continuation** by amplifying the reward if the project eventually succeeds. As Bolton et al. explain, “providing him more ‘skin in the game’ only makes the option more valuable” to the entrepreneur. Even if the entrepreneur’s stake $(1-\alpha)$ is increased, he only profits if the venture is developed and succeeds – which first requires _passing_ the experiment – so he remains driven to choose a test design that yields a pass signal. Crucially, because the entrepreneur **does not invest his own funds** into $K$, “no amount of skin in the game will undo this objective” of maximizing continuation. He faces no personal financial loss from a false positive, so high-powered equity incentives cannot make him act like the investor. Anticipating this, the investor’s optimal contract in the model actually involves **giving the entrepreneur _no_ equity stake ($\alpha=1$ for the investor)**. Standard pay-for-success contracts fail – the entrepreneur would still design the experiment to maximize his private benefit of continuation rather than the project’s true NPV. In fact, aligning incentives would paradoxically require _rewarding failure_: the only way to induce a more informative test is to pay the entrepreneur when the experiment yields a negative result (to offset his desire to avoid failure). Bolton et al. note that such a scheme (rewarding “proof of failure”) is extremely fragile: if the failure-payment is too low, it won’t change behavior; if it’s too high, the entrepreneur then tilts the experiment toward producing **false negatives** to collect the reward. In summary, the model’s moral hazard arises from the entrepreneur’s ability to secretly choose a low-information, “failure-avoiding” experiment design. His incentives to prolong the project diverge sharply from the investor’s desire to learn the truth quickly, and conventional incentive mechanisms like equity stakes cannot bridge this gap.

**Summary:** In Bolton et al.’s framework, the entrepreneur controls the experiment’s design (a hidden action) and so can influence the information revealed by the outcome. The **moral hazard** is that he will choose a less informative test that simply keeps the venture alive (maximizing the chance of a “pass” result), whereas the investor would prefer a highly informative **“killer experiment”** that exposes failure early. The entrepreneur’s private gain from continuation and lack of downside exposure drive this incentive misalignment. Critically, giving the entrepreneur a bigger equity stake or bonus for success **does not fix the problem** – it only heightens his preference for continuation, since he still doesn’t share the interim losses. Thus, unlike typical principal–agent problems, the **experiment-design moral hazard** cannot be resolved by ordinary high-powered incentives, which underscores the need for alternative contractual or institutional solutions.

## 2. Moral Hazard Under Independent vs. Substitutable vs. Complementary Experiment Attributes

Bolton et al. analyze three cases for how an experiment’s **specificity and sensitivity** can be controlled, and these cases alter the nature of the moral hazard. Formally, **specificity** $s_1$ is the probability the test returns a success signal _P_ given the project is truly viable (one minus the false-negative rate), and **sensitivity** $s_2$ is the probability the test returns a failure signal _F_ given the project is truly not viable (one minus the false-positive rate). Higher $s_1$ makes the test less likely to miss a good project, and higher $s_2$ makes it better at rejecting a bad project. The investor’s first-best “killer experiment” would maximize both $s_1$ and $s_2$, thus minimizing both type I and type II errors. The entrepreneur, however, will choose differently under each scenario of experimental control:

- **(i) Independent $s_1$ and $s_2$:** In this case, the entrepreneur can adjust specificity and sensitivity separately, without one constraint affecting the other. This is the baseline scenario, and it produces the _severest_ moral hazard. Since the entrepreneur’s utility increases in $s_1$ and decreases in $s_2$ (he wants to maximize the chance of continuation), he will choose the **most extreme, least informative design**: namely **$s_1$ as high as possible** and **$s_2$ as low as possible**. Intuitively, he makes the test very easy to pass – ensuring even marginal or unviable projects likely yield a “pass” – and avoids any design that would terminate the venture early. Bolton et al. confirm that under independent tasks the entrepreneur’s optimum is indeed $(s_1, s_2) = (s_1^{\max},,s_2^{\min})$ (the boundaries of the feasible set). The result is an experiment with many false positives (low $s_2$ means even an unworkable project often passes). The **moral hazard is most acute** here: the entrepreneur’s chosen test is maximally “inconclusive” from the investor’s perspective. If the investor funds under these conditions, she updates very little upon seeing a pass, and thus may end up investing $K$ in a worthless project. Indeed, Bolton et al. show the venture will **“fail more often than the first-best”** conditional on a pass, imposing an expected loss of $(1-p_0)\Delta s_2 K$ (where $\Delta s_2$ is the shortfall in sensitivity compared to the first-best). Anticipating this, the investor may sometimes refuse to fund at all. There is a critical condition under which **no funding (market failure)** occurs: if the entrepreneur’s extreme design makes the investor’s expected payoff negative (inequality (6) in the paper). In summary, with independent attributes the entrepreneur fully exploits his freedom to tilt the test in favor of continuation, and unless the prior $p_0$ or project value is high enough to compensate, the investor’s best response is to walk away.
    
- **(ii) Substitutable attributes:** Here, specificity and sensitivity trade off against each other. Bolton et al. consider an extreme form: _perfect substitutes_ such that $s_1 + s_2 = \kappa$ is fixed (with $\kappa\in(1,2)$). Any increase in the test’s ability to catch true positives comes at the expense of its ability to catch true negatives, and vice versa. This constraint softens the moral hazard slightly but does not eliminate it. The entrepreneur still prioritizes $s_1$ (to avoid false negatives) and dislikes $s_2$ (because a higher $s_2$ would force more failures), so **he will push $s_1$ to the maximum** and let $s_2$ be as low as the substitute constraint allows. In fact, Bolton et al.’s Proposition 3 finds that the entrepreneur’s optimal design in this case is $s_1 = s_1^{\max}$ and $s_2 = \max{\kappa - s_1^{\max},,s_2^{\min}}$. In other words, he chooses the corner of the feasible frontier that maximizes $s_1$ (and thus minimizes $s_2$ unless a lower bound on $s_2$ binds). The **investor’s preferred design under substitutes** will generally be a different point on the $s_1 + s_2 = \kappa$ frontier – one that balances the trade-off between false positives and false negatives to maximize information value. If $\kappa$ is large (close to 2), even the investor would like both dimensions high; but if $\kappa$ is modest, the investor might prefer a more **balanced test** (e.g. moderate $s_1$ and $s_2$) to reduce false positives. The entrepreneur, by contrast, always chooses the **high-$s_1$ extreme**, indicating a continued conflict. However, the substitutability imposes a cost on the entrepreneur’s strategy: pushing $s_1$ to the maximum forces $s_2$ down (more false positives), which eventually hurts the investor’s willingness to fund. Relative to the independent case, this coupling means the **moral hazard is somewhat less catastrophic**, because the entrepreneur cannot simultaneously have an extremely low $s_2$ _and_ a high $s_1$ without constraint – he must give up some $s_1$ if $\kappa$ is low. Still, for all parameter values the entrepreneur’s design **remains more “forgiving”** (biased toward false positives) than the investor’s ideal. Only if the substitute trade-off is very steep (making extreme $s_1$ untenable because it would push $s_2$ below the minimum feasible) would the entrepreneur’s choice inch closer to the investor’s preference. In general, the investor is worse off than in a world with an exogenous optimal test – she either accepts a suboptimal test or forgoes the project if the information loss is too severe.
    
- **(iii) Complementary attributes:** In this scenario, improvements in one dimension of the test _also_ improve the other – specificity and sensitivity move together (the tasks are complements). Bolton et al. model this as, for example, $s_2 = \lambda,s_1$ with a fixed ratio $\lambda\in(0,1)$. Here, any design is constrained such that a test that is **more specific is automatically more sensitive** (and vice versa). This linkage significantly **attenuates the moral hazard**. The entrepreneur cannot increase $s_1$ (reducing false negatives) without also increasing $s_2$ (reducing false positives), which works _against_ his desire to keep the project alive at all costs. In choosing an experiment design, he faces a trade-off: a very high-$s_1$ test would be very informative (high $s_2$ as well), which is good for catching true successes but bad for him because it also more readily kills failures. A very low-information test (low $s_1$ and $s_2$) might let failures slip through (which he likes) but also risks failing a truly viable project (which he dislikes). As a result, with complements the entrepreneur’s **optimal design can become interior** – more informative than in the previous cases. In fact, if the entrepreneur has _any substantial financial stake or future payoff_ tied to actual success, he will lean towards a more informative test when tasks are complementary. Bolton et al. find a critical condition under which the entrepreneur’s incentives fully **coincide with the investor’s “killer experiment”**: if the entrepreneur has **sufficient skin in the game** such that
    

1−α  ≥  Z [ λ(1−p0)−p0 ] p0 V ,1 - \alpha \;\ge\; \frac{Z\,[\,\lambda(1-p_0) - p_0\,]}{\,p_0\,V\,},

then his optimal choice is the _most_ conclusive test (maximizing both $s_1$ and $s_2$). In words, when specificity and sensitivity are complements, giving the entrepreneur a high enough equity stake (or equivalent reward for genuine success) can induce him to select the high-information design _if_ that stake outweighs his private benefit $Z$ from merely continuing. Under this condition the **moral hazard disappears** – the entrepreneur effectively acts as if he were an owner-investor, valuing the elimination of false positives equally to the VC. However, if that inequality does not hold (i.e. if the entrepreneur’s private “science advancement” benefit $Z$ is too large relative to his financial stake), he will still **deviate toward a less conclusive experiment**. Even then, the deviation is limited. Compared to the independent or substitutes cases, complementary tasks mean the entrepreneur cannot freely sacrifice one dimension of accuracy: any attempt to allow more false positives automatically drags down sensitivity, which threatens his upside from a truly viable project. Thus, the **information distortion is much smaller**. The “challenge is… attenuated in cases where [specificity and sensitivity] are complements”. In summary, with complementary attributes the moral hazard is weakest. In the best case (with sufficiently aligned financial incentives) the entrepreneur might **choose the investor’s preferred design exactly**, essentially conducting the “killer experiment” when it maximizes his own expected return. Absent that, he will still choose a more moderate test than in other scenarios, because making the experiment too easy (low $s_2$) would also make it likely to fail even a good project (low $s_1$), which is against his interest. The investor thus faces less extreme information risk, and funding is more likely viable. Bolton et al. formally demonstrate that providing the entrepreneur **minimum necessary skin-in-the-game** can ensure the investor breaks even and the first-best test is implemented in the complementary case. This is a stark contrast to the independent case, where no finite equity share could achieve the first-best because of the entrepreneur’s zero downside exposure.

**Summary:** The severity of the experiment-design moral hazard depends on how flexibility in **specificity ($s_1$)** and **sensitivity ($s_2$)** is structured. With **independent** tasks, the entrepreneur can maximize $s_1$ and minimize $s_2$ independently – yielding an experiment very prone to false positives and very misaligned with the investor’s ideal. This case produces the greatest inefficiency: the entrepreneur’s “least informative” design often causes the venture to be funded when it shouldn’t, or even leads to **market failure** when the investor anticipates unacceptably low information content. Under **substitutability** ($s_1+s_2$ fixed), the entrepreneur still skews toward a high-$s_1$, low-$s_2$ corner, but the trade-off forces a slightly more balanced outcome than the fully independent case. The moral hazard persists – the investor’s preferred balance differs from the entrepreneur’s – but extreme designs are somewhat constrained. Finally, with **complementary** attributes (improving one accuracy measure improves the other), the entrepreneur cannot pursue continuation at all costs without also making the test stricter on bad projects. This linkage **reduces the incentive conflict**. In fact, if the entrepreneur is given a large enough stake in success, he will choose the high-specificity high-sensitivity “killer experiment,” aligning with the investor’s interests. Absent such an incentive, he may still favor a slightly weaker test, but the deviation is much smaller than in the other cases. Thus, **complementarity moderates the moral hazard**, whereas independence or substitutability of experimental attributes exacerbates it – with the independent-case entrepreneur always choosing the most continuation-friendly (least informative) experiment design, and only in the complementary case can their choice sometimes **coincide** with the investor’s optimal design (under appropriate incentive conditions).

## 3. Effect of Multiple Sequential Milestones on the Moral Hazard

If the venture’s development involves **multiple sequential experiments or milestones** (for example, a technical feasibility test followed by a market validation test), the moral hazard in experiment design can compound over stages. In Bolton et al.’s one-stage model, the entrepreneur designs a single experiment knowing the investor will decide whether to continue funding based on that result. Now consider a multi-stage setting: **Milestone 1** (e.g. lab prototype success) and **Milestone 2** (e.g. market acceptance or scalability), each potentially requiring experimental validation before proceeding. The entrepreneur’s incentive at _each_ stage will be to design the experiment in a way that maximizes the probability of passing that stage, securing continuation to the next. This means the moral hazard **recursively applies** at each milestone – the entrepreneur has an incentive to make **early experiments “too soft”**, pushing the project forward even if it should have been halted. Consequently, a project may sail through Milestone 1 on the back of a permissive test, only to fail at Milestone 2 or later. The net effect is that with multiple sequential tests, the entrepreneur can repeatedly defer the revelation of bad news, **passing intermediate hurdles with less-informative experiments**, which increases the chance of late-stage failure. Bolton et al. note that their model helps explain why we observe **low success rates in later stages** of R&D projects and clinical trials – many ventures that pass early tests end up flopping in subsequent phases. This is consistent with the idea that early milestones were not stringent: _“pivotal trials are often initiated with insufficient evidence”_ because the earlier experiments (Milestone 1) were designed under the entrepreneur’s bias, yielding overly optimistic signals. Thus, **sequential milestones can worsen the overall hazard** by allowing an entrepreneur to prolong unviable projects through stage after stage of lenient tests. Each milestone passed under moral hazard essentially **propagates false optimism** to the next stage.

An investor’s ability to **discriminate across milestones or dimensions** – that is, to observe or deduce which aspect of the project caused failure – could partially mitigate this hazard, but only to an extent. In a multi-dimensional experiment (say one that tests several attributes of the venture in parallel), a savvy principal might glean more information about the experiment’s design. Bolton et al. emphasize that in their one-dimensional milestone model, the investor’s inference is very limited: with only a single pass/fail outcome, the investor cannot statistically infer the experiment’s informativeness from one data point. If instead the investor had **multiple signals** or could see results for each milestone separately, the situation approaches a multitask setting where the principal can perform **cross-checks**. For example, suppose Milestone 1 involves testing both technical success and safety profile. If the entrepreneur skews the technical test to always pass, but the safety test (an independent dimension) fails, the investor learns that at least one dimension didn’t meet the bar. More generally, **independent evidence from multiple milestones can expose inconsistencies** in the entrepreneur’s story. Bates et al. (2023) (cited by Bolton et al.) show that when an agent’s actions leave statistical traces, the principal can sometimes deduce private information by examining those patterns. In our context, an investor who sees, for instance, that **Milestone 1 was barely passed and Milestone 2 subsequently fails** can update her beliefs about the design of the first experiment. If this pattern is observed across several ventures (or across the two milestones of the same venture), the investor might infer that Milestone 1’s test was likely too lax (a false positive outcome). This **ex post discrimination** doesn’t save the current venture (which still failed at stage 2), but it can inform the investor’s future behavior or mid-course decisions. For instance, after a Milestone 1 pass, an investor who knows moral hazard is at play might require **additional confirmation** before committing full $K$ to Milestone 2 – effectively inserting another “killer experiment” if possible. If the investor can design or mandate a more rigorous follow-up test at the next stage, that acts as a partial remedy.

However, we should note that unless the investor can directly verify experiment designs or outcomes at each stage, the moral hazard is _not entirely eliminated_. Even with multiple signals, the entrepreneur can attempt to manipulate each one. An investor’s **discrimination across milestones** _reduces_ the hazard by narrowing the entrepreneur’s ability to hide information. By observing which milestone failed, the investor at least knows _where_ the project’s weakness lies (technical vs. market, for example). This could prevent the entrepreneur from blaming a failure on “bad luck” in general – the locus of failure reveals if the earlier test was misleading. Moreover, if milestones are contractually linked (e.g. stage-specific financing), the investor can drop the project as soon as one critical milestone fails, limiting losses. In expectation, having multiple independent hurdles means the **entrepreneur must overcome more chances for failure**, which, if the investor sets them properly, decreases the probability an unviable project makes it all the way through. Thus, additional milestones can serve as **“filters”** that catch bad projects eventually – but the entrepreneur’s manipulation pushes the catching to later rather than sooner. The paradox is that adding milestones gives the investor more opportunities to catch a bad project, yet the entrepreneur’s strategy will be to pass as many as possible, potentially **delaying failure to the latest stage**. On balance, giving the investor more visibility into each milestone’s outcome and cause of failure **helps**: it provides a finer-grained update. In Bayesian terms, the investor can update her belief $p$ after each stage. If stage 1 pass is not very informative (due to moral hazard), a stage 2 result (pass or fail) adds new information that can either confirm or contradict the implied promise of stage 1. For example, if the project passes stage 1 but then **fails at stage 2, this sharpens the inference** that the stage 1 test was too lenient (a false positive). Knowing this pattern, a rational investor might ex ante demand a more stringent stage 1 (if she can influence it) or discount the credibility of a stage 1 success when deciding on stage 2 funding. In effect, **multi-stage financing with discrimination** can act like a sequential screening mechanism: the investor updates cautiously after each milestone, anticipating the entrepreneur’s bias. This tends to **reduce the inefficiency** compared to a single all-or-nothing experiment, but it does not completely remove the entrepreneur’s incentive to game each test.

In summary, multiple sequential experiments **extend the horizon of the moral hazard** – the entrepreneur has incentives at _every_ milestone to design experiments that keep the project alive, potentially resulting in a string of optimistic outcomes followed by a late failure. The investor’s ability to see outcomes at different milestones (and identify which milestone ultimately failed) allows more nuanced decision-making and can somewhat curb the entrepreneur’s freedom to deceive. By statistically analyzing cross-milestone results (for instance, noticing if many stage 2 failures followed stage 1 passes), the investor can adjust her prior or require greater evidence at early stages. Overall, **discriminating across milestones tends to reduce the moral hazard’s impact** – it makes it harder for an entrepreneur to pass a truly unviable project through all gates. Yet, as long as each gate’s design is in the entrepreneur’s hands, the fundamental problem remains. The moral hazard may actually **manifest as higher failure rates in later stages** (since weak projects slip through early), which is precisely what Bolton et al. point to in real-world innovation settings. The multi-stage context underscores the value of **informationally independent tests** and perhaps third-party validation at critical milestones, to counteract the entrepreneur’s bias at each step.

**Summary:** With **multiple sequential milestones**, the entrepreneur’s incentive to avoid failure simply repeats at each stage, often leading to **early-stage tests that are too lenient** and a concentration of failures in later stages. This can worsen the overall inefficiency: many projects that “pass” Milestone 1 under moral hazard will **fail at Milestone 2**, meaning resources were wasted in the interim. An investor who can **distinguish which milestone failed** or gather multiple signals can mitigate this – for example, noticing a pattern of stage 2 failures after stage 1 passes would signal that stage 1 experiments are not stringent. In general, more granular information (multiple independent dimensions or milestones) lets the investor update beliefs more accurately and impose stage-specific stops, **reducing** the moral hazard compared to a single all-or-nothing test. However, so long as the entrepreneur controls each experiment’s design, he will try to game **each hurdle**, so the hazard persists. The investor’s discrimination across stages helps **identify and limit the damage** (dropping projects at the first clear failure), but it cannot fully align incentives without additional mechanisms. This dynamic explains observed phenomena like **low success rates in later trial phases** – early experiments were too permissive – and suggests that while multi-stage financing provides more checkpoints, it must be coupled with careful inference or external validation to meaningfully curb the entrepreneur’s opportunistic experiment design.

## 4. Role of Informational Assumptions and Effect of Reintroducing Traditional Agency Problems

Bolton et al. (2024) deliberately assume away classic informational frictions – there is **no adverse selection** (the entrepreneur and investor share the same prior $p_0$ about project quality), **no shirking or unobservable effort**, and **no diversion of funds** by the entrepreneur. This clean setup isolates the new moral hazard in experiment design. How realistic is this? In practice, early-stage ventures likely face _all_ of these agency problems: entrepreneurs often have private information about the project’s prospects (hidden types), may not always exert maximum effort without incentives (hidden action in effort), and could misuse funding. However, Bolton et al. consciously **“shut down the typical sources of adverse selection and moral hazard”** in venture financing models to highlight the experiment design friction. They assume the investor knows the project’s ex ante NPV and technical merits as well as the entrepreneur does, and that the entrepreneur is **honest and hardworking aside from experiment design choices**. While not fully realistic, this simplification is useful: it shows that _even if_ we eliminate asymmetries in project quality or effort, a serious inefficiency remains purely due to experiment design incentives. In other words, the model’s insights apply on top of any traditional agency issues – and in a sense, Bolton et al. argue this may be a first-order problem especially in “deep tech” ventures where verifying experimental methods is notoriously hard.

If we **reintroduce traditional agency problems** into the model, the interplay becomes complex, but the main insights would qualitatively still hold – in fact, the situation could be even more challenging. Consider **adverse selection**: if the entrepreneur had private information about $p_0$ (the project’s true success probability), a low-quality entrepreneur (with a low actual $p_0$) has an even stronger incentive to design a deceptive experiment to secure funding. The investor, knowing this possibility, would be more suspicious of “good” experimental outcomes. Essentially, a separating equilibrium might be impossible – _every_ entrepreneur would claim a pass result, so the investor cannot tell a genuinely promising project from a fluke or trick. This could **exacerbate market failure**, as the investor might interpret any ambiguous experimental outcome as likely manipulated by a bad type. In such a scenario, one might need additional screening mechanisms (e.g. requiring the entrepreneur to invest some of their own money or provide warranties about the experiment). However, those mechanisms lead us back to Bolton et al.’s conclusion: requiring the entrepreneur to share the downside or to be paid for failure might be necessary. For example, if the entrepreneur had to **co-invest** in the project’s $K$ (i.e. put some of their own capital at risk), it would mitigate both adverse selection and experiment-hazard – only entrepreneurs who truly believe in the project would invest, and having skin in the game (actual cash at risk) would make them less willing to pass a bad project (since they’d lose their investment if it eventually fails). Bolton et al.’s model initially assumed the entrepreneur is capital-constrained (no funds to invest), but in reality, requiring some founder capital could partially align incentives. Still, many deep-tech founders _are_ capital-constrained, so this classical solution is often infeasible or limited.

For **moral hazard in effort**, reintroducing it means the entrepreneur might slack off in conducting the experiment or subsequent development. Typically, giving the entrepreneur an equity stake or performance-based pay would encourage effort. But here we encounter a **trade-off**: giving the entrepreneur more equity _helps_ induce effort but _hurts_ experiment design incentives (as discussed in Question 1). This becomes a multitask incentive problem in the spirit of Holmström and Milgrom (1991): the agent has to both design a good experiment _and_ work hard on execution. High-powered incentives (large equity stake) motivate effort on execution but also motivate the agent to bias the experiment (because that stake makes continuation more valuable to him). Low-powered incentives (no stake) might reduce the experiment bias (since the entrepreneur gains less from continuation) but can lead to underinvestment in effort or diligence. The **optimal contract** in a full model with both moral hazards might involve a delicate balance – perhaps a moderate stake coupled with explicit bonuses for truthful reporting or penalties for project failure. Indeed, Bolton et al. hint that sometimes **low-powered incentives are optimal** to manage multitask moral hazard. If effort is reintroduced, the investor might accept that the entrepreneur works a bit less enthusiastically (with low equity) in order to ensure he doesn’t overly skew the experiment; conversely, if effort is crucial, the investor might give equity but then faces a worse experiment hazard, potentially offsetting the benefits. The presence of both problems likely means **no simple contract can perfectly address both** – it would reinforce the paper’s message that the experiment-design friction is novel and not solved by standard contracts.

Similarly, if **fund diversion or private benefits of funds** were allowed (beyond the private benefit of working on the venture, $Z$), the investor would normally combat that by monitoring or by staged financing with milestones. Bolton et al.’s model already has a flavor of staged financing: the investor only pays $K$ if the experiment passes. In a world with diversion, the investor would be even more cautious in releasing funds and might demand transparency in how funds are used – but that still wouldn’t give transparency into how the experiment is _designed_. In fact, one could argue that if an entrepreneur could divert funds, he might prefer many **false positives** not just for continuation, but to prolong drawing a salary or perks from the venture. This would further worsen his bias towards lenient experiments. On the other hand, a contract that claws back funds or rewards early termination could deter diversion but again would require paying for failure, which Bolton et al. showed is a tricky lever (too high a failure reward causes other distortions).

Overall, reintroducing traditional agency frictions would mean the investor has **more tools** (and needs more tools) but also more objectives to achieve. The main insights of Bolton et al. – that the entrepreneur will choose an experiment design maximizing continuation probability, and that this can kill efficient financing for otherwise NPV-positive projects – remain valid, and indeed those conclusions become more robust. If anything, the presence of asymmetric information about project quality (adverse selection) could make investors **even less willing to fund early experiments** without credible signals, compounding the “market failure” region identified in the paper. The addition of standard agency problems might necessitate combining incentive mechanisms: e.g. giving the entrepreneur some equity to encourage effort, but also maybe a **side contract with a third-party tester** to validate the experiment (addressing design moral hazard), or requiring the entrepreneur to **personally invest** or post collateral (to align downside incentives). Bolton et al. actually discuss potential remedies like third-party validation as ways to alleviate the market failure. In a more realistic model, those interventions become even more important, because one cannot simply dial up equity incentives (that would solve effort selection issues but worsen experiment bias). Thus, the presence of conventional agency problems would _alter the optimal contract mix_, but the **core challenge identified – the strategic design of experiments – would persist and interact with those problems**. We would likely see a need for **innovative contracts**: for instance, contracts that _reward honest revelation_ of bad news or involve milestone-based equity vesting where the entrepreneur’s stake increases only if later milestones confirm early results (to discourage pushing bad projects through the first milestone). These ideas go beyond Bolton et al.’s model, but they stem from the same insight: when the entrepreneur’s informational advantage lies in experiment design, typical “pay-for-performance” contracts alone are inadequate. You must either **give the entrepreneur a stake in failure** (to want to run conclusive tests) or bring in external validators, all while still keeping them motivated to exert effort and not misreport information. Incorporating adverse selection or shirking would not invalidate Bolton et al.’s findings – rather, it highlights that **their moral hazard friction is an independent problem that layers on top of existing ones**. It suggests that deep-tech financing is fraught not only with the usual agency issues, but also with this unique experimentation issue that requires creative solutions beyond the traditional VC toolkit.

**Summary:** Bolton et al. assume away adverse selection and classic moral hazard (effort shirking, fund diversion) to spotlight a new problem – an entrepreneur who _cannot commit to running a high-information experiment_. This assumption is somewhat idealized; in reality, a founder might also hide information or slack off. If those traditional frictions return, the **experiment-design hazard still remains** – and the overall contracting problem becomes more complex. The investor would have to balance incentivizing effort and truthfulness with curbing the experiment bias. For example, giving the entrepreneur equity can motivate effort but, as Bolton et al. show, **does not fix (and can worsen) the experiment design bias**. Likewise, adverse selection would make investors even warier of positive experimental results, potentially **deepening the funding gap**. The main insights would persist: an entrepreneur with no skin in downside will try to prolong the project via experiment design, and this misalignment can prevent otherwise worthy projects from being funded. Reintroducing standard agency issues would likely require **additional mechanisms** on top of those Bolton et al. consider – e.g. entrepreneur co-investment, third-party experiment audits, or milestone-contingent contracts – to address all dimensions of the problem. In sum, Bolton et al.’s friction is a distinct and significant source of inefficiency that doesn’t vanish when usual agency problems are solved; if anything, it calls for **new incentive designs** (like rewarding early failure or external validation) in tandem with traditional solutions, to ensure that entrepreneurs run truly informative experiments in the face of multi-faceted agency concerns.

---

## Summary of Key Points

|**Question**|**Key Analytical Points (Bolton et al. 2024)**|
|---|---|
|**1. Moral Hazard Nature & Incentives**|_Hidden action:_ Entrepreneur privately chooses experiment design (specificity/sensitivity), affecting information content. Investor wants a **“killer experiment”** (maximal true positives & true negatives) to learn viability early, while entrepreneur wants an experiment **“less likely to fail”** – i.e. one that minimizes chance of termination. The entrepreneur enjoys continuation benefits and bears no cost if the project wastes investor money, so he skews the test toward always giving a positive signal. This incentive gap is the core moral hazard, and it leads to **systematically less-informative experiments**.|
|**2(i). Independent Attributes**|When specificity ($s_1$) and sensitivity ($s_2$) can be set independently, the entrepreneur chooses the **most extreme design**: $s_1$ at its maximum and $s_2$ at its minimum. This yields many false positives (bad projects pass) and few false negatives (good projects almost never fail) – a highly **inconclusive test** from the investor’s view. The investor’s ideal is $s_1,s_2$ both high, but the entrepreneur’s design maximizes continuation probability instead. Result: ventures that pass the test still fail too often later (false positives lead to wasting $K$), and if the information gap is too large the investor may refuse to fund at all (market failure condition).|
|**2(ii). Substitutable Attributes**|With a trade-off (e.g. $s_1 + s_2 = \text{constant}$), the entrepreneur still **favors high $s_1$ and low $s_2$**, pushing toward the corner of the feasible set that maximizes success chances at the cost of specificity. He cannot make $s_2$ arbitrarily low without sacrificing $s_1$ (due to the sum constraint), so the moral hazard is slightly **less severe** than in (i). The investor would prefer a more balanced test on the $s_1+s_2$ frontier (to reduce false positives), but the entrepreneur’s design remains biased toward passing trials. Thus, conflict persists: the investor’s and entrepreneur’s “optimal” points on the substitute frontier differ.|
|**2(iii). Complementary Attributes**|If improving one dimension improves the other (e.g. $s_2$ increases with $s_1$), the entrepreneur can’t avoid failures without also lowering the chance of success. This linkage **dampens the moral hazard**. The entrepreneur’s optimal design moves closer to the investor’s: he might choose a moderately high $s_1$ (and thus $s_2$) instead of an extreme, because making the test too easy would also risk false negatives (losing a viable project). Notably, if the entrepreneur is given **sufficient financial stake** in outcomes, he will select the **first-best “killer experiment”** design in this complements case. (Bolton et al. derive a threshold condition on equity share vs. private benefit under which incentives align.) Absent that, the entrepreneur still leans toward a slightly laxer test than the investor wants, but the gap is much smaller – complements mean any reduction in false positives comes with fewer false negatives, aligning both parties more closely.|
|**Conditions for Alignment**|In general, the entrepreneur’s design coincides with the investor’s ideal only in edge cases. _Trivially_, if the entrepreneur had no private continuation benefit ($Z=0$) and cared only about NPV, there’d be no conflict. More interestingly, under **complementary tasks**, giving the entrepreneur a big enough stake (so that he internalizes the cost of false positives) can induce him to choose the high-information test. Bolton et al. show that when $(1-\alpha)$ (entrepreneur’s share) exceeds a threshold (proportional to $Z$ and project parameters), the entrepreneur’s best response is the maximally informative experiment, just like the investor’s preference. In other cases (independent or substitutes, or insufficient stake), their incentives do **not coincide** – the entrepreneur will always bias toward more “forgiving” tests.|
|**3. Multiple Sequential Milestones**|Multiple stages (e.g. Phase 1 technical test, Phase 2 market test) mean the moral hazard can **repeat at each stage**. The entrepreneur will design each experiment to pass, deferring failure to later if possible. This leads to early milestones being passed on overly optimistic criteria and a higher chance of failure in a later milestone. The overall hazard may increase in the sense that an unviable project can consume more capital across stages before being killed. The investor’s expected payoff shrinks, as early positive signals carry little information when they know the tests were lenient. Bolton et al. cite evidence of **low success rates in subsequent trial phases**, consistent with entrepreneurs pushing projects through initial screens with insufficient evidence.|
|**Investor Discrimination Across Stages**|If the investor can observe and distinguish outcomes across different dimensions or stages, it **helps mitigate the hazard**. Separate milestones provide more data points, allowing the investor to update beliefs more accurately. For instance, a failure at Milestone 2, after an easy pass at Milestone 1, signals that the Milestone 1 test was likely too soft (false positive). The investor can use this knowledge in future decisions – demanding stricter evidence at stage 1 or adjusting funding terms. In effect, multiple independent signals let the investor perform a form of statistical inference on the agent’s behavior, which is impossible with a single aggregated outcome. Thus, the ability to discriminate (know which stage failed, or get multiple metrics) **reduces inefficiency** by catching some bad projects earlier. However, it does not fully eliminate the entrepreneur’s incentive to game each stage. The entrepreneur may still attempt to pass all milestones by tweaking each experiment – the difference is that with multiple checkpoints, outright deception is harder to sustain throughout, and the investor can cut losses at the first detected failure.|
|**4. No Informational Asymmetry Assumption**|Bolton et al. assume **symmetric information and no conventional agency problems** (the entrepreneur has no private info about project quality, cannot divert funds, and always uses provided funds for the experiment). This is a simplifying assumption to isolate the new moral hazard. It’s somewhat idealized – in reality, entrepreneurs often do have better insight into their technology (adverse selection) and could shirk or misuse capital – but it ensures that any inefficiency in the model comes _purely_ from experiment design incentives, not from hidden information or effort. They show that even with these classical frictions turned off, a serious funding problem can arise solely due to the experiment design moral hazard.|
|**Reintroducing Traditional Agency Issues**|If we add back adverse selection, hidden effort, or fund diversion, the core experiment-design hazard **still persists** and likely interacts with these problems. For example, with **adverse selection**, a low-quality entrepreneur has an even stronger motive to design a misleading experiment, and the investor becomes more skeptical of positive results – this could further discourage investment (worsening the **market failure region**) unless additional screening is in place. With **effort moral hazard**, giving the entrepreneur equity might motivate work but, as Bolton et al. note, it doesn’t fix experiment bias and may worsen it. Thus the investor faces a multitask problem: balancing incentives for effort vs. truthful experiment design. Solving one problem (say, via high-powered incentives) can aggravate the other. The likely outcome is that **standard VC contracting tools alone are insufficient** – one might need new mechanisms (rewarding early failure, third-party experiment audits, entrepreneur co-investment to share downside, etc.) to address the experiment design issue alongside traditional agency issues. In sum, adding back realism would not nullify Bolton et al.’s insights; rather, it underlines that the identified moral hazard is an _additional_ friction that makes financing innovation even more difficult, requiring careful contract design beyond what’s used for ordinary moral hazard or adverse selection.|
